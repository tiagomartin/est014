---
title: "An√°lise de Componentes Principais"
format: 
  revealjs:
    width: 1600
    height: 900
    footer: ""
    theme: quartomonothemer.scss
    slide-number: c/t
    show-slide-number: all
    preview-links: auto
    self-contained: true
incremental: false
code-link: false
bibliography: references.bib
title-slide-attributes:
    data-background-image: /images/back.png
    data-background-size: cover
    data-background-opacity: "0.3"
execute:
  echo: true
---


## Introdu√ß√£o

Um problema central na an√°lise de dados multivariados √© a **redu√ß√£o da dimensionalidade:** √© poss√≠vel descrever com precis√£o a informa√ß√£o contida nos dados mensurados em $p$ vari√°veis utilizando um conjunto $r < p$ de novas vari√°veis, perdendo a menor quantidade de informa√ß√£o poss√≠vel?


. . .


A **an√°lise de componentes principais** tem este objetivo: dadas $n$ observa√ß√µes de $p$ vari√°veis, se analisa se √© poss√≠vel representar adequadamente esta informa√ß√£o com um n√∫mero menor de vari√°veis constru√≠das como **combina√ß√µes lineares** das vari√°veis originais.


## O Problema...

Dado um conjunto de vari√°veis  $\mathbf{x} = [X_1 \hspace{0.1cm} X_2 \hspace{0.1cm} \cdots \hspace{0.1cm} X_p]^t$, podemos 
encontrar outro conjunto de vari√°veis $\mathbf{y} = [Y_1 \hspace{0.1cm} Y_2 \hspace{0.1cm} \cdots \hspace{0.1cm} Y_r]^t$, dadas por 

$$Y_i= \displaystyle{\sum_{j=1}^p a_{ij}X_j}, \,\, i = 1, \cdots, r < p$$ 

de tal forma que a informa√ß√£o contida em $\mathbf{x}$ esteja sendo bem representada por $\mathbf{y}$?



## Algumas quest√µes

<br>

Vamos encontrar combina√ß√µes lineares para representar informa√ß√£o.


. . .



<p style="text-align: center;"> <span style='font-size:50px;'>&#129300;</span> O que √© **informa√ß√£o**?</p> 




. . .



**Informa√ß√£o $\Longrightarrow$ Vari√¢ncia: quanto maior a variabilidade, maior a informa√ß√£o contida nos dados, maior a 
vari√¢ncia dos dados**


## Algumas quest√µes

<br>


Outra quest√£o importante:


. . .




<p style="text-align: center;"> <span style='font-size:50px;'>&#129300;</span> O que √© **uma boa representa√ß√£o da informa√ß√£o**?</p> 



. . .



**Boa representa√ß√£o da informa√ß√£o $\Longrightarrow$ tomar as componentes de $\mathbf{y}$ que assegurem uma vari√¢ncia similar √† de $\mathbf{x}$**




## Esquematicamente

Nestas condi√ß√µes, temos que buscar **combina√ß√µes lineares** $\mathbf{y}$ das vari√°veis $\mathbf{x}$ de forma que se *maximize* a **vari√¢ncia**


. . .


<style type="text/css">
.tg  {border-collapse:collapse;border-color:white;border-spacing:0;border-style:solid;border-width:1px;margin:0px auto;}
.tg td{border-style:solid;border-width:0px;font-family:Arial, sans-serif;font-size:36px;overflow:hidden;
  padding:10px 5px;word-break:normal;}
.tg th{border-style:solid;border-width:0px;font-family:Arial, sans-serif;font-size:36px;font-weight:normal;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-c3ow{border-color:white;text-align:center;vertical-align:top}
.tg .tg-7btt{border-color:white;font-weight:bold;text-align:center;vertical-align:top}
</style>
<table class="tg"><thead>
  <tr>
    <th class="tg-7btt">Vari√°veis Originais</th>
    <th class="tg-c3ow"></th>
    <th class="tg-7btt">Combina√ß√µes Lineares</th>
  </tr></thead>
<tbody>
  <tr>
    <td class="tg-c3ow">$X_1$</td>
    <td class="tg-c3ow"></td>
    <td class="tg-c3ow">$Y_1$</td>
  </tr>
  <tr>
    <td class="tg-c3ow">$X_2$</td>
    <td class="tg-c3ow"></td>
    <td class="tg-c3ow">$Y_2$</td>
  </tr>
  <tr>
    <td class="tg-c3ow">$\vdots$</td>
    <td class="tg-c3ow"></td>
    <td class="tg-c3ow">$\vdots$</td>
  </tr>
  <tr>
    <td class="tg-c3ow">$X_{r}$</td>
    <td class="tg-c3ow">$\Longrightarrow$</td>
    <td class="tg-c3ow">$Y_r$</td>
  </tr>
  <tr>
    <td class="tg-c3ow">$\vdots$</td>
    <td class="tg-c3ow"></td>
    <td class="tg-c3ow">$\vdots$</td>
  </tr>
  <tr>
    <td class="tg-c3ow">$X_p$</td>
    <td class="tg-c3ow"></td>
    <td class="tg-c3ow">$Y_{p}$</td>
  </tr>
</tbody>
</table>

. . .



<p style="text-align: center;"> $\rm{Var}[\mathbf{y}]$: M√°xima</p> 




## Esquematicamente


Ideia b√°sica da t√©cnica de An√°lise de Componentes Principais:


. . .


<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;margin:0px auto;}
.tg td{border-color:white;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:36px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:white;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:36px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-dxqr{border-color:white;color:#fe0000;text-align:center;vertical-align:top}
.tg .tg-c3ow{border-color:white;text-align:center;vertical-align:top}
.tg .tg-7btt{border-color:white;font-weight:bold;text-align:center;vertical-align:top}
</style>
<table class="tg"><thead>
  <tr>
    <th class="tg-c3ow">Vari√°veis Originais</th>
    <th class="tg-c3ow"></th>
    <th class="tg-c3ow">Componentes Principais</th>
  </tr></thead>
<tbody>
  <tr>
    <td class="tg-c3ow">$X_1$</td>
    <td class="tg-7btt">ACP</td>
    <td class="tg-dxqr">$Y_1$</td>
  </tr>
  <tr>
    <td class="tg-c3ow">$X_2$</td>
    <td class="tg-c3ow">$\Longrightarrow$</td>
    <td class="tg-dxqr">$Y_2$</td>
  </tr>
  <tr>
    <td class="tg-c3ow">$\vdots$</td>
    <td class="tg-c3ow"></td>
    <td class="tg-dxqr">$\vdots$</td>
  </tr>
  <tr>
    <td class="tg-c3ow">$X_{p}$</td>
    <td class="tg-c3ow"></td>
    <td class="tg-dxqr">$Y_r$</td>
  </tr>
  <tr>
    <td class="tg-c3ow"></td>
    <td class="tg-c3ow"></td>
    <td class="tg-c3ow">$\vdots$</td>
  </tr>
  <tr>
    <td class="tg-c3ow"></td>
    <td class="tg-c3ow"></td>
    <td class="tg-c3ow">$Y_{p}$</td>
  </tr>
</tbody>
</table>


. . .

<p style="text-align: center;"> 
$r$ primeiras componentes resumam, por exemplo, 80\% do comportamento geral das $p$ vari√°veis originais 
</p>


## Principais objetivos


- Redu√ß√£o da dimensionalidade dos dados, projetando-os em uma dimens√£o $r < p$;




![](/images/CP/red_dimens.png){fig-align="center"}


## Principais objetivos


- Obten√ß√£o de combina√ß√µes interpret√°veis: determinar √≠ndices e produzir escores com base nos resultados avaliados para as $p$ vari√°veis;


![](/images/CP/idh.png){fig-align="center"}


## Principais objetivos


- Descri√ß√£o e entendimento da estrutura de correla√ß√£o entre as vari√°veis, atrav√©s de algumas combina√ß√µes lineares das mesmas.


![](/images/CP/fig_pca.png){fig-align="center"}



## Componentes Principais: o que s√£o?


**Algebricamente:** s√£o combina√ß√µes lineares das $p$ vari√°veis originais, $X_1, X_2, \cdots, X_p$.


. . .

**Geometricamente:** s√£o as coordenadas dos pontos amostrais em um sistema de eixos obtido pela rota√ß√£o do sistema de eixos original, na dire√ß√£o de variabilidade m√°xima.

<p style="text-align: center;"> 
[<span style='font-size:100px;'>&#128270;</span>](https://setosa.io/ev/principal-component-analysis)
</p>

## Componentes Principais: alguns coment√°rios


- N√£o pressup√µe normalidade dos dados, embora componentes derivadas de popula√ß√µes normais tenham interpreta√ß√µes √∫teis.

. . .

- Com frequ√™ncia, revela rela√ß√µes insuspeitas. Pode permitir interpreta√ß√µes que n√£o seriam obtidas preliminarmente. 



. . .


- Em algumas aplica√ß√µes, os componentes da ACP configuram o objetivo final do estudo. Em outras, servem como passo intermedi√°rio para realiza√ß√£o de outras an√°lises, como regress√£o, classifica√ß√£o, agrupamento, etc...


## Componentes Principais: como obt√™-los?


- Sejam $X_1, \hspace{0.1cm} X_2, \hspace{0.1cm} \cdots, \hspace{0.1cm} X_p$ as vari√°veis originais


. . .


- A ideia √© encontrar um novo conjunto de vari√°veis $Y_1, \hspace{0.1cm} Y_2, \hspace{0.1cm} \cdots, \hspace{0.1cm} Y_p$, tais que:

$$\textrm{Var}[Y_1] \geqslant \textrm{Var}[Y_2] \geqslant \cdots \geqslant \textrm{Var}[Y_p]$$


. . .

- Vamos tomar cada nova vari√°vel $Y_i$, $i = 1, \cdots, p$, como uma combina√ß√£o linear das vari√°veis originais $\mathbf{x}$:

$$Y_i = a_{i1}X_1 + a_{i2}X_2 + \cdots + a_{ip}X_p = \mathbf{a}_i^t \mathbf{x}$$


## Componentes Principais: como obt√™-los?

- Para fixar problemas de escala, adicionamos uma primeira restri√ß√£o aos vetores $\mathbf{a}_i$:


$$\mathbf{a}_i^t \mathbf{a}_i = \displaystyle{ \sum_{j=1}^p a_{ij}^2} = 1$$

. . .


- Para evitar que duas vari√°veis $Y_i$ e $Y_k$, $i \neq k$, $i,k = 1, \cdots, p$, compartilhem informa√ß√£o, adicionamos uma 
segunda restri√ß√£o aos vetores $\mathbf{a}_i$: 

$$\mathbf{a}_i^t \mathbf{a}_k = \displaystyle{ \sum_{j=1}^p a_{ij}a_{kj}}  = 0$$

## Componentes Principais: como obt√™-los?

<br>

<br>

<p style="text-align: center;"> 
üí° **Garantia:** ortogalidade, componentes n√£o correlacionadas, independ√™ncia
</p>


## Componentes Principais: como obt√™-los?



<p style="text-align: center; color: red;"> 
**Primeira Componente Principal**
</p>

$$Y_1 = a_{11}X_1 + a_{12}X_2 + \cdots + a_{1p}X_p = \boldsymbol{a}_1^t \mathbf{x}$$

. . .


**Objetivo:** Encontrar $\boldsymbol{a}_1^t = [a_{11} \hspace{0.3cm} a_{12} \hspace{0.3cm} \cdots \hspace{0.3cm} a_{1p}]^t$
tal que:

<p style="text-align: center; color: red;"> 
**$\rm{Var}[Y_1]$ seja m√°xima**
</p>

. . .

Sujeita √† restri√ß√£o:

$$\boldsymbol{a}_1^t \boldsymbol{a}_1 = a_{11}^2 + a_{12}^2 + \cdots + a_{1p}^2 = 1$$




## Componentes Principais: como obt√™-los?



<p style="text-align: center; color: red;"> 
**Segunda Componente Principal**
</p>

$$Y_2 = a_{21}X_1 + a_{22}X_2 + \cdots + a_{2p}X_p = \boldsymbol{a}_2^t \mathbf{x}$$

. . .



**Objetivo:** Encontrar $\boldsymbol{a}_2^t = [a_{21} \hspace{0.3cm} a_{22} \hspace{0.3cm} \cdots \hspace{0.3cm} a_{2p}]^t$
tal que:

<p style="text-align: center; color: red;"> 
**$\rm{Var}[Y_2]$ seja m√°xima**
</p>

. . .

Sujeita √† restri√ß√£o:

$$\boldsymbol{a}_2^t \boldsymbol{a}_2 = a_{21}^2 + a_{22}^2 + \cdots + a_{2p}^2 = 1$$

$$\rm{Cov}[Y_1,Y_2] = 0$$




## Componentes Principais: como obt√™-los?



<p style="text-align: center; color: red;"> 
**i-√©sima Componente Principal**
</p>

$$Y_i = a_{i1}X_1 + a_{i2}X_2 + \cdots + a_{ip}X_p = \boldsymbol{a}_i^t \mathbf{x}$$

. . .



**Objetivo:** Encontrar $\boldsymbol{a}_i^t = [a_{i1} \hspace{0.3cm} a_{i2} \hspace{0.3cm} \cdots \hspace{0.3cm} a_{ip}]^t$
tal que:

<p style="text-align: center; color: red;"> 
**$\rm{Var}[Y_i]$ seja m√°xima**
</p>

. . .

Sujeita √† restri√ß√£o:

$$\boldsymbol{a}_i^t \boldsymbol{a}_i =  a_{i1}^2 + a_{i2}^2 + \cdots + a_{ip}^2 = 1$$

$$\rm{Cov}[Y_,Y_k] = 0, \text{para  } k < i$$


## A escolha dos vetores $\boldsymbol{a}_i$


- Considere o vetor aleat√≥rio p-variado $\mathbf{x} = [X_1 \hspace{0.3cm} X_2 \hspace{0.3cm} \cdots \hspace{0.3cm} X_p]^t$
com vetor de m√©dias $\boldsymbol{\mu}$ e matriz de covari√¢ncias $\boldsymbol{\Sigma}$, positiva definida (todos os seus autovalores s√£o positivos), 
sendo

$$\boldsymbol{\mu} = [\mu_1 \hspace{0.3cm} \mu_2 \hspace{0.3cm} \cdots \hspace{0.3cm} \mu_p]^t \hspace{0.5cm} \textrm{e} \hspace{0.5cm}
\boldsymbol{\Sigma} = \left[ \begin{array}{cccc} \sigma_{11} & \sigma_{12} & \cdots & \sigma_{1p}
\\ \sigma_{21} & \sigma_{22} & \cdots & \sigma_{2p} 
\\ \vdots & \vdots & \ddots & \vdots
\\ \sigma_{p1} & \sigma_{p2} & \cdots & \sigma_{pp} \end{array} \right]$$


. . .


- Para determina√ß√£o dos componentes principais, com base no que foi exposto, usaremos o seguinte teorema:


## A escolha dos vetores $\boldsymbol{a}_i$

**Teorema - Maximiza√ß√£o de formas quadr√°ticas:** Seja $\boldsymbol{B}$ uma matriz positiva definida com autovalores $\lambda_1 \geqslant \lambda_2 \geqslant \cdots \geqslant \lambda_p > 0$ e autovetores associados normalizados ${\boldsymbol{e}_1, \boldsymbol{e}_2, \cdots, \boldsymbol{e}_p}$. Ent√£o:


$$\max_{\mathbf{x} \neq \boldsymbol{0}} \dfrac{\mathbf{x}^t \boldsymbol{B} \mathbf{x}}{\mathbf{x}^t \mathbf{x}} =  \lambda_1, \text{ obtido quando } \mathbf{x} = \boldsymbol{e}_1;$$

$$\min_{\mathbf{x} \neq \boldsymbol{0}} \dfrac{\mathbf{x}^t \boldsymbol{B} \mathbf{x}}{\mathbf{x}^t \mathbf{x}} =  \lambda_p, \text{ obtido quando } \mathbf{x} = \boldsymbol{e}_p.$$

. . .


- Adicionalmente,

$$\max_{\mathbf{x} \perp \boldsymbol{e}_1, \boldsymbol{e}_1, \cdots, \boldsymbol{e}_k} \dfrac{\mathbf{x}^t \boldsymbol{B} \mathbf{x}}{\mathbf{x}^t \mathbf{x}} =  \lambda_{k+1}, \text{ obtido quando } \mathbf{x} = \boldsymbol{e}_{k+1}.$$



## A escolha dos vetores $\boldsymbol{a}_i$

Assim, no contexto de componentes principais, seja $\mathbf{x} = [X_1 \hspace{0.3cm} X_2 \hspace{0.3cm} \cdots \hspace{0.3cm} X_p]^t$ um vetor aleat√≥rio. Seja $\boldsymbol{\Sigma}$ a matriz de vari√¢ncias e covari√¢ncias e $(\lambda_1, \boldsymbol{e}_1)$, $(\lambda_2, \boldsymbol{e}_2)$, ..., $(\lambda_p, \boldsymbol{e}_p)$ seus autovalores e autovetores, tal que $\lambda_1 \geqslant \lambda_2 \geqslant \cdots \geqslant \lambda_p > 0$. Ent√£o:



$$\max_{\boldsymbol{a} \neq \boldsymbol{0}} \dfrac{\boldsymbol{a}^t \boldsymbol{\Sigma} \boldsymbol{a}}{\boldsymbol{a}^t \boldsymbol{a}} = \max_{\boldsymbol{a} \neq \boldsymbol{0}}(\boldsymbol{a}^t \boldsymbol{\Sigma} \boldsymbol{a}) = \lambda_1, \text{ obtido quando } \boldsymbol{a} = \boldsymbol{e}_1;$$

$$\min_{\boldsymbol{a} \neq \boldsymbol{0}} \dfrac{\boldsymbol{a}^t \boldsymbol{\Sigma} \boldsymbol{a}}{\boldsymbol{a}^t \boldsymbol{a}} = \min_{\boldsymbol{a} \neq \boldsymbol{0}}(\boldsymbol{a}^t \boldsymbol{\Sigma} \boldsymbol{a}) = \lambda_p, \text{ obtido quando } \boldsymbol{a} = \boldsymbol{e}_p.$$

## A escolha dos vetores $\boldsymbol{a}_i$

- Adicionalmente,


$$\max_{\boldsymbol{a} \perp \boldsymbol{e}_1, \boldsymbol{e}_1, \cdots, \boldsymbol{e}_k} \dfrac{\boldsymbol{a}^t \boldsymbol{\Sigma} {\boldsymbol a}}{\boldsymbol{a}^t \boldsymbol{a}} = \max_{\boldsymbol{a} \perp \boldsymbol{e}_1, \boldsymbol{e}_1, \cdots, \boldsymbol{e}_k}(\boldsymbol{a}^t \boldsymbol{\Sigma} \boldsymbol{a})= \lambda_{k+1}, \text{ obtido quando } \boldsymbol{a} = \boldsymbol{e}_{k+1}.$$


## A escolha dos vetores $\boldsymbol{a}_i$


- Uma escolha interessante para os vetores de constantes ${\boldsymbol{a}_i}$, $i = 1, \cdots, p$ s√£o os **autovetores normalizados** ${\boldsymbol{e}_i}$ da matriz $\boldsymbol{\Sigma}$.



. . .


-  Dessa forma, podemos definir a $i$-√©sima componente principal da matriz $\boldsymbol{\Sigma}$, $i = 1, \cdots, p$ como sendo


. . .



$$Y_i = {\boldsymbol{e}_i^t}\mathbf{x} =  e_{i1}X_1 + e_{i2}X_2 + \cdots + e_{ip}X_p$$



## Componentes Principais: propriedades

- A esperan√ßa e a vari√¢ncia da componente $Y_i$ s√£o respectivamente dadas por:

$$
\begin{eqnarray*}
 E[Y_i] &=& E[e_{i1}X_1 + e_{i2}X_2 + \cdots + e_{ip}X_p]  \nonumber \\
&=& e_{i1}E[X_1] + e_{i2}E[X_2] + \cdots + e_{ip}E[X_p]  \nonumber \\
&=& e_{i1}\mu_1 + e_{i2}\mu_2 + \cdots + e_{ip}\mu_p \nonumber \\
&=& {\boldsymbol{e}_i^t}{\boldsymbol{\mu}} \nonumber
\end{eqnarray*}
$$

. . .



$$ \textrm{Var}[Y_i] = \textrm{Var}[{\boldsymbol{e}_i^t}\mathbf{x}] = {\boldsymbol{e}_i^t} \textrm{Var}[\mathbf{x}] {\boldsymbol{e}_i}
 =  {\boldsymbol{e}_i^t} \boldsymbol{\Sigma} {\boldsymbol{e}_i} = {\boldsymbol{e}_i^t} \lambda_i {\boldsymbol{e}_i} =   
{\boldsymbol{e}_i^t} {\boldsymbol{e}_i}\lambda_i = \lambda_i $$



## Na forma matricial


- Sejam $\boldsymbol{O}$ a matriz dos autovetores normalizados da matriz $\boldsymbol{\Sigma}$, isto √©,

$$\boldsymbol{O} = \left[ \begin{array}{cccc} e_{11} & e_{21} & \cdots & e_{p1}
\\ e_{12} & e_{22} & \cdots & e_{p2}
\\ \vdots & \vdots & \ddots & \vdots
\\ e_{1p} & e_{2p} & \cdots & e_{pp} \end{array} \right] = [{\boldsymbol{e}_1} \hspace{0.5cm} {\boldsymbol{e}_2}
\hspace{0.5cm} \cdots \hspace{0.5cm} {\boldsymbol{e}_p}]$$


e $\boldsymbol{y}$ o vetor das componentes principais. Ent√£o, $\boldsymbol{y} = \boldsymbol{O}^t \mathbf{x}$ e a
matriz de covari√¢ncias de $\boldsymbol{y}$ ser√°:

$$\textrm{Var}[\boldsymbol{y}] = \textrm{Var}[\boldsymbol{O}^t \mathbf{x}] =  \boldsymbol{O}^t \textrm{Var}[\mathbf{x}] \boldsymbol{O} =
\boldsymbol{O}^t \boldsymbol{\Sigma} \boldsymbol{O} = \boldsymbol{\Lambda}$$


## Na forma matricial

sendo

$$\boldsymbol{\Lambda} = \left[ \begin{array}{cccc} \lambda_1 & 0 & \cdots & 0
\\ 0 & \lambda_2 & \cdots & 0
\\ \vdots & \vdots & \ddots & \vdots
\\ 0 & 0 & \cdots & \lambda_p \end{array} \right] $$

ou ainda, $\boldsymbol{\Sigma} = \boldsymbol{O} \boldsymbol{\Lambda} \boldsymbol{O}^t = \displaystyle \sum_{i=1}^p \lambda_i \boldsymbol{e}_i \boldsymbol{e}_i^t$, uma vez que $\boldsymbol{O}$ √© uma matriz ortogonal tal que $\boldsymbol{O} \boldsymbol{O}^t = \boldsymbol{O}^t \boldsymbol{O} = \boldsymbol{I}$. Estes resultados s√£o conhecidos como **Teorema da decomposi√ß√£o espectral**.



## Variabilidade explicada

<style type="text/css">
.tg  {border:none;border-collapse:collapse;border-spacing:0;margin:0px auto;}
.tg td{border-style:solid;border-width:0px;font-family:Arial, sans-serif;font-size:36px;overflow:hidden;
  padding:10px 5px;word-break:normal;}
.tg th{border-style:solid;border-width:0px;font-family:Arial, sans-serif;font-size:36px;font-weight:normal;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}
</style>
<table class="tg"><thead>
  <tr>
    <th class="tg-c3ow">Vari√°vel</th>
    <th class="tg-c3ow">Vari√¢ncia</th>
    <th class="tg-c3ow">Componente</th>
    <th class="tg-c3ow">Vari√¢ncia</th>
  </tr></thead>
<tbody>
  <tr>
    <td class="tg-c3ow">$X_1$</td>
    <td class="tg-c3ow">$\sigma_{11}$</td>
    <td class="tg-c3ow">$Y_1$</td>
    <td class="tg-c3ow">$\lambda_1$</td>
  </tr>
  <tr>
    <td class="tg-c3ow">$X_2$</td>
    <td class="tg-c3ow">$\sigma_{22}$</td>
    <td class="tg-c3ow">$Y_2$</td>
    <td class="tg-c3ow">$\lambda_2$</td>
  </tr>
  <tr>
    <td class="tg-c3ow">$\vdots$</td>
    <td class="tg-c3ow">$\vdots$</td>
    <td class="tg-c3ow">$\vdots$</td>
    <td class="tg-c3ow">$\vdots$</td>
  </tr>
  <tr>
    <td class="tg-c3ow">$X_p$</td>
    <td class="tg-c3ow">$\sigma_{pp}$</td>
    <td class="tg-c3ow">$Y_p$</td>
    <td class="tg-c3ow">$\lambda_p$</td>
  </tr>
  <tr>
    <td class="tg-c3ow">Total</td>
    <td class="tg-c3ow">$\sigma_T^2$ = $\displaystyle{\sum_{j=1}^p \sigma_{jj}} = \rm{tr}(\boldsymbol{\Sigma})$</td>
    <td class="tg-c3ow">Total</td>
    <td class="tg-c3ow">$\lambda_T = \displaystyle{\sum_{j=1}^p \lambda_j} = \rm{tr}(\boldsymbol{\Lambda})$</td>
  </tr>
</tbody></table>

. . .

$$\rm{tr}(\boldsymbol{\Sigma}) = \rm{tr}(\boldsymbol{O} \boldsymbol{\Lambda} \boldsymbol{O}^t) = \rm{tr}(\boldsymbol{\Lambda} \boldsymbol{O}^t \boldsymbol{O}) = \rm{tr}(\boldsymbol{\Lambda} \boldsymbol{I}) = \rm{tr}(\boldsymbol{\Lambda})$$


. . .

$$\sigma_T^2 = \lambda_T$$




## Variabilidade explicada


- Pode-se ent√£o concluir que a **j-√©sima componente explica**

$$\displaystyle{\frac{\textrm{Var}[Y_j]}{\textrm{Vari√¢ncia Total de X}}} = \displaystyle{\frac{\lambda_j}{\textrm{tr}(\boldsymbol{\Sigma})}} = 
\displaystyle{\frac{\lambda_j}{\displaystyle{\sum_{i=1}^p \lambda_i}}}$$

da varia√ß√£o total original, e ainda, que as **$r$ primeiras** componentes explicam 

$$\displaystyle{\frac{ \displaystyle \sum_{j=1}^r \textrm{Var}[Y_j]}{\textrm{Vari√¢ncia Total de X}}} = \displaystyle{\frac{\displaystyle \sum_{j=1}^r \lambda_j}{\textrm{tr}(\boldsymbol{\Sigma})}} = 
\displaystyle{\frac{\displaystyle \sum_{j=1}^r \lambda_j}{\displaystyle{\sum_{i=1}^p \lambda_i}}}$$

da varia√ß√£o total.


## Variabilidade explicada


- Busca-se analisar um conjunto menor de vari√°veis sem perder muita informa√ß√£o sobre a estrutura de variabilidade original

. . .


- Aproxima√ß√£o de $\boldsymbol{\Sigma}$: Analisando as $r$ primeiras componentes principais

$$\boldsymbol{\Sigma} \approx \displaystyle \sum_{i=1}^r \lambda_i \boldsymbol{e}_i \boldsymbol{e}_i^t$$


. . .


- Cada parcela da soma envolve uma matriz de dimens√£o $p \times p$ correspondente apenas √† informa√ß√£o da $j$-√©sima componente principal


## Correla√ß√£o com as vari√°veis originais


- Os coeficientes de correla√ß√£o entre a $j$-√©sima vari√°vel e a $i$-√©sima componente principal √© dada por:


$$\rho_{Y_i,X_j} = \displaystyle{\frac{e_{ij} \sqrt{\lambda_i}}{\sqrt{\sigma_{jj}}}}$$

. . .




- As correla√ß√µes medem unicamente a import√¢ncia de uma vari√°vel individual sem considerar a influ√™ncia das demais. N√£o medem a import√¢ncia de $X_i$ na presen√ßa de outras vari√°veis.


. . .


- Os coeficientes (**cargas**) dos componentes ($e_{ij}$), seus sinais e magnitudes, permitem interpretar os componentes e avaliar a import√¢ncia das vari√°veis em sua constitui√ß√£o.



## Estima√ß√£o das Componentes Principais


- Em geral, $\boldsymbol{\Sigma}$ √© estimada por $\boldsymbol{S}$

$$\boldsymbol{S} = \left[ \begin{array}{cccc} s_{11} & s_{12} & \cdots & s_{1p}
\\ s_{21} & s_{22} & \cdots & s_{2p} 
\\ \vdots & \vdots & \ddots & \vdots
\\ s_{p1} & s_{p2} & \cdots & s_{pp} \end{array} \right]$$


. . .


- Autovalores de $\boldsymbol{S}$: $\hat{\lambda}_1, \hat{\lambda}_2, \cdots, \hat{\lambda}_p$


. . .


- Autovetores de $\boldsymbol{S}$: $\hat{\boldsymbol{e}}_1, \hat{\boldsymbol{e}}_2, \cdots, \hat{\boldsymbol{e}}_p$



## Estima√ß√£o das Componentes Principais

- Estima√ß√£o da $j$-√©sima componente principal de $\boldsymbol{S}$:

$$\hat{Y}_j = {\hat{\boldsymbol{e}}_j^t}\mathbf{x} =  \hat{e}_{j1}X_1 + \hat{e}_{j2}X_2 + \cdots + \hat{e}_{jp}X_p, \,\,\,\,\,\, j = 1, 2, \cdots, p$$

. . .




- Componentes principais amostrais - Propriedades
    - Vari√¢ncia: $\text{Var}(\hat{Y}_j) = \hat{\lambda}_j$
    - Covari√¢ncia entre as componentes: $\text{Cov}(\hat{Y}_j, \hat{Y}_k) = 0, \,\,\, j \neq k$
    - Vari√¢ncia total estimada explicada pela componente:
$$\dfrac{\text{Var}(\hat{Y}_j)}{\text{Vari√¢ncia total estimada de } \mathbf{x}} = \dfrac{\hat{\lambda}_j}{\text{tr}(\boldsymbol{S})} =  \dfrac{\hat{\lambda}_j}{\sum \limits_{i=1}^p \hat{\lambda}_i}$$

## Estima√ß√£o das Componentes Principais

- Correla√ß√£o estimada entre componente e vari√°vel:
$$r_{\hat{Y}_j,\hat{X}_k} = \displaystyle{\frac{\hat{e}_{jk} \sqrt{\hat{\lambda}_j}}{\sqrt{\sigma_{kk}}}}$$

. . .

-  Decomposi√ß√£o espectral de $\boldsymbol{S}$:

$$\boldsymbol{S} = \displaystyle \sum_{j=1}^p \hat{\lambda}_j  \hat{\boldsymbol{e}}_j  \hat{\boldsymbol{e}}_j^t$$


## Estima√ß√£o das Componentes Principais


- Aproxima√ß√£o de $\boldsymbol{S}$ pelas $r$ primeiras componentes


$$\boldsymbol{S} \approx \displaystyle \sum_{j=1}^r \hat{\lambda}_j \hat{\boldsymbol{e}}_j  \hat{\boldsymbol{e}}_j^t$$

. . .



- Escores das componentes
    - Valor das componentes para cada elemento amostral
    - Na pr√°tica, o uso das componentes relevantes se d√° atrav√©s dos escores


## Exemplo: (Mingoti,2007)

<p style="text-align: center; color: black; font-size: 34px;"> 
12 empresas, 3 vari√°veis: ganho bruto ($X_1$), ganho l√≠quido ($X_2$) e patrim√¥nio acumulado ($X_3$)
</p>


<style type="text/css">
.tg  {border:none;border-collapse:collapse;border-color:#ccc;border-spacing:0;margin:0px auto;}
.tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:0px;color:#333;
  font-family:Arial, sans-serif;font-size:30px;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:0px;color:#333;
  font-family:Arial, sans-serif;font-size:30px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-c3ow{border-color:white;text-align:center;vertical-align:top}
</style>
<table class="tg"><thead>
  <tr>
    <th class="tg-c3ow">Empresa</th>
    <th class="tg-c3ow">Ganho bruto $(X_1)$</th>
    <th class="tg-c3ow">Ganho l√≠quido $(X_2)$</th>
    <th class="tg-c3ow">Patrim√¥nio $(X_3)$</th>
  </tr></thead>
<tbody>
  <tr>
    <td class="tg-c3ow">E1</td>
    <td class="tg-c3ow">9893</td>
    <td class="tg-c3ow">564</td>
    <td class="tg-c3ow">17689</td>
  </tr>
  <tr>
    <td class="tg-c3ow">E2</td>
    <td class="tg-c3ow">8776</td>
    <td class="tg-c3ow">389</td>
    <td class="tg-c3ow">17359</td>
  </tr>
  <tr>
    <td class="tg-c3ow">E3</td>
    <td class="tg-c3ow">13572</td>
    <td class="tg-c3ow">1103</td>
    <td class="tg-c3ow">18597</td>
  </tr>
  <tr>
    <td class="tg-c3ow">E4</td>
    <td class="tg-c3ow">6455</td>
    <td class="tg-c3ow">743</td>
    <td class="tg-c3ow">8745</td>
  </tr>
  <tr>
    <td class="tg-c3ow">E5</td>
    <td class="tg-c3ow">5129</td>
    <td class="tg-c3ow">203</td>
    <td class="tg-c3ow">14397</td>
  </tr>
  <tr>
    <td class="tg-c3ow">E6</td>
    <td class="tg-c3ow">5432</td>
    <td class="tg-c3ow">215</td>
    <td class="tg-c3ow">3467</td>
  </tr>
  <tr>
    <td class="tg-c3ow">E7</td>
    <td class="tg-c3ow">3807</td>
    <td class="tg-c3ow">385</td>
    <td class="tg-c3ow">4679</td>
  </tr>
  <tr>
    <td class="tg-c3ow">E8</td>
    <td class="tg-c3ow">3423</td>
    <td class="tg-c3ow">187</td>
    <td class="tg-c3ow">6754</td>
  </tr>
  <tr>
    <td class="tg-c3ow">E9</td>
    <td class="tg-c3ow">3708</td>
    <td class="tg-c3ow">127</td>
    <td class="tg-c3ow">2275</td>
  </tr>
  <tr>
    <td class="tg-c3ow">E10</td>
    <td class="tg-c3ow">3294</td>
    <td class="tg-c3ow">297</td>
    <td class="tg-c3ow">6754</td>
  </tr>
  <tr>
    <td class="tg-c3ow">E11</td>
    <td class="tg-c3ow">5433</td>
    <td class="tg-c3ow">432</td>
    <td class="tg-c3ow">5589</td>
  </tr>
  <tr>
    <td class="tg-c3ow">E12</td>
    <td class="tg-c3ow">6287</td>
    <td class="tg-c3ow">451</td>
    <td class="tg-c3ow">8972</td>
  </tr>
</tbody></table>



## Primeiro Exemplo: (Mingoti,2007)

```{r}
load <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg))
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
} 

## Pacotes utilizados nessa an√°lise

packages = c("tidyverse", "factoextra", "psych", "gridExtra")
load(packages)
```



```{r}
dados <- read.table("https://raw.githubusercontent.com/tiagomartin/est014/refs/heads/master/dados/empresas.txt", row.names = 1, header = TRUE) 
dados %>% 
  str()
```


## Primeiro Exemplo: (Mingoti,2007)



```{r}
x_barra = dados %>% 
  colMeans()

x_barra

S = dados %>% 
  var()

S
```


## Primeiro Exemplo: (Mingoti,2007)



```{r}
dados %>% 
  boxplot() 
```


## Primeiro Exemplo: (Mingoti,2007)

```{r}
## Analise de Componentes Principais utilizando a matriz de covariancias (Nao aconselhavel, neste caso)
acp_S = prcomp(dados)

## Proporcao da variacao explicada
summary(acp_S)

## Loadings (cargas)
acp_S$rotation

## Opcional: trocar sinal da primeira componente e escores
acp_S$rotation = -acp_S$rotation
acp_S$x = -acp_S$x

## Loadings (cargas)
acp_S$rotation
```


## Primeiro Exemplo: (Mingoti,2007)

$$Y_1 = 0,425 \times \text{GB} + 0,028 \times \text{GL} + 0,905 \times \text{PA} \Longrightarrow
94,18\% \text{ da informa√ß√£o total de } \mathbf{x}$$


. . .


<p style="text-align: center; color: red; font-size: 46px;"> 
**Qual a vari√°vel mais importante para $Y_1$?**
</p>


::: {.fragment .fade-in}
<p style="text-align: center; color: black; font-size: 46px;"> 
**Patrim√¥nio**
</p>
:::

::: {.fragment .fade-up}

:::: {.columns}

::: {.column width="60%"}
![](/images/CP/empresas_bp.png){fig-align="center"}
:::

::: {.column width="40%"}
<p style="text-align: center; color: red; font-size: 100px;"> 
<span style='font-size:150px;'>&#129300;</span>**Ser√°?**
</p>
:::
::::
:::



## Componentes Principais: vari√°veis padronizadas

- Padroniza√ß√£o do vetor aleat√≥rio $\mathbf{x}$: 

$$\boldsymbol{z} = \boldsymbol{D}^{-1}(\mathbf{x} - \boldsymbol{\mu})$$

. . .


- $\boldsymbol{D}$: matriz diagonal de desvios-padr√£o 



. . .


- Vari√°vel padronizada: $Z_i = \dfrac{X_i - \mu_i}{\sqrt{\sigma_{ii}}}$


. . .



- Matriz de covari√¢ncias de $\boldsymbol{z}$

$$\text{Cov}(\boldsymbol{z}) = \boldsymbol{D}^{-1}\boldsymbol{\Sigma}  \boldsymbol{D}^{-1} = \boldsymbol{P} = \text{Cor}(\mathbf{x})$$

. . .


- Componentes principais de $\boldsymbol{z}$: obtidas dos autovalores e autovetores de $\boldsymbol{P}$


## Componentes Principais: vari√°veis padronizadas


- A $j$-√©sima componente principal da matriz $\boldsymbol{P}$: 

$$Y_j = \boldsymbol{e}_j^t \boldsymbol{z} =  \boldsymbol{e}_j^t \boldsymbol{D}^{-1}(\mathbf{x} - \boldsymbol{\mu}) = {e}_{j1}Z_1 + {e}_{j2}Z_2 + \cdots + {e}_{jp}Z_p$$

sendo $\boldsymbol{e}_j$, o $j$-√©simo autovetor da matriz $\boldsymbol{P}$, $j = 1, \cdots p$.


. . .


- Vari√¢ncia total de $\boldsymbol{P}$ 

$$\sum \limits_{j=1}^p \text{Var}(Y_j) = \sum \limits_{j=1}^p \text{Var}(Z_j) = p$$

## Componentes Principais: vari√°veis padronizadas


- Propor√ß√£o da vari√¢ncia populacional padronizada devido √† $j$-√©sima componente

$$\dfrac{\text{Var}({Y}_j)}{\text{Vari√¢ncia total de } \boldsymbol{z}} = \dfrac{{\lambda}_j}{\text{tr} (\boldsymbol{P})} =  \dfrac{{\lambda}_j}{p}$$


. . .


- Correla√ß√£o entre componente $Y_j$ e a vari√°vel padronizada $Z_k$: 

$$\rho_{Y_j,Z_k} = e_{jk} \sqrt{\lambda_j}$$



## Observa√ß√µes


- As componentes principais **n√£o s√£o invariantes √† mudan√ßas de escala**. Os resultados s√£o **diferentes** quando se faz a an√°lise utilizando a matriz de covari√¢ncias e a matriz de correla√ß√µes.


. . .


- **As componentes obtidas a partir da matriz de covari√¢ncias s√£o influenciadas pelas vari√°veis de maior vari√¢ncia**. A **matriz de correla√ß√µes**, em geral, √© a melhor op√ß√£o quando as **vari√¢ncias s√£o muito heterog√™neas**. 


. . .


- Um valor **pequeno incomum** para o **√∫ltimo autovalor** da matriz de covari√¢ncias (ou correla√ß√£o) amostral pode indicar uma **depend√™ncia linear n√£o detectada** no conjunto de dados.


. . .

- Valores grandes de autovalores (e correspondentes autovetores) s√£o importantes em uma an√°lise.




## Quantas componentes devem ser retidas?


- **Crit√©rio de Kaiser:** Trata-se de uma regra pr√°tica (heur√≠stica) para decidir quantas componentes principais manter em uma PCA extra√≠da atrav√©s da matriz de correla√ß√µes. Ele diz:

> Retenha apenas as componentes associadas a um autovalor maior que 1.

- $\lambda = 1$ ‚Üí a componente explica tanta variabilidade quanto 1 vari√°vel original
- $\lambda > 1$ ‚Üí explica mais variabilidade do que qualquer vari√°vel isolada
- $\lambda < 1$ ‚Üí explica menos variabilidade do que uma √∫nica vari√°vel ‚Üí ent√£o n√£o ‚Äúvale a pena‚Äù



## Quantas componentes devem ser retidas?


- **M√©dia dos autovalores:** √â um crit√©rio alternativo quando a PCA √© feita sobre **matriz de covari√¢ncias**.

> Se PCA foi feita na matriz de covari√¢ncias, retenha PCs com autovalores maiores que a m√©dia dos autovalores.


 Ent√£o, se uma componente tem autovalor:

$$\lambda_k > \dfrac{\sum \limits_{j=1}^p \lambda_j}{p}$$

ela ret√©m mais informa√ß√£o do que a vari√¢ncia m√©dia por dimens√£o do espa√ßo original.




## Quantas componentes devem ser retidas?


- **Screeplot:** √â um gr√°fico que coloca, no eixo X, o n√∫mero da componente principal e, no eixo Y, o autovalor (ou a vari√¢ncia explicada).


> O ponto onde a curva deixa de cair abruptamente e come√ßa a ‚Äúhorizontalizar‚Äù √© onde voc√™ para de manter componentes.



- A ideia √© identificar o ‚Äúcotovelo‚Äù da curva.
    - antes do cotovelo: cada componente adiciona bastante vari√¢ncia
    - depois do cotovelo: os autovalores ficam ‚Äúquase uma linha reta‚Äù ‚Üí s√≥ ru√≠do
    
    

## Quantas componentes devem ser retidas?


```{r,  echo=FALSE}
pca <- prcomp(scale(iris[,1:4]))
fviz_eig(pca, geom="line",  choice = c("eigenvalue"), addlabels = TRUE, main = "")
```


## Quantas componentes devem ser retidas?


- Reter o n√∫mero de componentes principais que acumulem pelo menos certa porcentagem da variabilidade total dos dados, digamos 70\%.


```{r, echo=FALSE}
fviz_eig(pca, geom="bar",  choice = c("variance"), addlabels = TRUE, main = "")
```



## Quantas componentes devem ser retidas?

- **Parallel Analysis (Horn, 1965):** √â o m√©todo moderno mais recomendado para decidir quantos componentes reter em PCA.

> Compare seus autovalores reais com autovalores esperados ao acaso.


## Quantas componentes devem ser retidas?

- O procedimento envolve: 
    - Calcular os autovalores dos dados originais.
    - Gerar dados aleat√≥rios com a mesma estrutura dos dados originais.
    - Calcular autovalores para os dados aleat√≥rios, repetindo o processo muitas vezes para obter uma distribui√ß√£o amostral.
    - Comparar os autovalores reais com os autovalores m√©dios ou percentis (como o 95¬∫) dos dados simulados.
    - Reter os componentes cujos autovalores reais s√£o maiores do que os autovalores aleat√≥rios correspondentes. 


## Quantas componentes devem ser retidas?


```{r, echo=FALSE}
pca <- principal(iris[,1:4], nfactors = 4, rotate = "none")
fa.parallel(iris[,1:4], fa="pc", n.iter=1000)
```


## Quantas componentes devem ser retidas?


- **Interpreta√ß√£o desse gr√°fico**
    - linha azul = autovalores reais dos seus dados
    - linhas vermelha / pontilhada = autovalores esperados pelo acaso (parallel analysis)

. . .


- **Regra:** retenha apenas os componentes cuja linha azul est√° acima da linha vermelha.
    - Parallel Analysis (Horn) est√° dizendo: retenha 1 componente principal.



## Interpreta√ß√£o das componentes principais

- Em geral, quando existe uma **alta correla√ß√£o positiva** entre todas as vari√°veis, os **sinais associados √†s vari√°veis coincidem** na primeira componente principal.


. . .


- Neste caso, a primeira componente principal pode ser interpretada como um **√≠ndice global**, calculado como uma m√©dia ponderada de todas as vari√°veis.


. . .

- O restante das componentes, normalmente possuem **pesos negativos e positivos** e s√£o interpretadas como um **contraste entre grupos de vari√°veis**.


## Voltando ao Exemplo


<p style="text-align: center; color: black; font-size: 34px;"> 
12 empresas, 3 vari√°veis: ganho bruto ($X_1$), ganho l√≠quido ($X_2$) e patrim√¥nio acumulado ($X_3$)
</p>


<style type="text/css">
.tg  {border:none;border-collapse:collapse;border-color:#ccc;border-spacing:0;margin:0px auto;}
.tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:0px;color:#333;
  font-family:Arial, sans-serif;font-size:30px;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:0px;color:#333;
  font-family:Arial, sans-serif;font-size:30px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-c3ow{border-color:white;text-align:center;vertical-align:top}
</style>
<table class="tg"><thead>
  <tr>
    <th class="tg-c3ow">Empresa</th>
    <th class="tg-c3ow">Ganho bruto $(X_1)$</th>
    <th class="tg-c3ow">Ganho l√≠quido $(X_2)$</th>
    <th class="tg-c3ow">Patrim√¥nio $(X_3)$</th>
  </tr></thead>
<tbody>
  <tr>
    <td class="tg-c3ow">E1</td>
    <td class="tg-c3ow">9893</td>
    <td class="tg-c3ow">564</td>
    <td class="tg-c3ow">17689</td>
  </tr>
  <tr>
    <td class="tg-c3ow">E2</td>
    <td class="tg-c3ow">8776</td>
    <td class="tg-c3ow">389</td>
    <td class="tg-c3ow">17359</td>
  </tr>
  <tr>
    <td class="tg-c3ow">E3</td>
    <td class="tg-c3ow">13572</td>
    <td class="tg-c3ow">1103</td>
    <td class="tg-c3ow">18597</td>
  </tr>
  <tr>
    <td class="tg-c3ow">E4</td>
    <td class="tg-c3ow">6455</td>
    <td class="tg-c3ow">743</td>
    <td class="tg-c3ow">8745</td>
  </tr>
  <tr>
    <td class="tg-c3ow">E5</td>
    <td class="tg-c3ow">5129</td>
    <td class="tg-c3ow">203</td>
    <td class="tg-c3ow">14397</td>
  </tr>
  <tr>
    <td class="tg-c3ow">E6</td>
    <td class="tg-c3ow">5432</td>
    <td class="tg-c3ow">215</td>
    <td class="tg-c3ow">3467</td>
  </tr>
  <tr>
    <td class="tg-c3ow">E7</td>
    <td class="tg-c3ow">3807</td>
    <td class="tg-c3ow">385</td>
    <td class="tg-c3ow">4679</td>
  </tr>
  <tr>
    <td class="tg-c3ow">E8</td>
    <td class="tg-c3ow">3423</td>
    <td class="tg-c3ow">187</td>
    <td class="tg-c3ow">6754</td>
  </tr>
  <tr>
    <td class="tg-c3ow">E9</td>
    <td class="tg-c3ow">3708</td>
    <td class="tg-c3ow">127</td>
    <td class="tg-c3ow">2275</td>
  </tr>
  <tr>
    <td class="tg-c3ow">E10</td>
    <td class="tg-c3ow">3294</td>
    <td class="tg-c3ow">297</td>
    <td class="tg-c3ow">6754</td>
  </tr>
  <tr>
    <td class="tg-c3ow">E11</td>
    <td class="tg-c3ow">5433</td>
    <td class="tg-c3ow">432</td>
    <td class="tg-c3ow">5589</td>
  </tr>
  <tr>
    <td class="tg-c3ow">E12</td>
    <td class="tg-c3ow">6287</td>
    <td class="tg-c3ow">451</td>
    <td class="tg-c3ow">8972</td>
  </tr>
</tbody></table>



## Voltando ao Exemplo

![](/images/CP/empresas_bp.png){fig-align="center"}


## Voltando ao Exemplo


```{r}
dados %>% 
  scale(center=TRUE, scale=TRUE) 
```




## Voltando ao Exemplo

```{r}
dados %>% 
  scale(center=TRUE, scale=TRUE) %>% 
  boxplot()
```



## Voltando ao Exemplo

```{r}
## Analise de Componentes Principais utilizando a matriz de correlacoes (Mais aconselhavel, neste caso)
acp_R = prcomp(dados, scale. = TRUE)

## Proporcao da variacao explicada
summary(acp_R)

## Loadings (cargas)
acp_R$rotation

## Opcional: trocar sinal da primeira componente e escores
acp_R$rotation = -acp_R$rotation
acp_R$x = -acp_R$x

## Loadings (cargas)
acp_R$rotation
```


## Voltando ao Exemplo

```{r}
## Escores das componentes principais
Yr = acp_R$x
Yr %>% head()
```

```{r}
## Matriz de correlacoes entre variaveis originais e componentes principais
Ryx_R = cor(dados,Yr)
Ryx_R
```


## Voltando ao Exemplo

```{r}
fa.parallel(dados %>% scale(center=TRUE, scale=TRUE), fa="pc", n.iter=1000)
```


## Voltando ao Exemplo

$$Z_{GB} = \dfrac{\text{Ganho Bruto} - \overline{\text{Ganho Bruto}}}{s_{\text{Ganho Bruto}}}$$

$$Z_{GL} = \dfrac{\text{Ganho L√≠quido} - \overline{\text{Ganho L√≠quido}}}{s_{\text{Ganho L√≠quido}}}$$

$$Z_{PA} = \dfrac{\text{Patrim√¥nio} - \overline{\text{Patrim√¥nio}}}{s_{\text{Patrim√¥nio}}}$$




## Voltando ao Exemplo

$$Y_1 =  0,617 \times Z_{GB} + 0,557 \times Z_{GL} + 0,556 \times Z_{PA} \Longrightarrow
83,08\% \text{ da informa√ß√£o total de } \boldsymbol{z}$$

. . .


- **Interpreta√ß√£o:** √â basicamente um √≠ndice de desempenho global da empresa. O coeficiente de maior grandeza num√©rica desta componente √© relativo a ganho bruto enquanto que os demais coeficientes s√£o aproximadamente iguais. Quanto maior os valores de ganhos brutos e l√≠quido e patrim√¥nio da empresa, maior ser√° o valor num√©rico da componente. Al√©m disso, todas as tr√™s vari√°veis possui alta correla√ß√£o com essa componente, indicando serem importantes na composi√ß√£o da mesma.





## Voltando ao Exemplo

$$Y_2 =  -0,001 \times Z_{GB} - 0,706 \times Z_{GL} + 0,708 \times Z_{PA} \Longrightarrow
14,12\% \text{ da informa√ß√£o total de } \boldsymbol{z}$$

. . .


- **Interpreta√ß√£o:** √â uma compara√ß√£o entre as vari√°veis ganho l√≠quido e patrim√¥nio, sendo que essas duas vari√°veis possuem igual import√¢ncia na composi√ß√£o da mesma. Valores pr√≥ximos de zero dessa componente indicam empresas com um certo equil√≠brio entre ganho l√≠quido e patrim√¥nio acumulado no per√≠odo.




## Voltando ao Exemplo


```{r}
fviz_pca_biplot(acp_R, repel = TRUE,
                col.var = "#2E9FDF", # Variables color
                col.ind = "#696969"  # Individuals color
                )

```


## Voltando ao Exemplo

**Interpreta√ß√£o do biplot** 


- As setas indicam em que dire√ß√£o a vari√°vel aumenta. os indiv√≠duos que est√£o posicionados no sentido da seta s√£o os que t√™m valores maiores naquela vari√°vel.

. . .

- **√Çngulo entre a seta (vari√°vel) e o eixo PC1:** O cosseno do √¢ngulo entre a vari√°vel e a PC √© exatamente o **loading**.

$$\cos(\theta_j, PC_i) = \text{loading}_{X_j, PC_i}$$


## Voltando ao Exemplo


ent√£o:

- se a seta est√° quase colada no eixo $PC_i$
    - loading pr√≥ximo de +1 ‚Üí vari√°vel altamente alinhada com a componente
    - √© vari√°vel que define o componente principal

- se a seta faz um √¢ngulo grande (perto de 90¬∞) com $PC_i$
    - loading $\approx 0$ ‚Üí essa vari√°vel n√£o contribui para a componente

- se a seta aponta para o lado oposto da $PC_i$ (180¬∞)
    - loading $\approx -1$ ‚Üí vari√°vel altamente alinhada negativamente

## Voltando ao Exemplo

- **Observa√ß√£o:** Quando a PCA √© feita na **matriz de correla√ß√µes**, os loadings s√£o exatamente as correla√ß√µes entre as vari√°veis originais e as componentes principais. Quando usamos **matriz de covari√¢ncias**, os loadings refletem contribui√ß√£o em vari√¢ncia e o cosseno do √¢ngulo n√£o corresponde numericamente √† correla√ß√£o.

## Voltando ao Exemplo



- **PC1 (Dim1 = 83.1%):** Praticamente toda a informa√ß√£o relevante est√° aqui.

> PC1 representa um eixo geral de n√≠vel financeiro / escala econ√¥mica.
Quanto maior Patrim√¥nio / Ganho Bruto / Ganho L√≠quido ‚Üí mais √† direita o indiv√≠duo aparece.


- Todas vari√°veis apontam para a direita, e com √¢ngulos semelhantes ‚Üí alt√≠ssima correla√ß√£o entre elas





## Voltando ao Exemplo

- Quem est√° mais pra direita (E1, E2, E3) ‚Üí s√£o aqueles com valores altos nessas vari√°veis

- Quem est√° mais pra esquerda (E8, E9, E10...) ‚Üí s√£o os com valores baixos

- A empresa E12 posiciona-se muito pr√≥xima √† origem do plano principal, indicando um perfil mediano em todas as vari√°veis financeiras consideradas. 
    - Ela n√£o apresenta caracter√≠sticas extremas nem para valores altos, nem para valores baixos, sendo portanto uma empresa altamente representativa do centro da distribui√ß√£o.


. . .

- **PC2 (Dim2 = 14.1%):** quase n√£o traz informa√ß√£o nova ‚Üí setas n√£o sobem muito, elas est√£o quase horizontais
    - n√£o existe uma segunda dimens√£o ‚Äúconceitual‚Äù forte



## Voltando a Exemplo

```{r}
# Contribui√ß√µes das vari√°veis para a PC1
fviz_contrib(acp_R, choice = "var", axes = 1)
```

## Voltando ao Exemplo

- Esse gr√°fico mostra o quanto cada vari√°vel explica/contribui para a forma√ß√£o do primeiro componente principal.
    - Ganho Bruto contribuiu um pouco mais
    - Ganho Liquido e Patrimonio contribuem praticamente igual e muito pr√≥ximo


. . .


- A linha vermelha tracejada √© a contribui√ß√£o m√©dia esperada (se todas contribu√≠ssem igual).
    - Como temos 3 vari√°veis ‚Üí contribui√ß√£o m√©dia = $100\%/3 \approx 33.33\%$
    
## Voltando ao Exemplo

```{r}
fviz_contrib(acp_R, choice = "ind", axes = 1:2)
```


## Voltando ao Exemplo

- Esse gr√°fico mostra quais indiv√≠duos contribuem mais para definir o plano principal da PC, isto √©, quais observa√ß√µes est√£o orientando a dire√ß√£o das componentes principais.

. . .


- A empresa E3 √© a maior influenciadora da PCA: sua contribui√ß√£o para o plano principal √© muito superior √† m√©dia (linha vermelha), indicando que ela √© um caso extremo na dire√ß√£o da primeira componente. Esse ponto est√° orientando de forma dominante a estrutura da an√°lise.

