---
title: "Revisão de Álgebra Matricial"
format: 
  revealjs:
    width: 1600
    height: 900
    footer: ""
    theme: quartomonothemer.scss
    slide-number: c/t
    show-slide-number: all
    preview-links: auto
    self-contained: true
incremental: false
code-link: true
bibliography: references.bib
title-slide-attributes:
    data-background-image: /images/back.png
    data-background-size: cover
    data-background-opacity: "0.3"
execute:
  echo: true
---

## Definição e propriedades básicas

- Uma matriz é um conjunto de números ou variáveis dispostos em linhas e colunas.

. . .


- Uma matriz $\mathbf{A}$ de $n$ linhas e $p$ colunas (dimensão $n \times p$) pode ser representada, genericamente, por:

$${\mathbf A} = \left[ \begin{array}{cccc} a_{11} & a_{12} & \cdots & a_{1p} \\ a_{21} & a_{22} & \cdots & a_{2p} \\ \vdots & \vdots & \ddots & \vdots \\ 
a_{n1} & a_{n2} & \cdots & a_{np} \end{array} \right]$$

. . .

- A matriz $\mathbf{A}$ pode ser denotada ainda por $\mathbf{A} = \{a_{ij}\}$, onde o primeiro índice indica linha, o segundo coluna e $a_{ij}$ é o termo geral da matriz.


## Definição e propriedades básicas

- Um vetor $\mathbf{x}$, de dimensão $n$, é representado, genericamente, por:
		

$$\mathbf{x} = \left[ \begin{array}{c} x_1 \\ x_2 \\ \vdots \\ x_n \end{array} \right] $$
		


. . .
		

- Numa análise multivariada com $n$ indivíduos e $p$ variáveis, as linhas da **matriz de dados** (observações dos indivíduos) podem ser consideradas	$n$ vetores de dimensão $p$: $\mathbf{x}_i^t = (x_{i1}, x_{i2}, \cdots, x_{ip}), \,\,\,\,\, i = 1, 2, \cdots, n$;
		
		
		
## Definição e propriedades básicas

		
- As colunas da matriz de dados (observações referentes à variáveis) podem ser consideradas $p$ vetores de dimensão $n$: 

$$\mathbf{x}_j^t = (x_{1j}, x_{2j}, \cdots, x_{nj}), \,\,\,\,\, j = 1, 2, \cdots, p$$


. . .


- A multiplicação de um vetor $\mathbf{x} = (x_1, x_2 , \cdots, x_p)^t$ por um escalar real $c$		resulta em um vetor $\mathbf{y} = c \mathbf{x} = (cx_1 , cx_2 , \cdots, cx_p)^t$, de igual dimensão em relação ao vetor original;


. . .

- Geometricamente, a multiplicação de um vetor por um escalar pode mudar seu tamanho e sentido, mas não sua direção.	




## Definição e propriedades básicas


- A soma de dois vetores $\mathbf{x}$ e $\mathbf{y}$, de iguais dimensões, resulta em um terceiro vetor dado por:
		

$$\mathbf{z} = \mathbf{x} + \mathbf{y} = (x_1 + y_1, x_2 + y_2, \cdots, x_p + y_p)^t$$
		
		

. . . 


- A diferença de dois vetores $\mathbf{x}$ e $\mathbf{y}$, de iguais dimensões, resulta em um terceiro vetor dado por:
		


$$\mathbf{w} = \mathbf{x} - \mathbf{y} = (x_1 - y_1, x_2 - y_2, \cdots, x_p - y_p)^t$$





## Definição e propriedades básicas




- O produto interno de dois vetores $\mathbf{x}$ e $\mathbf{y}$ é definido por:

				
$$\mathbf{v} = \mathbf{x}^t\mathbf{y} = \displaystyle{\sum_{i=1}^{p}} x_iy_i = x_1 y_1 + x_2 y_2 + \cdots + x_p y_p$$
		
. . .


- O tamanho do vetor $\mathbf{x} = (x_1, x_2, \cdots, x_p)^t$ é definido pela distância do ponto $p$-dimensional, determinado por suas coordenadas, à origem:
		
		
		
$$L_x = \sqrt{\mathbf{x}^t\mathbf{x}} = \sqrt{x_1^2 + x_2^2 + \cdots + x_p^2}$$






## Definição e propriedades básicas


- O cosseno do ângulo $\theta$ entre os vetores $\mathbf{x}$ e $\mathbf{y}$ definidos em $\mathbb{R}^p$ é dado
		por:
		
		

$$\cos({\theta}) = \dfrac{\mathbf{x}^t\mathbf{y}}{\sqrt{\mathbf{x}^t\mathbf{x}} \sqrt{\mathbf{y}^t\mathbf{y}}}$$
		


. . .
		
		
- Dois vetores $\mathbf{x}$ e $\mathbf{y}$ são \textbf{ortogonais} entre si se o ângulo $\theta$ entre eles é $90^o$, de tal forma que $\cos(\theta) = 0$, ou, de forma equivalente, $\mathbf{x}^t\mathbf{y} = 0$.
		
		


. . .


- A **normalização** de um vetor $\mathbf{x}$ corresponde à divisão de $\mathbf{x}$ por $L_x$, de tal forma que o vetor resultante tenha comprimento unitário:
		
		
$$\mathbf{x}^* = \dfrac{\mathbf{x}}{L_x}$$



## Definição e propriedades básicas



- A **projeção** de um vetor $\mathbf{x}$ em um vetor $\mathbf{y}$ é um novo vetor, com coordenadas:
		
		
		
$$\text{Projeção de } \mathbf{x} \text{ em } \mathbf{y} = \dfrac{\mathbf{x}^t\mathbf{y}}{\mathbf{y}^t\mathbf{y}} \mathbf{y}$$
		


. . .
		
		
- O comprimento da projeção de $\mathbf{x}$ em $\mathbf{y}$ é dado por:
		
		
		
$$\text{Tamanho da projeção de } \mathbf{x} \text{ em } \mathbf{y} = \dfrac{|\mathbf{x}^t\mathbf{y}|}{L_y} =  L_x \cos(\theta)$$





## Definição e propriedades básicas


- **Igualdade de matrizes:** Dizemos que duas matrizes $\mathbf{A}$ e $\mathbf{B}$ são iguais se elas tem iguais dimensões e $\{a_{ij}\} = \{b_{ij}\}$ para todo $i$ e para todo $j$.
				

. . .
		
		

- **Matriz transposta:** A transposta de uma matriz $\mathbf{A}_{n \times p}$ é a matriz $\mathbf{A}^t_{p \times n}$ tal que $\{a_{ij}\} = \{a_{ji}\}$ para todo $i$ e para todo $j$:
		
		
$$\mathbf{A}^t = \left[ \begin{array}{cccc} a_{11} & a_{21} & \cdots & a_{n1} \\ a_{12} & a_{22} & \cdots & a_{n2} \\ \vdots & \vdots & \ddots & \vdots \\ 
		a_{1p} & a_{2p} & \cdots & a_{np} \end{array} \right]$$
		



## Definição e propriedades básicas



- **Matriz simétrica:** Dizemos que uma matriz $\mathbf{A}_{p \times p}$ é simétrica se $\{a_{ij}\} = \{a_{ji}\}$ para todo $i$ e para todo $j$, ou seja, $\mathbf{A}^t = \mathbf{A}$.



. . .




- **Diagonal de uma matriz:** A diagonal de uma matriz quadrada $\mathbf{A}_{p \times p}$ corresponde ao conjunto de elementos $a_{11}, a_{22}, \cdots, a_{pp}$.
		


. . .



- **Matriz diagonal:** Dizemos que a matriz quadrada $\mathbf{A}_{p \times p}$ é diagonal se todos os elementos fora da diagonal são iguais a zero:
		
		
		
$$\mathbf{A} = \left[ \begin{array}{cccc} a_{11} & 0 & \cdots & 0 \\ 0 & a_{22} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 
		0 & 0 & \cdots & a_{pp} \end{array} \right]$$
		
		
		
		
## Definição e propriedades básicas


- **Matriz identidade:** Dizemos que a matriz quadrada $\mathbf{I}_{p\times p}$ é uma matriz identidade se ela é uma matriz diagonal com todos os elementos da diagonal iguais a 1:
		
		
		
$$\mathbf{I} = \left[ \begin{array}{cccc} 1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 
		0 & 0 & \cdots & 1 \end{array} \right]$$
		
		
		
		
## Definição e propriedades básicas		


- **Matriz triangular superior:** Dizemos que a matriz quadrada $\mathbf{A}_{p \times p}$ é uma matriz triangular superior se todos os elementos abaixo da 	diagonal são iguais a zero:
		

$$\mathbf{A} = \left[ \begin{array}{cccc} a_{11} & a_{12} & \cdots & a_{1p} \\ 0 & a_{22} & \cdots & a_{2p} \\ \vdots & \vdots & \ddots & \vdots \\ 
0 & 0 & \cdots & a_{pp} \end{array} \right]$$


. . .


- Uma **matriz triangular inferior** é definida de forma semelhante.





## Operações envolvendo matrizes		


- A **soma** de duas matrizes $\mathbf{A}_{n \times p}$ e $\mathbf{B}_{n \times p}$ de **iguais dimensões** é a matriz resultante das somas dos elementos nas posições correspondentes:
		
		

$$\mathbf{A} + \mathbf{B} = \left[ \begin{array}{cccc} a_{11} + b_{11} & a_{12} + b_{12} & \cdots & a_{1p} + b_{1p}\\ a_{21} +  b_{21}& a_{22} + b_{22}& \cdots & a_{2p} + b_{2p} \\ \vdots & \vdots & \ddots & \vdots \\ 
		a_{n1} + b_{n1} & a_{n2} + b_{n2} & \cdots & a_{np} + b_{np} \end{array} \right]$$
		
		
		
		

## Operações envolvendo matrizes		


- A **diferença** de duas matrizes $\mathbf{A}_{n \times p}$ e $\mathbf{B}_{n \times p}$ de **iguais dimensões** é a matriz resultante das diferenças dos elementos nas posições correspondentes:
		
		
$$\mathbf{A} - \mathbf{B} = \left[ \begin{array}{cccc} a_{11} - b_{11} & a_{12} - b_{12} & \cdots & a_{1p} - b_{1p}\\ a_{21} -  b_{21}& a_{22} - b_{22}& \cdots & a_{2p} - b_{2p} \\ \vdots & \vdots & \ddots & \vdots \\ 		a_{n1} - b_{n1} & a_{n2} - b_{n2} & \cdots & a_{np} - b_{np} \end{array} \right]$$




## Operações envolvendo matrizes


- Sejam $\mathbf{A}_{n \times k}$ e $\mathbf{B}_{k \times p}$ duas matrizes, tais que o número de linhas da segunda é igual ao número de colunas da primeira. O **produto** $\mathbf{AB}$ é definido por:
		
		
		
$$\mathbf{A} \mathbf{B} = \left[ \begin{array}{cccc} \sum_{r = 1}^k a_{1r}. b_{r1} & \sum_{r = 1}^k a_{1r}. b_{r2} & \cdots & \sum_{r = 1}^k a_{1r}. b_{rp}\\ \sum_{r = 1}^k a_{2r}. b_{r1} & \sum_{r = 1}^k a_{2r}. b_{r2} & \cdots & \sum_{r = 1}^k a_{2r}. b_{rp} \\ \vdots & \vdots & \ddots & \vdots \\ 
		\sum_{r = 1}^k a_{nr}. b_{r1} & \sum_{r = 1}^k a_{nr}. b_{r2} & \cdots & \sum_{r = 1}^k a_{nr}. b_{rp} \end{array} \right]$$
		

. . .


- Dizemos que uma matriz quadrada $\mathbf{Q}$ é **ortogonal** se $\mathbf{QQ}^t = \mathbf{Q}^t \mathbf{Q} = \mathbf{I}$.




## Operações envolvendo matrizes


- Sejam $\mathbf{A}_{n \times p}$ e $c$ uma constante. O produto $c \mathbf{A}$ resulta no produto de	cada elemento de $\mathbf{A}$ por $c$:
		
		
		
		
$$c\mathbf{A} = \left[ \begin{array}{cccc} ca_{11} & ca_{12} & \cdots & ca_{1p} \\ ca_{21} & ca_{22} & \cdots & ca_{2p} \\ \vdots & \vdots & \ddots & \vdots \\ 
	ca_{n1} & ca_{n2} & \cdots & ca_{np} \end{array} \right]$$
	
	
	
	
	

## Operações envolvendo matrizes


- Sejam $\mathbf{A}$, $\mathbf{B}$ e $\mathbf{C}$ matrizes com dimensões compatíveis para as operações consideradas. Então:

    - $(\mathbf{A}^t)^t = \mathbf{A}$;
    - $\mathbf{A} + \mathbf{B} = \mathbf{B} + \mathbf{A}$;
    - $(\mathbf{A} + \mathbf{B})^t = \mathbf{A}^t + \mathbf{B}^t$;
    - $(\mathbf{A} - \mathbf{B})^t = \mathbf{A}^t - \mathbf{B}^t$;
    - $(\mathbf{AB})^t = \mathbf{B}^t \mathbf{A}^t$;
    - $\mathbf{AB} \neq \mathbf{BA}$, a menos de situações bem específicas;





## Operações envolvendo matrizes


- Sejam $\mathbf{A}$, $\mathbf{B}$ e $\mathbf{C}$ matrizes com dimensões compatíveis para as operações consideradas. Então:

    - $\mathbf{A}(\mathbf{B} + \mathbf{C}) = \mathbf{AB} + \mathbf{AC}$, valendo o mesmo ao substituir a soma pela diferença;
    - $(\mathbf{A} + \mathbf{B}) \mathbf{C} = \mathbf{AC} + \mathbf{BC}$, valendo o mesmo ao substituir a soma pela diferença;
    - $(\mathbf{A} + \mathbf{B})\mathbf{C} \neq \mathbf{CA} + \mathbf{BA}$, a menos de situações bem específicas;
    - $\mathbf{IA} = \mathbf{AI} = \mathbf{A}$, para qualquer $\mathbf{A}$.
    
    
    
## Operações envolvendo matrizes


- O **traço** de uma matriz de uma matriz $\mathbf{A}_{p \times p}$, denotado por $\text{tr}(\mathbf{A})$, corresponde à soma dos elementos da diagonal de $\mathbf{A}$:
		
		
$$\text{tr}(\mathbf{A}) = \displaystyle{\sum_{i=1}^{p}a_{ii}}$$
		

. . .


- Sejam $\mathbf{A}$ e $\mathbf{B}$ matrizes quadradas. Então:
	
	 - $\text{tr}(\mathbf{A} + \mathbf{B}) = \text{tr}(\mathbf{A}) + \text{tr}(\mathbf{B})$
	 - $\text{tr}(\mathbf{A} \mathbf{B}) = \text{tr}(\mathbf{B} \mathbf{A})$




## Combinações lineares e formas quadráticas


- Para um conjunto de constantes $a_1, a_2, \cdots, a_p$, o vetor $\mathbf{y} = a_1 \mathbf{x}_1 + a_2 \mathbf{x}_ 2 + \cdots + a_p \mathbf{x}_p$ é uma **combinação linear** dos vetores $\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_p$ .
		

. . .


- O conjunto de vetores $\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_p$ é dito **linearmente dependente** se há um conjunto de constantes $a_1, a_2, \cdots, a_p$, **nem todas nulas**, tal que:
		

$$a_1 \mathbf{x}_1 + a_2 \mathbf{x}_ 2 + \cdots + a_p \mathbf{x}_p = 0$$

. . .
		

- Caso contrário os vetores são **linearmente independentes**.





## Combinações lineares e formas quadráticas


- **Formas quadráticas** surgem de forma recorrente na estatística	multivariada, por exemplo, na definição de distâncias.
		
		
. . .


- Uma **forma quadrática**, definida a partir de uma matriz simétrica $\mathbf{A}_{p \times p}$, é definida como:
		
		

$$Q(\mathbf{x}) = {\mathbf{x}^t} \mathbf{A} \mathbf{x} =\displaystyle{\sum_{i=1}^p a_{ii} x_{i}^2} + 2 \displaystyle{\sum_{i = 1}^{p-1}} \displaystyle{\sum_{k = i+1}^{p}} a_{ik} x_i  x_k  = \displaystyle{\sum_{i=1}^p} \displaystyle{\sum_{k=1}^p} a_{ik} x_i  x_k$$ 
		
para $\mathbf{x} \neq \mathbf{0}$ definido em $\mathbb{R}^p$.



## Combinações lineares e formas quadráticas


- Classificamos a matriz $\mathbf{A}$, e a consequente forma quadrática $\mathbf{x}^t \mathbf{A}\mathbf{x}$, como **positiva definida** se $Q(\mathbf{x}) > 0$ para qualquer $\mathbf{x} \neq \mathbf{0}$.
		

. . .


		
- Outras classificações:
		
    - **Positiva semidefinida:**  $Q(\mathbf{x}) \geqslant 0$
    - **Negativa definida:**  $Q(\mathbf{x}) < 0$ 
    - **Negativa semidefinida:**  $Q(\mathbf{x}) \leqslant 0$ 
    - **Indefinida:**  $Q(\mathbf{x}) > 0$ para alguns $\mathbf{x} \in \mathbb{R}^p$ e $Q(\mathbf{x}) < 0$ para outros $\mathbf{x} \in \mathbb{R}^p$
    


## Matriz inversa

- **Matriz inversa:** Considere uma matriz $\mathbf{A}_{p \times p}$. Caso exista uma matriz $\mathbf{B}_{p \times p}$ tal que
		
		
$$\mathbf{AB} = \mathbf{BA} = \mathbf{I}$$


dizemos que $\mathbf{B}$ é a matriz inversa de $\mathbf{A}$, sendo usualmente denotada por $\mathbf{A}^{-1}$.



. . .


- Quando uma matriz possui uma matriz inversa, dizemos que ela é **não-singular**. Caso contrário, ela é classificada como **singular**.



## Matriz inversa


- A condição fundamental para que uma matriz tenha inversa é que suas colunas sejam linearmente independentes (matriz de $rank$ completo).
	
		
. . .
		
		
- O $rank$ de uma matriz $\mathbf{A}_{n \times p}$ , denotado por $rank(\mathbf{A})$, é definido como	o número de linhas (ou colunas) linearmente independentes de $\mathbf{A}$.
		
		
. . .
		
	
- Dizemos que a matriz quadrada $\mathbf{A}_{p \times p}$ tem $rank$ completo se $rank(\mathbf{A}) = p$, configurando uma matriz não singular.
		

. . .


- Para matrizes de $rank$ incompleto ou não-quadradas, define-se a inversa generalizada de $\mathbf{A}$ como a matriz $\mathbf{A}^-$ que satisfaz $\mathbf{A} \mathbf{A}^- \mathbf{A} = \mathbf{A}$.



## Matriz inversa

- A inversa de uma matriz diagonal é dada pela matriz diagonal composta pelos inversos dos elementos da matriz original:
		
		
$$\mathbf{A} = \left[ \begin{array}{cccc} a_{11} & 0 & \cdots & 0 \\ 0 & a_{22} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 
0 & 0 & \cdots & a_{pp} \end{array} \right]; \,\,\,\,\,\,\,\,\, \mathbf{A}^{-1} = \left[ \begin{array}{cccc} \frac{1}{a_{11}} & 0 & \cdots & 0 \\ 0 & \frac{1}{a_{22}} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \frac{1}{a_{pp}} \end{array} \right]$$





## Matriz inversa


- $\mathbf{A}$ e $\mathbf{B}$ não singulares $(p \times p)$, $(\mathbf{AB})^{-1} = \mathbf{B}^{-1} \mathbf{A}^{-1}$;
		

. . .
		

- Para $c$ uma constante real diferente de zero, $(c \mathbf{B})^{-1} = c^{-1}(\mathbf{A})^{-1}$;


. . .

		
- $(\mathbf{A}^t)^{-1} = (\mathbf{A}^{-1})^t$;
	


. . .



- Se $rank(\mathbf{A}) = p$ então $\mathbf{A}^{-1}$ existe;
	


. . .

	
- Se $\mathbf{A}$ é ortogonal, então $\mathbf{A}^{-1}$ existe, além do que $\mathbf{A}^{-1} = \mathbf{A}^t$;



. . .


- Se $\mathbf{B}$ é não singular, $\mathbf{AB} = \mathbf{CB}$ implica $\mathbf{A} = \mathbf{C}$.




## Determinante


- O **determinante** de uma matriz $\mathbf{A}_{p \times p}$ , denotado por $\det(\mathbf{A})$ ou $|\mathbf{A}|$, é definido como:
		

	
	$$\det(\mathbf{A}) = \begin{cases} a_{11} & \text{ se } p = 1 \\ \sum \limits_{j=1}^p a_{ij} |\mathbf{A}_{ij}| (-1)^{i+j} & \text{ se } p > 1\end{cases}$$
	


	
sendo $\mathbf{A}_{ij}$ a matriz $(p - 1) \times (p - 1)$ resultante da exclusão da $i$-ésima linha e $j$-ésima coluna de $\mathbf{A}$.



## Determinante


![](/images/RAL/det.jpg){fig-align="center"}




## Determinante


- Sejam as matrizes $\mathbf{A}$ e $\mathbf{B}$ quadradas de ordem $p$ e seja $c$ um escalar. Então,
		
    - $\left|c \mathbf{A}\right| = c^n \left| \mathbf{A} \right|$;
    - $\left|\mathbf{A}^t \right| = \left|\mathbf{A}\right|$;
    - $\left|\mathbf{A}^{-1} \right| = \displaystyle{\dfrac {1}{\left|\mathbf{A}\right|}} = \left|\mathbf{A}\right|^{-1}$;
    - Se $rank(\mathbf{A}) < p$ então $|\mathbf{A}| = 0$;
    - Se $rank(\mathbf{A}) = p$ então $|\mathbf{A}| \neq 0$;
    
    


## Determinante


- Sejam as matrizes $\mathbf{A}$ e $\mathbf{B}$ quadradas de ordem $p$ e seja $c$ um escalar. Então,
		
    - $\left| \mathbf{AB} \right| = \left|\mathbf{A}\right| \left|\mathbf{B}\right|$;
    - $\left| \mathbf{ABA}^{-1} \right| = \left|\mathbf{A}\right| \left|\mathbf{B}\right|  \left|\mathbf{A}^{-1}\right|$;
    - Se $\mathbf{A}$ é uma matriz diagonal, então $|\mathbf{A}| = \displaystyle{\prod_{i=1}^p a_{ii}}$;
    - Se uma matriz $\mathbf{A}$ é singular, então $\left| \mathbf{A} \right| = 0$;
    - Se uma matriz $\mathbf{A}$ é não-singular, então $\left| \mathbf{A} \right| \neq 0$;
    - Se uma matriz $\mathbf{A}$ é positiva definida, então $\left| \mathbf{A} \right| > 0$.
    
    

## Autovalores e autovetores


- Seja $\mathbf{A}$ uma matriz quadrada e $\mathbf{I}$ a matriz identidade, ambas $p \times p$. Os escalares $\lambda_1, \lambda_2, \cdots, \lambda_p$ que são a solução da equação polinomial $|\mathbf{A} - \lambda \mathbf{I}| = 0$ são chamados **autovalores** (ou valores característicos) de $\mathbf{A}$.
		

. . .


- A equação $|\mathbf{A} - \lambda \mathbf{I}| = 0$ (como função de $\lambda$) é chamada **equação característica**.
	

. . .

		
- Seja $\mathbf{A}$ uma matriz quadrada $p \times p$ e $\lambda$ um autovalor de $\mathbf{A}$. Então, o	vetor $\mathbf{x}$ $(p \times 1)$, não nulo, que satisfaz:
		

$$\mathbf{Ax} = \lambda \mathbf{x}$$
		
é chamado **autovetor** (ou vetor característico) de $\mathbf{A}$ associado ao autovalor $\lambda$.




## Autovalores e autovetores


- Para qualquer matriz simétrica $\mathbf{A}$ com autovalores $\lambda_1, \lambda_2, \cdots, \lambda_p$, valem:
		  
		

$$\text{tr}(\mathbf{A}) = \displaystyle{\sum_{i=1}^{p}\lambda_{i}} \hspace{1cm} \text{e} \hspace{1cm} \left|\mathbf{A} \right| = \displaystyle{\prod_{i=1}^{p}\lambda_{i}}$$
		 

	

. . .


	
- Se todos os autovalores da matriz $\mathbf{A}$ são positivos maiores que zero, então a matriz $\mathbf{A}$ é positiva definida;
	


. . .


	
- Se os autovalores da matriz $\mathbf{A}$ são positivos ou iguais a zero, então a matriz $\mathbf{A}$ é positiva semidefinida. Neste caso, o número de autovalores positivos será igual ao posto da matriz $\mathbf{A}$
	

. . .


	
- Os autovetores de uma matriz $\mathbf{A}$ simétrica de dimensão $p \times p$ são ortogonais.	




## Teorema da decomposição espectral


- Como resultado da ortogonalidade dos autovetores de $\mathbf{A}$ tem-se o	**Teorema da Decomposição Espectral**.
		
		
. . .


		
- Toda matriz simétrica $\mathbf{A}$ de ordem $p \times p$ pode ser decomposta em:


$$\mathbf{A} = \mathbf{C} \mathbf{\Lambda} \mathbf{C}^t = \displaystyle{\sum_{i = 1}^p \lambda_i {\mathbf{e}_i \mathbf{e}_i^t}}$$		



## Teorema da decomposição espectral



em que $\mathbf{\Lambda}$ é a matriz diagonal dos autovalores:
		

$$\mathbf{\Lambda} = \left[ \begin{array}{cccc} \lambda_1 & 0 & \cdots & 0 \\ 0 & \lambda_2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \lambda_p \end{array} \right]$$


e $\mathbf{C}$ é a matriz ortogonal com os autovetores normalizados de $\mathbf{A}$ nas colunas:
	

$$\mathbf{C} = \left[\begin{array}{rrrr} \mathbf{e}_1 & \mathbf{e}_2 & \cdots & \mathbf{e}_p \end{array} \right]$$


