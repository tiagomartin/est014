[
  {
    "objectID": "programacao/semana-5.html",
    "href": "programacao/semana-5.html",
    "title": "Semana 05",
    "section": "",
    "text": "Distribuição Normal Multivariada"
  },
  {
    "objectID": "programacao/semana-5.html#slides",
    "href": "programacao/semana-5.html#slides",
    "title": "Semana 05",
    "section": "",
    "text": "Distribuição Normal Multivariada"
  },
  {
    "objectID": "programacao/semana-3.html",
    "href": "programacao/semana-3.html",
    "title": "Semana 03",
    "section": "",
    "text": "Introdução à Estatística Multivariada"
  },
  {
    "objectID": "programacao/semana-3.html#slides",
    "href": "programacao/semana-3.html#slides",
    "title": "Semana 03",
    "section": "",
    "text": "Introdução à Estatística Multivariada"
  },
  {
    "objectID": "programacao/semana-1.html",
    "href": "programacao/semana-1.html",
    "title": "Semana 01",
    "section": "",
    "text": "Sejam bem-vindos à disciplina EST014 - Estatística Multivariada I.\nLeiam com atenção o plano de ensino da disciplina. Nele estão as regras do jogo!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estatística Multivariada I",
    "section": "",
    "text": "Página dedicada à disciplina EST014 - Estatística Multivariada I\n\n\nEstatística Multivariada I é uma disciplina que apresenta o estudo de técnicas estatísticas voltadas para a análise simultânea de múltiplas variáveis. O curso abrange temas como análise de componentes principais, análise fatorial, análise de agrupamentos e análise discriminante. Essas metodologias são amplamente aplicadas em diversas áreas do conhecimento, permitindo a extração de informações relevantes a partir de conjuntos de dados complexos. O objetivo da disciplina é fornecer tanto a fundamentação teórica quanto a aplicação prática dessas técnicas, capacitando os alunos a utilizar ferramentas estatísticas avançadas em suas pesquisas e projetos.\n\n\nEmenta\nRevisão de Álgebra Matricial. Introdução à Estatística Multivariada. Distribuição Normal Multivariada. Análise de Componentes Principais. Análise Fatorial. Análise de Conglomerados ou Agrupamentos. Análise Discriminante.\n\n\nConteúdo Programático\n\nRevisão de Álgebra Matricial: matrizes e vetores. Operações com matrizes. Inversão matricial. Formas quadráticas. Autovalores e autovetores. Teorema da decomposição espectral. Determinante.\nIntrodução à Estatística Multivariada: exemplos de aplicação. Definição de Vetores Aleatórios, Vetores de Médias e Matrizes de Covariâncias e Correlação. Interpretação destas Matrizes. Vetores de Médias Amostrais e Matrizes Covariâncias e Correlações Amostrais. Variância Generalizada e Variância Total. Distâncias: Euclidiana, Euclidiana padronizada e Mahalanobis.\nDistribuição Normal Multivariada: função densidade. Propriedades. Distribuição Normal Bivariada. Elipsóides de concentração. Métodos práticos de verificação da hipótese de normalidade multivariada.\nAnálise de Componentes Principais: construção das Componentes Principais pela Matriz de Covariância e pela Matriz de Correlação. Proporção da Variância Total Explicada pelas Componentes. Estimação das Componentes Principais e dos Escores. Exemplos Práticos de Aplicação.\nAnálise Fatorial: apresentação teórica da metodologia. Modelo de Fatores Ortogonais. Estimação dos Fatores pelos Métodos de Componentes Principais, de Fatores Principais e de Máxima Verossimilhança. Rotação de Fatores: Rotações Ortogonais e Oblíquas. Estimação dos Escores dos Fatores: Método de Mínimos Quadrados e Método de Regressão. Exemplos Práticos de Aplicação.\nAnálise de Conglomerados ou Agrupamentos: discussão dos vários Métodos de Formação de Conglomerados, Variáveis Quantitativas e Qualitativas. Métodos Hierárquicos: Método de Ligação Simples (Single Linkage), de Ligação Completa (Complete Linkage), de Ligação Média (Average Linkage), do Centróide, e de Ward. Métodos para encontrar o Número de Conglomerados Ótimo da Partição. Métodos Não Hierárquicos: Método das K-Médias (KMeans). Exemplos Práticos de Aplicação.\nAnálise Discriminante: discriminação e classificação em 2 grupos. Estimação das Probabilidades de Erro de Classificação. Discriminação e Classificação Multivariada. Função Discriminante de Fischer. Exemplos Práticos de Aplicação.\n\n\n\nHorário de Aulas\nNeste semestre, as aulas da disciplina serão ministradas no LABEST II.\n\n\n\nDia\nHorário\nLocal\n\n\n\n\nTerça-feira\n21:00 - 22:40\nLABEST III\n\n\nQuinta-feira\n19:00 - 20:40\nLABEST III\n\n\n\n\n\n\n\n\n\n\n\nReferências Bibliográficas\n\nAnderson, T. W. 2009. AN INTRODUCTION TO MULTIVARIATE STATISTICAL ANALYSIS, 3RD ED. Wiley India Pvt. Limited.\n\n\nCORRAR, Luiz J., Edilson PAULO, and José M. DIAS FILHO. 2007. Análise Multivariada: Para Os Cursos de Administração, Ciências Contábeis e Economia. Editora Atlas.\n\n\nFávero, L. P., and P. Belfiore. 2017. Manual de análise de Dados: Estatı́stica e Modelagem Multivariada Com Excel, SPSS e Stata. Elsevier Editora Ltda.\n\n\nFerreira, D. F. 2018. Estatística Multivariada. Editora UFLA.\n\n\nHair, J. F., W. C. Black, B. J. Babin, R. E. Anderson, and R. L. Tatham. 2009. Análise Multivariada de Dados - 6ed. Bookman.\n\n\nJohnson, R. A., and D. W. Wichern. 2007. Applied Multivariate Statistical Analysis. Applied Multivariate Statistical Analysis. Pearson Prentice Hall.\n\n\nLATTIN, James, J. Douglas CARROLL, and Paul E. GREEN. 2011. Análise de Dados Multivariados. CENGAGE Learning.\n\n\nMingoti, Sueli. 2005. Análise de Dados Através de Métodos de Estatística Multivariada: Uma Abordagem Aplicada.\n\n\nRencher, A. C., and W. F. Christensen. 2012. Methods of Multivariate Analysis. Wiley Series in Probability and Statistics. Wiley."
  },
  {
    "objectID": "exercicios/lista03.html",
    "href": "exercicios/lista03.html",
    "title": "Lista de exercícios 03: Distribuição Normal Multivariada",
    "section": "",
    "text": "Data de entrega: 24 de novembro de 2025\n\n\n\nSuponha que \\(\\mathbf{x} \\sim N_3(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), onde\n\n\\[\n\\boldsymbol{\\mu} = \\left[ \\begin{array}{l} 3 \\\\ 1 \\\\ 4 \\end{array} \\right],\n\\boldsymbol{\\Sigma} = \\left[ \\begin{array}{rrr} 6 & 1 & -2\\\\ 1 & 13 & 4 \\\\ -2 & 4 & 4\n\\end{array} \\right]\n\\]\n\nEncontre a distribuição de \\(z = 2x_1 - x_2 + 3x_3\\)\nEncontre a distribuição conjunta de \\(z_1 = x_1 + x_2 + x_3\\) e \\(z_2 = x_1 - x_2 + 2x_3\\)\nEncontre a distribuição marginal de \\(x_2\\)\nEncontre a distribuição de \\([x_1 \\hspace{0.2cm} x_3]^t\\)\nEncontre a distribuição conjunta de \\(x_1\\), \\(x_3\\) e \\(3(x_1 + x_2)\\)\n\n\n\nConsidere uma população normal bivariada com \\(\\mu_1 = 0\\), \\(\\mu_2 = 2\\), \\(\\sigma_{11} = 2\\), \\(\\sigma_{22} = 1\\) e \\(\\rho_{12} = 0,5\\).\n\n\nEscreva a função densidade normal bivariada correspondente.\nEscreva a distância generalizada quadrática de Mahalanobis \\((\\mathbf{x} - \\boldsymbol{\\mu})^t \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x} - \\boldsymbol{\\mu})\\) em função de \\(x_1\\) e \\(x_2\\).\n\n\n\nSuponha que \\(\\mathbf{x} \\sim N_3(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), onde\n\n\\[\n\\boldsymbol{\\mu} = \\left[ \\begin{array}{r} 2 \\\\ -3 \\\\ 1 \\end{array} \\right],\n\\boldsymbol{\\Sigma} = \\left[ \\begin{array}{rrr} 1 & 1 & 1\\\\ 1 & 3 & 2 \\\\ 1 & 2 & 2\n\\end{array} \\right]\n\\]\n\nEncontre a distribuição de \\(z = 3x_1 - 2x_2 + x_3\\)\nEncontre a distribuição de \\(w = 2x_1\\)\nEncontre a distribuição conjunta de \\(\\mathbf{y} = \\left[ \\begin{array}{r} z \\\\ w \\end{array} \\right]\\)\n\n\n\nSuponha que \\(\\mathbf{x} \\sim N_3(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), com\n\n\\[\n\\boldsymbol{\\mu} = \\left[ \\begin{array}{r} -3 \\\\ 1 \\\\ 4 \\end{array} \\right],\n\\boldsymbol{\\Sigma} = \\left[ \\begin{array}{rrr} 1 & -2 & 0 \\\\ -2 & 5 & 0  \\\\ 0 & 0 & 2  \n\\end{array} \\right]\n\\]\nVerifique se as seguintes variáveis são independentes. JUSTIFIQUE!\n\n\\(X_1\\) e \\(X_2\\)\n\\(X_2\\) e \\(X_3\\)\n\\((X_1, X_2)\\) e \\(X_3\\)\n\\(\\displaystyle \\frac{X_1 + X_2}{2}\\) e \\(X_3\\)\n\\(X_2\\) e \\(\\displaystyle X_2 - \\frac{5}{2} X_1 - X_3\\)\n\n\n\nSeja \\(\\mathbf{x} \\sim N_3(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), com\n\n\\[\n\\boldsymbol{\\mu} = \\left[ \\begin{array}{r} 3 \\\\ 5 \\\\ 2 \\end{array} \\right],\n\\boldsymbol{\\Sigma} = \\left[ \\begin{array}{rrr} 25 & -2 & 4 \\\\ -2 & 4 &1  \\\\ 4& 1 &9  \n\\end{array} \\right]\n\\]\nDetermine a distribuição condicional de \\(X_1\\) dado \\(\\mathbf{x_2} = [4 \\hspace{0.2cm} 4]^t\\).\n\n\nSeja o vetor \\(\\mathbf{x}\\) com distribuição normal \\(N_3(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), sendo\n\n\\[\n\\boldsymbol{\\mu} = \\left[ \\begin{array}{r} 0 \\\\ 0 \\\\ 0 \\end{array} \\right],\n\\boldsymbol{\\Sigma} = \\left[ \\begin{array}{rrr} 5 & 0 & 0 \\\\ 0 & 7 & -1  \\\\ 0 & -1 & 2  \n\\end{array} \\right]\n\\]\nDetermine as distribuições marginal de \\(X_1\\) e condicional de \\(X_1\\) dado \\(\\mathbf{x_2} = [X_2 \\hspace{0.2cm} X_3]^t\\). O que podemos dizer a respeito dessas distribuições? Explique a razão desse fato."
  },
  {
    "objectID": "exercicios/lista01.html",
    "href": "exercicios/lista01.html",
    "title": "Lista de exercícios 01: Revisão de Álgebra Linear",
    "section": "",
    "text": "Data de entrega: 31 de outubro de 2025\n\n\n\nSejam as seguintes matrizes:\n\n\\[\\mathbf{A} = \\left(\\begin{array}{ccc} 4 & 2 & 3 \\\\ 7 & 5 & 8 \\end{array}\\right) \\text{     e     } \\mathbf{B} = \\left(\\begin{array}{ccc} 3 & -2 & 4\\\\ 6 & 9 & -5 \\end{array}\\right)\\]\n\nEncontre \\(\\mathbf{A + B}\\) e \\(\\mathbf{A- B}\\).\nEncontre \\(\\mathbf{AA}^t\\) e \\(\\mathbf{A}^t\\mathbf{A}\\).\nCalcule \\((\\mathbf{A+B})^t\\) e verifique que a matriz resultante é igual a \\(\\mathbf{A}^t + \\mathbf{B}^t\\).\nMostre que \\((\\mathbf{A}^t)^t = \\mathbf{A}\\).\n\n\n\nSejam as matrizes:\n\n\\[\\mathbf{A} = \\left(\\begin{array}{cc} 1 & 3 \\\\ 2 & -1 \\end{array}\\right)  \\text{      e      } \\mathbf{B} = \\left(\\begin{array}{cc} 2 & 0 \\\\ 1 & 5 \\end{array}\\right)\\]\n\nEncontre \\(\\mathbf{AB}\\) e \\(\\mathbf{BA}\\).\nEncontre \\(\\mathbf{|AB|}\\) e verifique se \\(\\mathbf{|AB| = |A| \\cdot |B|}\\).\nCalcule \\(\\mathbf{A+B}\\) e \\(\\mathbf{tr(A+B)}\\).\nCalcule \\(\\mathbf{tr(A)}\\) e \\(\\mathbf{tr(B)}\\) e verifique se \\(\\mathbf{tr(A+B) = tr(A) + tr(B)}\\).\nEncontre \\(\\mathbf{(AB)^t}\\) e verifique que \\(\\mathbf{(AB)^t = B^t A^t}\\).\n\n\n\nSejam as matrizes:\n\n\\[\\mathbf{A} = \\left(\\begin{array}{ccc}\n1 & 2& 3\\\\\n2 & -1 &1\n\\end{array}\\right)  \\text{      e      } \\mathbf{B} = \\left(\\begin{array}{cc}\n3 & -2 \\\\\n2 & 0 \\\\\n-1&1\n\\end{array}\\right)\\]\n\nEncontre \\(\\mathbf{AB}\\) e \\(\\mathbf{BA}\\).\nCalcule \\(\\mathbf{tr(AB)}\\) e \\(\\mathbf{tr(BA)}\\) e verifique se são iguais.\n\n\n\nSejam as matrizes:\n\n\\[\\mathbf{A} = \\left(\\begin{array}{ccc}\n1 & 2& 3\\\\\n2 & 4 & 6 \\\\\n5 & 10 & 15\n\\end{array}\\right)  \\text{      e      } \\mathbf{B} =  \\left(\\begin{array}{ccc}\n-1 &1& -2 \\\\\n-1 &1& -2 \\\\\n1&-1&2\n\\end{array}\\right)\\]\n\nMostre que \\(\\mathbf{AB = 0}\\).\nEncontre um vetor \\(\\mathbf{x}\\) tal que \\(\\mathbf{A} \\mathbf{x} = 0\\).\nMostre que \\(\\mathbf{|A|} = 0\\).\n\n\n\nSejam\n\n\\[\\mathbf{A} = \\left(\\begin{array}{ccc}\n1 & -1& 4\\\\\n-1 & 1 & 3 \\\\\n4 & 3 & 2\n\\end{array}\\right), \\mathbf{B} =  \\left(\\begin{array}{ccc}\n3 &-2& 4 \\\\\n7 &1& 0 \\\\\n2&3&5\n\\end{array}\\right), \\mathbf{x} = \\left(\\begin{array}{c}\n1 \\\\\n-1  \\\\\n2\n\\end{array}\\right), \\mathbf{y} = \\left(\\begin{array}{c}\n3  \\\\\n2  \\\\\n1\n\\end{array}\\right)\\]\nEncontre o que se pede:\n\n\\(\\mathbf{B} \\mathbf{x}\\)\n\\(\\mathbf{y^t B}\\)\n\\(\\mathbf{x^t A x}\\)\n\\(\\mathbf{x A y}\\).\n\\(\\mathbf{x y^t}\\).\n\\(\\mathbf{({x}-{y})^t A ({x}-{y})}\\)\n\n\n\nSejam\n\n\\[\\mathbf{A} = \\left(\\begin{array}{ccc}\n1 & 2& 3\\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{array}\\right)  \\text{      e      } \\mathbf{D} =  \\left(\\begin{array}{ccc}\na &0& 0 \\\\\n0 &b& 0 \\\\\n0&0&c\n\\end{array}\\right)\\]\nEncontre \\(\\mathbf{AD}\\), \\(\\mathbf{DA}\\) e \\(\\mathbf{DAD}\\)\n\n\nSejam\n\n\\[\\mathbf{A} = \\left(\\begin{array}{ccc} 1 & 3& 2\\\\ 2 & 0 & -1 \\end{array}\\right), \\mathbf{B} =  \\left(\\begin{array}{cc} 1 &2 \\\\ 0 &1 \\\\ 1&0 \\end{array}\\right), \\mathbf{C} = \\left(\\begin{array}{ccc} 2& 1& 1\\\\ 5 & -6 & -4 \\end{array}\\right)\\]\n\nEncontre \\(\\mathbf{AB}\\) e \\(\\mathbf{CB}\\). Elas são iguais?\nEncontre o posto das matrizes \\(\\mathbf{A}\\), \\(\\mathbf{B}\\) e \\(\\mathbf{C}\\).\n\n\n\nVerifique se as matrizes abaixo são ortogonais:\n\n\\[\\mathbf{A} =  \\displaystyle  \\frac{1}{169} \\cdot \\left(\\begin{array}{cc}\n5 & 12\\\\\n-12 & 5  \n\\end{array}\\right), \\mathbf{B} =  \\displaystyle  \\frac{1}{\\sqrt{2}} \\cdot \\left(\\begin{array}{cc}\n1 & 1\\\\\n1 & -1  \n\\end{array}\\right)\\]\n\n\nSeja a matriz\n\n\\[\\mathbf{A} =  \\left(\\begin{array}{cc}\n1 & 2\\\\\n2 & -2  \n\\end{array}\\right)\\]\n\nMostre que \\(\\mathbf{A}\\) é simétrica.\nObtenha os autovalores e autovetores da matriz \\(\\mathbf{A}\\).\nMostre que os autovetores são ortogonais.\nEscreva a decomposição espectral de \\(\\mathbf{A}\\).\nObtenha \\(\\mathbf{A^{-1}}\\), seus autovalores e autovetores e a decomposição espectral. Relacione com a decomposição espectral de \\(\\mathbf{A}\\).\nMostre que o determinante de \\(\\mathbf{A^{-1}}\\) é o inverso do determinante de \\(\\mathbf{A}\\).\nMostre que o determinante de \\(\\mathbf{A}\\) é o produto dos autovalores.\nEncontre a forma quadrática da matriz \\(\\mathbf{A}\\) e classifique-a.\nMostre que \\(\\mathbf{(A^t)^{-1} = (A^{-1})^t}\\).\n\n\n\nSeja a matriz\n\n\\[\\mathbf{A} =   \\left(\\begin{array}{ccc}\n3 & 6 &-1\\\\\n6 & 9 & 4 \\\\\n-1& 4& 3  \n\\end{array}\\right)\\]\n\nEncontre a decomposição espectral de \\(\\mathbf{A}\\).\nEncontre a decomposição espectral de \\(\\mathbf{A^2}\\) e mostre que a matriz diagonal de autovalores é igual ao quadrado da matriz \\(\\mathbf{D}\\) encontrada na parte \\(a.\\)\nEncontre a decomposição espectral de \\(\\mathbf{A^{-1}}\\) e mostre que a matriz diagonal de autovalores é igual à inversa da matriz \\(\\mathbf{D}\\) encontrada na parte \\(a.\\)\n\n\n\nDados os vetores \\(\\mathbf{x^t} = [1 \\hspace{0.5cm} 3]\\) e \\(\\mathbf{y^t} = [2 \\hspace{0.5cm} -5]\\):\n\n\nObtenha a norma de \\(\\mathbf{x}\\) e de \\(\\mathbf{y}\\).\nObtenha o ângulo e a distância entre esses vetores.\nObtenha a distância entre \\(\\mathbf{x}\\) e \\(\\mathbf{y}\\) na métrica\n\n\\[\\mathbf{A} =   \\left(\\begin{array}{cc}\n4 &0\\\\\n0 & 2  \n\\end{array}\\right)\\]\n\nObtenha a distância entre \\(\\mathbf{x}\\) e \\(\\mathbf{y}\\) na métrica\n\n\\[\\mathbf{A} =   \\left(\\begin{array}{cc}\n4 &2\\\\\n2 & 2  \n\\end{array}\\right)\\]\n\n\nUma forma quadrática \\(\\mathbf{x^t A x}\\) é dita ser positiva definida se a matriz \\(\\mathbf{A}\\) é positiva definida. A forma quadrática \\(3x_1^2 + 3x_2^2 - 2x_1x_2\\) é positiva definida?"
  },
  {
    "objectID": "aulas.html",
    "href": "aulas.html",
    "title": "Aulas",
    "section": "",
    "text": "Revisão de Álgebra Matricial\nIntrodução à Estatística Multivariada\nDistribuição Normal Multivariada\nAnálise de Componentes Principais\nAnálise Fatorial\nAnálise de Conglomerados ou Agrupamentos\nAnálise Discriminante",
    "crumbs": [
      "Aulas"
    ]
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#análise-estatística-multivariada",
    "href": "aulas/intro_est_multi/intro_est_multi.html#análise-estatística-multivariada",
    "title": "Introdução à Estatística Multivariada",
    "section": "Análise Estatística Multivariada",
    "text": "Análise Estatística Multivariada\n\nA capacidade de coleta e armazenamento de dados tem aumentado significativamente ao longo do tempo, tornando cada vez maior a quantidade de informações (variáveis) que se dispõe sobre cada indivíduo.\n\n\n\nAssim, tem-se a necessidade de transformar essa grande quantidade de dados em conhecimento, que possa fundamentar a compreensão de diferentes fenômenos e subsidiar tomadas de decisões.\n\n\n\n\nNesse contexto, técnicas de análise de dados que permitam explorar e compreender as relações existentes entre múltiplas variáveis tornam-se essenciais na análise"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#definição-de-análise-multivariada",
    "href": "aulas/intro_est_multi/intro_est_multi.html#definição-de-análise-multivariada",
    "title": "Introdução à Estatística Multivariada",
    "section": "Definição de Análise Multivariada",
    "text": "Definição de Análise Multivariada\n\n\n\n\nAnálise Multivariada\n\n\nA análise multivariada contempla um conjunto de métodos estatísticos utilizados na análise conjunta de múltiplas variáveis avaliadas nos indivíduos sob estudo."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#principais-objetivos-da-análise-multivariada",
    "href": "aulas/intro_est_multi/intro_est_multi.html#principais-objetivos-da-análise-multivariada",
    "title": "Introdução à Estatística Multivariada",
    "section": "Principais objetivos da Análise Multivariada",
    "text": "Principais objetivos da Análise Multivariada\nMétodos de Análise Multivariada podem ser aplicados para diversas finalidades, dentre as quais podemos destacar:\n\n\nRedução ou simplificação de dados;\n\n\n\n\nClassificação;\n\n\n\n\nAgrupamento;\n\n\n\n\ndentre outros…"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#exemplos-de-aplicações",
    "href": "aulas/intro_est_multi/intro_est_multi.html#exemplos-de-aplicações",
    "title": "Introdução à Estatística Multivariada",
    "section": "Exemplos de aplicações",
    "text": "Exemplos de aplicações\n\n\nRedução ou simplificação de dados\n\nDados referentes aos sintomas de determinada doença e limitações relatadas pelos pacientes, decorrentes da doença ou do tratamento, podem ser usados para a elaboração de um índice de qualidade de vida;\nDiferentes indicadores demográficos e sócio-econômicos podem ser usados para a elaboração de um gráfico, em duas dimensões, em que proximidade entre pontos (representando bairros de um município, por exemplo) configure similaridade entre eles."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#exemplos-de-aplicações-1",
    "href": "aulas/intro_est_multi/intro_est_multi.html#exemplos-de-aplicações-1",
    "title": "Introdução à Estatística Multivariada",
    "section": "Exemplos de aplicações",
    "text": "Exemplos de aplicações\n\n\nAgrupamento\n\nDados cadastrais podem ser utilizados com o objetivo de definir grupos de clientes de uma loja de departamentos similares quanto a informações disponíveis em suas fichas;\nVariáveis referentes à contabilidade de indústrias no último ano (gastos com mão de obra, investimento em matéria prima, produção…) podem ser usadas para agrupá-las em clusters de indústrias com características contábeis similares."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#exemplos-de-aplicações-2",
    "href": "aulas/intro_est_multi/intro_est_multi.html#exemplos-de-aplicações-2",
    "title": "Introdução à Estatística Multivariada",
    "section": "Exemplos de aplicações",
    "text": "Exemplos de aplicações\n\n\nClassificação\n\nResultados de diversas variáveis psicológicas e comportamentais podem ser usados para criar uma regra de discriminação de usuários de drogas que reincidem, após período de abstinência, daqueles que não reincidem;\nVariáveis referentes à anatomia de moscas (comprimento de asas, peso, coloração…) podem ser usadas para discriminar moscas em uma de quatro espécies distintas segundo os valores apresentados para tais variáveis."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#disposição-dos-dados-em-uma-análise-multivariada",
    "href": "aulas/intro_est_multi/intro_est_multi.html#disposição-dos-dados-em-uma-análise-multivariada",
    "title": "Introdução à Estatística Multivariada",
    "section": "Disposição dos dados em uma análise multivariada",
    "text": "Disposição dos dados em uma análise multivariada\n\nNuma análise multivariada, dispõe-se, em geral, de uma amostra de \\(n\\) indivíduos, com \\(p &gt; 1\\) variáveis avaliadas em cada um deles.\n\n\n\nO uso de técnicas multivariadas permite analisar simultaneamente as \\(p\\) variáveis.\n\n\n\n\nOs métodos multivariados consistem, muitas vezes, de generalizações de procedimentos univariados utilizados para fins semelhantes."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#disposição-dos-dados-em-uma-análise-multivariada-1",
    "href": "aulas/intro_est_multi/intro_est_multi.html#disposição-dos-dados-em-uma-análise-multivariada-1",
    "title": "Introdução à Estatística Multivariada",
    "section": "Disposição dos dados em uma análise multivariada",
    "text": "Disposição dos dados em uma análise multivariada\n\nVamos denotar por \\(x_{ij}\\) o valor da variável \\(j\\) verificado no indivíduo \\(i\\), \\(i = 1, 2, \\cdots, n\\); \\(j = 1, 2, \\cdots, p\\).\n\n\n\nA disposição usual dos dados é na forma convencional, entrando com os indivíduos nas linhas e as variáveis nas colunas.\n\n\n\n\n\n\n\n\n\n\nVar 1\n\n\nVar 2\n\n\n\\(\\cdots\\)\n\n\nVar j\n\n\n\\(\\cdots\\)\n\n\nVar p\n\n\n\n\n\n\nInd 1\n\n\n\\(x_{11}\\)\n\n\n\\(x_{12}\\)\n\n\n\\(\\cdots\\)\n\n\n\\(x_{1j}\\)\n\n\n\\(\\cdots\\)\n\n\n\\(x_{1p}\\)\n\n\n\n\nInd 2\n\n\n\\(x_{21}\\)\n\n\n\\(x_{22}\\)\n\n\n\\(\\cdots\\)\n\n\n\\(x_{2j}\\)\n\n\n\\(\\cdots\\)\n\n\n\\(x_{2p}\\)\n\n\n\n\n\\(\\vdots\\)\n\n\n\\(\\vdots\\)\n\n\n\\(\\vdots\\)\n\n\n\\(\\vdots\\)\n\n\n\\(\\vdots\\)\n\n\n\\(\\vdots\\)\n\n\n\\(\\vdots\\)\n\n\n\n\nInd i\n\n\n\\(x_{i1}\\)\n\n\n\\(x_{i2}\\)\n\n\n\\(\\cdots\\)\n\n\n\\(x_{ij}\\)\n\n\n\\(\\cdots\\)\n\n\n\\(x_{ip}\\)\n\n\n\n\n\\(\\vdots\\)\n\n\n\\(\\vdots\\)\n\n\n\\(\\vdots\\)\n\n\n\\(\\cdots\\)\n\n\n\\(\\vdots\\)\n\n\n\\(\\cdots\\)\n\n\n\\(\\vdots\\)\n\n\n\n\nInd n\n\n\n\\(x_{n1}\\)\n\n\n\\(x_{n2}\\)\n\n\n\\(\\cdots\\)\n\n\n\\(x_{nj}\\)\n\n\n\\(\\cdots\\)\n\n\n\\(x_{np}\\)"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#representação-matricial-dos-dados",
    "href": "aulas/intro_est_multi/intro_est_multi.html#representação-matricial-dos-dados",
    "title": "Introdução à Estatística Multivariada",
    "section": "Representação matricial dos dados",
    "text": "Representação matricial dos dados\n\nPara fins de apresentação e desenvolvimento da teoria, a representação matricial dos dados é necessária.\n\n\n\nA matriz dos dados tem dimensão \\(n \\times p\\), apresentando nas linhas os \\(n\\) indivíduos e nas colunas as \\(p\\) variáveis.\n\n\n\n\\[ \\mathbf{X}_{n \\times p} = \\left[ \\begin{array}{cccc} x_{11} & x_{12} & \\cdots & x_{1p} \\\\ x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n1} & x_{n2} & \\cdots & x_{np} \\end{array} \\right] \\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios",
    "href": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios",
    "title": "Introdução à Estatística Multivariada",
    "section": "Vetores aleatórios",
    "text": "Vetores aleatórios\n\nPara o tratamento probabilístico de dados multivariados, vamos relembrar a definição de vetor aleatório.\n\n\n\nUm vetor aleatório de dimensão \\(p\\) é um vetor em que cada um de seus \\(p\\) componentes é uma variável aleatória.\n\n\n\n\nVamos denotar um vetor aleatório \\(\\mathbf{x}\\) por \\(\\mathbf{x} = (X_1, X_2, \\cdots, X_p)^t\\). A função distribuição de probabilidade conjunta de \\(\\mathbf{x}\\) é definida como:\n\n\\[F(\\mathbf{x}) = P(X_1 \\leqslant x_1, X_2 \\leqslant x_2, \\cdots, X_p \\leqslant x_p)\\]\npara \\(\\mathbf{x} \\in \\mathbb{R}^p\\)"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-1",
    "href": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-1",
    "title": "Introdução à Estatística Multivariada",
    "section": "Vetores aleatórios",
    "text": "Vetores aleatórios\n\nSe \\(\\mathbf{x}\\) for contínua, então a função densidade de probabilidade \\(f(\\mathbf{x})\\) fica definida por:\n\n\\[f(\\mathbf{x}) = \\dfrac{\\partial F(\\mathbf{x})}{\\partial x_1 \\partial x_2 \\cdots \\partial x_p},\\]\nsatisfazendo\n\\[\\int \\limits_{-\\infty}^{\\infty} \\int \\limits_{-\\infty}^{\\infty} \\cdots \\int \\limits_{-\\infty}^{\\infty} f(x_1, x_2, \\cdots, x_p)dx_1 dx_2 \\cdots dx_p = 1\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-2",
    "href": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-2",
    "title": "Introdução à Estatística Multivariada",
    "section": "Vetores aleatórios",
    "text": "Vetores aleatórios\ne,\n\\[f(x_1, x_2, \\cdots, x_p) \\geqslant 0\\]\npara qualquer conjunto de valores \\(x_1, x_2, \\cdots, x_p\\)."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-3",
    "href": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-3",
    "title": "Introdução à Estatística Multivariada",
    "section": "Vetores aleatórios",
    "text": "Vetores aleatórios\n\nCada elemento de \\(\\mathbf{x} = (X_1, X_2, \\cdots, X_p)^t\\) é uma variável aleatória \\(X_i\\), \\(i = 1, 2, \\cdots, p\\), cuja distribuição (denominada distribuição marginal) fica determinada pela função densidade de probabilidade \\(f(x_i)\\):\n\n\\[f(x_i) = \\int \\limits_{x_1} \\int \\limits_{x_2} \\cdots \\int \\limits_{x_p} f(\\mathbf{x})dx_1 dx_2 \\cdots dx_p, \\,\\,\\,\\, j \\neq i\\]\n\n\nSe \\(X_1, X_2, \\cdots, X_p\\) forem independentes, então:\n\n\\[f(x_1, x_2, \\cdots, x_p) = \\prod \\limits_{i=1}^p f(x_i) = f(x_1) f(x_2) \\cdots f(x_p)\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-4",
    "href": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-4",
    "title": "Introdução à Estatística Multivariada",
    "section": "Vetores aleatórios",
    "text": "Vetores aleatórios\n\nA média e a variância de \\(X_i\\), ficam definidas como:\n\n\\[\\mu_i = E(X_i) = \\int \\limits_{-\\infty}^{\\infty} x_i f(x_i)dx_i\\]\n\\[\\sigma_{ii} = Var(X_i) = \\int \\limits_{-\\infty}^{\\infty} (x_i - \\mu_i)^2 f(x_i)dx_i\\]\npara \\(i = 1, 2, \\cdots, p\\)."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-5",
    "href": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-5",
    "title": "Introdução à Estatística Multivariada",
    "section": "Vetores aleatórios",
    "text": "Vetores aleatórios\n\nPara duas variáveis aleatórias \\(X_i\\) e \\(X_j\\), a covariância é um parâmetro da distribuição conjunta bivariada que mede a associação linear entre elas:\n\n\\[\\sigma_{ij} = Cov(X_i, X_j) = \\int \\limits_{-\\infty}^{\\infty} \\int \\limits_{-\\infty}^{\\infty} (x_i - \\mu_i)(x_j - \\mu_j) f(x_i, x_j)dx_idx_j\\]\n\n\nSe \\(X_i\\) e \\(X_j\\) forem independentes, então \\(Cov(X_i, X_j ) = 0\\)."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-6",
    "href": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-6",
    "title": "Introdução à Estatística Multivariada",
    "section": "Vetores aleatórios",
    "text": "Vetores aleatórios\n\nO coeficiente de correlação de \\(X_i\\) e \\(X_j\\) é definido como:\n\n\\[\\rho_{ij} = \\dfrac{\\sigma_{ij}}{\\sigma_i \\sigma_j}\\]\nsendo uma medida de associação linear adimensional, tal que \\(-1 \\leqslant \\rho \\leqslant 1\\).\n\n\nTodos os resultados equivalentes para o caso discreto são obtidos substituindo adequadamente as integrais por somas."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-7",
    "href": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-7",
    "title": "Introdução à Estatística Multivariada",
    "section": "Vetores aleatórios",
    "text": "Vetores aleatórios\n\nA esperança matemática de um vetor aleatório \\(\\mathbf{x} = (X_1, X_2, \\cdots, X_p)^t\\) é definida pelo vetor de mesma dimensão em que cada elemento corresponde à esperança matemática da respectiva variável.\n\n\\[ \\mathbf{\\mu} = E(\\mathbf{x}) =  E \\left( \\left[ \\begin{array}{c} X_{1}  \\\\ X_{2} \\\\ \\vdots\n\\\\ X_{p} \\end{array} \\right] \\right)  = \\left[ \\begin{array}{c} E(X_{1})  \\\\ E(X_{2}) \\\\ \\vdots\n\\\\ E(X_{p}) \\end{array} \\right]  = \\left[ \\begin{array}{c} \\mu_1  \\\\ \\mu_2 \\\\ \\vdots\n\\\\ \\mu_p \\end{array} \\right] \\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-8",
    "href": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-8",
    "title": "Introdução à Estatística Multivariada",
    "section": "Vetores aleatórios",
    "text": "Vetores aleatórios\n\nA matriz de variâncias e covariâncias de um vetor aleatório \\(\\mathbf{x} = (X_1, X_2, \\cdots, X_p)^t\\) é definida pela matriz \\(\\mathbf{\\Sigma}\\) dada por:\n\n\\[\\begin{eqnarray*} \\mathbf{\\Sigma} &=& E \\left[(\\mathbf{x} - \\mathbf{\\mu})(\\mathbf{x} -\n\\mathbf{\\mu})^t \\right] \\\\ &=&\n      \\left[ \\begin{array}{ccc} E(X_1 - \\mu_1)^2  & \\cdots & E(X_1 - \\mu_1)( X_p - \\mu_p) \\\\ E(X_2 - \\mu_2)(X_1 - \\mu_1)  & \\cdots & E(X_2 - \\mu_2)( X_p - \\mu_p)\\\\ \\vdots & \\ddots & \\vdots \\\\ E( X_p - \\mu_p)(X_1 - \\mu_1) & \\cdots & E( X_p - \\mu_p)^2 \\end{array} \\right]\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-9",
    "href": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-9",
    "title": "Introdução à Estatística Multivariada",
    "section": "Vetores aleatórios",
    "text": "Vetores aleatórios\n\nDe maneira semelhante, a matriz de correlações do vetor aleatório \\(\\mathbf{x}\\) fica dada por:\n\n\\[ \\mathbf{P} = \\left[ \\begin{array}{cccc} 1 & \\rho_{12} & \\cdots & \\rho_{1p} \\\\ \\rho_{21} & 1 & \\cdots & \\rho_{2p}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\rho_{p1} & \\rho_{p2} & \\cdots & 1\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-10",
    "href": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-10",
    "title": "Introdução à Estatística Multivariada",
    "section": "Vetores aleatórios",
    "text": "Vetores aleatórios\n\nA matriz de correlações pode ser determinada facilmente a partir da matriz de covariâncias por:\n\n\\[\\mathbf{P} = \\left(\\mathbf{V}^\\frac{1}{2}\\right)^{-1} \\mathbf{\\Sigma} \\left(\\mathbf{V}^\\frac{1}{2}\\right)^{-1}\\]\nsendo \\(\\mathbf{V}^\\frac{1}{2}\\) a matriz diagonal com elementos \\(\\sqrt{\\sigma_{11}}, \\sqrt{\\sigma_{22}}, \\cdots, \\sqrt{\\sigma_{pp}}\\)."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#propriedades-da-média-e-da-variância-de-vetores-aleatórios",
    "href": "aulas/intro_est_multi/intro_est_multi.html#propriedades-da-média-e-da-variância-de-vetores-aleatórios",
    "title": "Introdução à Estatística Multivariada",
    "section": "Propriedades da média e da variância de vetores aleatórios",
    "text": "Propriedades da média e da variância de vetores aleatórios\n\nSejam \\(\\mathbf{A}\\) e \\(\\mathbf{B}\\) matrizes matrizes e \\(c\\) um vetor de constantes (todos com dimensões compatíveis às operações apresentadas). Sejam \\(\\mathbf{x}\\) e \\(\\mathbf{y}\\) vetores aleatórios.\n\n\\(E(\\mathbf{x} + \\mathbf{y}) = E(\\mathbf{x}) + E(\\mathbf{y})\\);\n\\(E(\\mathbf{AxB}) = \\mathbf{A} E(\\mathbf{x}) \\mathbf{B}\\);\n\\(E(\\mathbf{Ax} + c) = \\mathbf{A} E(\\mathbf{x}) + c\\);\n\\(Cov(\\mathbf{x}) = E(\\mathbf{x} \\mathbf{x}^t) - \\mathbf{\\mu}_x \\mathbf{\\mu}_x^t\\);\n\\(Cov(c^t \\mathbf{x}) = c^t Cov(\\mathbf{x})c\\);\n\\(Cov(\\mathbf{A}^t \\mathbf{x} + c) = \\mathbf{A} Cov(\\mathbf{x})\\mathbf{A}^t\\)."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados",
    "href": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados",
    "title": "Introdução à Estatística Multivariada",
    "section": "Descrição de dados multivariados",
    "text": "Descrição de dados multivariados\n\nNa prática, todas as matrizes teóricas ( \\(\\mathbf{\\mu}\\), \\(\\mathbf{\\Sigma}\\), \\(\\mathbf{P}\\) e \\(\\mathbf{V}\\) ) são desconhecidas e precisam ser estimadas através de dados amostrais.\n\n\n\nSeja então, uma amostra aleatória multivariada de tamanho \\(n\\).\n\n\n\n\nPodemos calcular a média amostral separadamente para cada variável:\n\n\\[\\bar{x}_j = \\dfrac{1}{n} \\sum \\limits_{i=1}^n x_{ij}, \\,\\,\\,\\,\\,\\, j = 1, 2,  \\cdots, p\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-1",
    "href": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-1",
    "title": "Introdução à Estatística Multivariada",
    "section": "Descrição de dados multivariados",
    "text": "Descrição de dados multivariados\n\nO vetor de médias amostrais fica definido por:\n\n\\[\\bar{\\mathbf{x}} = \\left[ \\begin{array}{c} \\bar{x}_1  \\\\ \\bar{x}_2\\\\ \\vdots  \\\\ \\bar{x}_p \\end{array} \\right]\\]\n\n\nMatricialmente, temos \\(\\bar{\\mathbf{x}} = \\dfrac{1}{n} \\mathbf{X}^t \\mathbf{j}\\), sendo \\(\\mathbf{X}\\) a matriz de dados e \\(\\mathbf{j}\\) o vetor de 1’s de dimensão \\(n\\)."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-2",
    "href": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-2",
    "title": "Introdução à Estatística Multivariada",
    "section": "Descrição de dados multivariados",
    "text": "Descrição de dados multivariados\n\nA variância amostral para a \\(j\\)-ésima variável e a covariância amostral para as variáveis \\(X_j\\) e \\(X_k\\) são definidas, respectivamente, por:\n\n\\[ s_{jj} = \\frac{\\displaystyle\\sum _{i=1}^n (x_{ij}-\\bar{x_j})^2}{n-1} \\]\n\\[ s_{jk} = \\frac{\\displaystyle\\sum _{i=1}^n (x_{ij}-\\bar{x_j})(x_{ik}-\\bar{x_k})}{n-1}, \\hspace{0.5cm} j \\neq k\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-3",
    "href": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-3",
    "title": "Introdução à Estatística Multivariada",
    "section": "Descrição de dados multivariados",
    "text": "Descrição de dados multivariados\n\nA matriz de covariâncias amostral (simétrica) fica definida por:\n\n\\[\\mathbf{S}_{p \\times p} = \\left[ \\begin{array}{cccc} s_{11} & s_{12} & \\cdots & s_{1p} \\\\ s_{21} & s_{22} & \\cdots & s_{2p}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ s_{p1} & s_{p2} & \\cdots & s_{pp} \\end{array} \\right]  \\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-4",
    "href": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-4",
    "title": "Introdução à Estatística Multivariada",
    "section": "Descrição de dados multivariados",
    "text": "Descrição de dados multivariados\n\nPodemos expressar a matriz de covariâncias em termos dos vetores observados:\n\n\\[\\mathbf{S} = \\dfrac{1}{n-1} \\sum \\limits_{i=1}^n (\\mathbf{x}_i - \\bar{\\mathbf{x}})(\\mathbf{x}_i - \\bar{\\mathbf{x}})^t = \\dfrac{1}{n-1} \\left( \\sum \\limits_{i=1}^n \\mathbf{x}_i \\mathbf{x}_i^t - n \\bar{\\mathbf{x}}\\bar{\\mathbf{x}}^t \\right) \\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-5",
    "href": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-5",
    "title": "Introdução à Estatística Multivariada",
    "section": "Descrição de dados multivariados",
    "text": "Descrição de dados multivariados\n\nO coeficiente de correlação amostral entre as variáveis \\(X_j\\) e \\(X_k\\) é dado por:\n\n\\[r_{jk} = \\dfrac{s_{jk}}{\\sqrt{s_{jj}} \\sqrt{s_{kk}}} = \\dfrac{\\sum \\limits_{i=1}^n (x_{ij} - \\bar{x}_j)(x_{ik} - \\bar{x}_k)}{\\sqrt{\\sum \\limits_{i=1}^n (x_{ij} - \\bar{x}_j)^2} \\sqrt{\\sum \\limits_{i=1}^n (x_{ik} - \\bar{x}_k)^2}}\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-6",
    "href": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-6",
    "title": "Introdução à Estatística Multivariada",
    "section": "Descrição de dados multivariados",
    "text": "Descrição de dados multivariados\n\nA matriz de correlações amostrais (simétrica) é determinada pelos coeficientes calculados para cada par de variáveis.\n\n\\[\\mathbf{R}_{p \\times p} = \\left[ \\begin{array}{cccc} 1 & r_{12} & \\cdots & r_{1p} \\\\\nr_{21} & 1 & \\cdots & r_{2p}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nr_{p1} & r_{p2} & \\cdots & 1\n\\end{array} \\right]\n\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-7",
    "href": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-7",
    "title": "Introdução à Estatística Multivariada",
    "section": "Descrição de dados multivariados",
    "text": "Descrição de dados multivariados\n\nComo complemento às medidas descritivas apresentadas, o uso de gráficos permite extrair informações importantes dos dados. Alguns gráficos (e recursos adicionais):\n\nHistograma;\nBoxplot;\nDiagrama de dispersão;\nCorrelograma;\nMatrizes de gráficos;\nFaces de Chernoff;\nGráficos tridimensionais…"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#variância-generalizada-e-variância-total",
    "href": "aulas/intro_est_multi/intro_est_multi.html#variância-generalizada-e-variância-total",
    "title": "Introdução à Estatística Multivariada",
    "section": "Variância generalizada e variância total",
    "text": "Variância generalizada e variância total\n\nEm algumas situações, é de interesse exprimir a varição presente nos dados multivariados em um único valor.\n\n\n\nUma das alternativas para isso é a variância generalizada, que é definida como o determinante da matriz de covariâncias amostral.\n\n\\[\\text{Variância generalizada} = |\\mathbf{S}|\\]\n\n\n\nNaturalmente, por resumir toda a variação em um único valor, parte da informação referente à variação dos dados se perde nesse resumo, originando possíveis distorções."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#variância-generalizada-e-variância-total-1",
    "href": "aulas/intro_est_multi/intro_est_multi.html#variância-generalizada-e-variância-total-1",
    "title": "Introdução à Estatística Multivariada",
    "section": "Variância generalizada e variância total",
    "text": "Variância generalizada e variância total\n\nA variância generalizada pode ser definida ainda a partir das variáveis originais padronizadas \\((x_{ij} - \\bar{x}_j)/s_{jj}\\), contornando problema de diferentes escalas das variáveis. Como resultado da padronização, temos:\n\n\\[\\text{Variância generalizada das variáveis padronizadas} = |\\mathbf{R}|\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#variância-generalizada-e-variância-total-2",
    "href": "aulas/intro_est_multi/intro_est_multi.html#variância-generalizada-e-variância-total-2",
    "title": "Introdução à Estatística Multivariada",
    "section": "Variância generalizada e variância total",
    "text": "Variância generalizada e variância total\n\nAlgumas propriedades da variância generalizada:\n\nQuanto maior a variância de uma variável, maior sua contribuição para a variância generalizada;\nQuanto maior a correlação entre duas variáveis, menor a variância generalizada;\nCaso duas ou mais variáveis sejam perfeitamente correlacionadas a variância generalizada atingirá seu mínimo valor (zero);\nA variância generalizada será máxima caso as variáveis tenham correlação nula."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#variância-generalizada-e-variância-total-3",
    "href": "aulas/intro_est_multi/intro_est_multi.html#variância-generalizada-e-variância-total-3",
    "title": "Introdução à Estatística Multivariada",
    "section": "Variância generalizada e variância total",
    "text": "Variância generalizada e variância total\n\nUma das limitações da variância generalizada é o fato de diferentes estruturas de correlação produzirem, algumas vezes, igual variância generalizada.\n\n\n\nExemplo: Calcular, para cada matriz de covariâncias, a variância generalizada e a correlação entre as variáveis.\n\n\\[\\mathbf{S}_1 = \\left[ \\begin{array}{rr} 10 & 8  \\\\  8 & 10 \\end{array} \\right], \\,\\,\\,\\,\\,\\,\\,\\, \\mathbf{S}_2 = \\left[ \\begin{array}{rr} 10 & -8  \\\\  -8 & 10 \\end{array} \\right], \\,\\,\\,\\,\\,\\,\\,\\, \\mathbf{S}_3 = \\left[ \\begin{array}{rr} 9 & 0  \\\\  0 & 4 \\end{array} \\right]\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#variância-generalizada-e-variância-total-4",
    "href": "aulas/intro_est_multi/intro_est_multi.html#variância-generalizada-e-variância-total-4",
    "title": "Introdução à Estatística Multivariada",
    "section": "Variância generalizada e variância total",
    "text": "Variância generalizada e variância total\n\nOutra medida usada para resumir a variação total em um único valor é a variância total, definida por:\n\n\\[\\text{Variância total} = \\text{tr}(\\mathbf{S}) = s_{11} + s_{22} + \\cdots + s_{pp}\\]\n\n\nPor se basear apenas na diagonal de \\(\\mathbf{S}\\), a variância total claramente ignora a estrutura de correlação dos dados."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#distâncias",
    "href": "aulas/intro_est_multi/intro_est_multi.html#distâncias",
    "title": "Introdução à Estatística Multivariada",
    "section": "Distâncias",
    "text": "Distâncias\n\nBoa parte das técnicas multivariadas baseiam-se no conceito de distância.\n\n\n\nUsamos distâncias para medir quão semelhantes são dois indivíduos com relação aos valores observados para \\(p\\) variáveis.\n\n\n\n\nSejam dois vetores \\(\\mathbf{x}\\) e \\(\\mathbf{y} \\in \\mathbb{R}^p\\) e uma matriz \\(\\mathbf{\\Psi}\\), positiva definida. Então, a expressão geral para a distância quadrática entre os vetores \\(\\mathbf{x}\\) e \\(\\mathbf{y}\\) é dada por:\n\n\\[d^2(\\mathbf{x}, \\mathbf{y}) = \\left| \\mathbf{x} - \\mathbf{y} \\right|^2_{\\mathbf{\\Psi}} = (\\mathbf{x} - \\mathbf{y})^t{\\mathbf{\\Psi}} (\\mathbf{x} - \\mathbf{y})\\]\nem que a matriz \\(\\mathbf{\\Psi}\\), positiva definida, é chamada de métrica."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#distâncias-1",
    "href": "aulas/intro_est_multi/intro_est_multi.html#distâncias-1",
    "title": "Introdução à Estatística Multivariada",
    "section": "Distâncias",
    "text": "Distâncias\n\nConsideremos agora \\(\\mathbf{z}\\), um terceiro vetor de \\(\\mathbb{R}^p\\). Então as seguintes propriedades são válidas:\n\n\\(d(\\mathbf{x}, \\mathbf{y}) = d(\\mathbf{y}, \\mathbf{x})\\)\n\\(d(\\mathbf{x}, \\mathbf{y}) &gt; 0 \\hspace{0.5cm} \\forall \\hspace{0.2cm} \\mathbf{x} \\neq \\mathbf{y}\\)\n\\(d(\\mathbf{x}, \\mathbf{y}) = 0  \\hspace{0.5cm} \\text{se, e somente se,} \\hspace{0.5cm} \\mathbf{x} = \\mathbf{y}\\)\n\n\\(d(\\mathbf{x}, \\mathbf{y}) \\leq d(\\mathbf{x}, \\mathbf{z}) + d(\\mathbf{y}, \\mathbf{z}) \\,\\,\\,\\,\\,\\, \\text{(desigualdade triangular)}\\)\n\n\n\n\nDependendo da escolha da métrica \\(\\mathbf{\\Psi}\\), podemos obter diferentes medidas de distâncias, cada uma com suas características e aplicações. A escolha de uma medida adequada é fundamental para qualquer análise."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#distância-euclidiana",
    "href": "aulas/intro_est_multi/intro_est_multi.html#distância-euclidiana",
    "title": "Introdução à Estatística Multivariada",
    "section": "Distância Euclidiana",
    "text": "Distância Euclidiana\n\nSe a métrica \\(\\mathbf{\\Psi}\\) é dada por \\(\\mathbf{\\Psi} = \\mathbf{I}\\), temos a distância euclidiana quadrática. Neste caso, a expressão da distância é dada por:\n\n\\[d^2(\\mathbf{x}, \\mathbf{y}) = (\\mathbf{x} - \\mathbf{y})^t (\\mathbf{x} - \\mathbf{y})\\]\nde forma que a distância euclidiana é dada por:\n\\[d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{(\\mathbf{x} - \\mathbf{y})^t (\\mathbf{x} - \\mathbf{y})}\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#distância-euclidiana-1",
    "href": "aulas/intro_est_multi/intro_est_multi.html#distância-euclidiana-1",
    "title": "Introdução à Estatística Multivariada",
    "section": "Distância Euclidiana",
    "text": "Distância Euclidiana"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#distância-euclidiana-2",
    "href": "aulas/intro_est_multi/intro_est_multi.html#distância-euclidiana-2",
    "title": "Introdução à Estatística Multivariada",
    "section": "Distância Euclidiana",
    "text": "Distância Euclidiana\n\nA distância euclidiana confere mesmos pesos às diferenças verificadas em cada uma das \\(p\\) dimensões.\n\n\n\nNas análises estatísticas, em que distâncias serão aplicadas a dados de variáveis com diferentes variâncias, e na presença de covariâncias, incorporar tais características ao cálculo da distância pode ser fundamental.\n\n\n\n\nNesse contexto, duas medidas de distância mais apropriadas são as distâncias de Karl Pearson e a de Mahalanobis."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#distância-de-karl-pearson",
    "href": "aulas/intro_est_multi/intro_est_multi.html#distância-de-karl-pearson",
    "title": "Introdução à Estatística Multivariada",
    "section": "Distância de Karl Pearson",
    "text": "Distância de Karl Pearson\n\nSe considerarmos a métrica \\(\\mathbf{\\Psi}\\) como sendo igual a\n\n\\[\\mathbf{\\Psi} = \\mathbf{D}^{-1} = \\text{diag}\\left(\\dfrac{1}{s_{kk}}\\right), \\hspace{1cm} k = 1,\\cdots, p,\\]\nentão podemos definir a distância quadrática de Karl Pearson por:\n\\[d^2_p(\\mathbf{x}, \\mathbf{y}) = (\\mathbf{x} - \\mathbf{y})^t \\mathbf{D}^{-1}(\\mathbf{x} - \\mathbf{y})\\]\nde forma que a distância euclidiana ponderada é dada por,\n\\[d_p(\\mathbf{x}, \\mathbf{y}) = \\sqrt{(\\mathbf{x} - \\mathbf{y})^t\\mathbf{D}^{-1} (\\mathbf{x} - \\mathbf{y})}\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#distância-de-karl-pearson-1",
    "href": "aulas/intro_est_multi/intro_est_multi.html#distância-de-karl-pearson-1",
    "title": "Introdução à Estatística Multivariada",
    "section": "Distância de Karl Pearson",
    "text": "Distância de Karl Pearson"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#distância-de-karl-pearson-2",
    "href": "aulas/intro_est_multi/intro_est_multi.html#distância-de-karl-pearson-2",
    "title": "Introdução à Estatística Multivariada",
    "section": "Distância de Karl Pearson",
    "text": "Distância de Karl Pearson\n\nA distância de Karl Pearson evita o problema de heterogeneidade em um sistema de variáveis é dividindo cada variável por um fator que elimine o fator escala.\n\n\n\nTambém é conhecida como distância euclidiana ponderada ou padronizada"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#distância-de-mahalanobis",
    "href": "aulas/intro_est_multi/intro_est_multi.html#distância-de-mahalanobis",
    "title": "Introdução à Estatística Multivariada",
    "section": "Distância de Mahalanobis",
    "text": "Distância de Mahalanobis\n\nA distância de Mahalanobis configura um caso mais geral, em que são usadas tanto as variâncias quanto as covariâncias no cálculo da distância.\n\n\n\nSe considerarmos a métrica \\(\\mathbf{\\Psi}\\) igual a \\(\\mathbf{\\Psi} = \\mathbf{S}^{-1}\\), temos a chamada distância quadrática de Mahalanobis, dada por:\n\n\\[d^2_M(\\mathbf{x}, \\mathbf{y}) = (\\mathbf{x} - \\mathbf{y})^t \\mathbf{S}^{-1}(\\mathbf{x} - \\mathbf{y})\\]\nde forma que a distância de Mahalanobis é dada por,\n\\[d_M(\\mathbf{x}, \\mathbf{y}) = \\sqrt{(\\mathbf{x} - \\mathbf{y})^t\\mathbf{S}^{-1} (\\mathbf{x} - \\mathbf{y})}\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#distância-de-mahalanobis-1",
    "href": "aulas/intro_est_multi/intro_est_multi.html#distância-de-mahalanobis-1",
    "title": "Introdução à Estatística Multivariada",
    "section": "Distância de Mahalanobis",
    "text": "Distância de Mahalanobis"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#distância-de-mahalanobis-2",
    "href": "aulas/intro_est_multi/intro_est_multi.html#distância-de-mahalanobis-2",
    "title": "Introdução à Estatística Multivariada",
    "section": "Distância de Mahalanobis",
    "text": "Distância de Mahalanobis\n\nA distância de Mahalanobis é largamente aplicada, permitindo acomodar diferentes escalas e correlações entre as variáveis.\n\n\n\nDada sua definição, a distância de Mahalanobis, ao incorporar a inversa da matriz de covariância, tem como efeitos:\n\nPadronizar todas as variáveis de forma que apresentem mesma variância;\nEliminar correlações."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#matriz-de-distâncias",
    "href": "aulas/intro_est_multi/intro_est_multi.html#matriz-de-distâncias",
    "title": "Introdução à Estatística Multivariada",
    "section": "Matriz de distâncias",
    "text": "Matriz de distâncias\n\nSeja qual for a distância utilizada, é comum, em análises multivariadas, calculá-la para cada par de indivíduos e armazenar os valores em uma matriz, denominada matriz de distâncias.\n\n\\[\\mathbf{D}_{n \\times n} = \\left[ \\begin{array}{cccc} 0 & d_{12} & \\cdots & d_{1n} \\\\ d_{21} & 0 & \\cdots & d_{2n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ d_{n1} & d_{n2} & \\cdots & 0 \\end{array} \\right]\n        \\]\nem que \\(d_{ij} = d_{ji}\\) é a distância calculada para os indivíduos \\(i\\) e \\(j\\), utilizando uma métrica qualquer.\n\n\nA visualização das distâncias num gráfico do tipo “mapa de calor” permite uma apreciação conjunta das distâncias calculadas duas a duas."
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#introdução",
    "href": "aulas/analise_comp_princ/comp_princ.html#introdução",
    "title": "Análise de Componentes Principais",
    "section": "Introdução",
    "text": "Introdução\nUm problema central na análise de dados multivariados é a redução da dimensionalidade: é possível descrever com precisão a informação contida nos dados mensurados em \\(p\\) variáveis utilizando um conjunto \\(r &lt; p\\) de novas variáveis, perdendo a menor quantidade de informação possível?\n\nA análise de componentes principais tem este objetivo: dadas \\(n\\) observações de \\(p\\) variáveis, se analisa se é possível representar adequadamente esta informação com um número menor de variáveis construídas como combinações lineares das variáveis originais."
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#o-problema",
    "href": "aulas/analise_comp_princ/comp_princ.html#o-problema",
    "title": "Análise de Componentes Principais",
    "section": "O Problema…",
    "text": "O Problema…\nDado um conjunto de variáveis \\(\\mathbf{x} = [X_1 \\hspace{0.1cm} X_2 \\hspace{0.1cm} \\cdots \\hspace{0.1cm} X_p]^t\\), podemos encontrar outro conjunto de variáveis \\(\\mathbf{y} = [Y_1 \\hspace{0.1cm} Y_2 \\hspace{0.1cm} \\cdots \\hspace{0.1cm} Y_r]^t\\), dadas por\n\\[Y_i= \\displaystyle{\\sum_{j=1}^p a_{ij}X_j}, \\,\\, i = 1, \\cdots, r &lt; p\\]\nde tal forma que a informação contida em \\(\\mathbf{x}\\) esteja sendo bem representada por \\(\\mathbf{y}\\)?"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#algumas-questões",
    "href": "aulas/analise_comp_princ/comp_princ.html#algumas-questões",
    "title": "Análise de Componentes Principais",
    "section": "Algumas questões",
    "text": "Algumas questões\n\nVamos encontrar combinações lineares para representar informação.\n\n\n🤔 O que é informação?\n\n\n\nInformação \\(\\Longrightarrow\\) Variância: quanto maior a variabilidade, maior a informação contida nos dados, maior a variância dos dados"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#algumas-questões-1",
    "href": "aulas/analise_comp_princ/comp_princ.html#algumas-questões-1",
    "title": "Análise de Componentes Principais",
    "section": "Algumas questões",
    "text": "Algumas questões\n\nOutra questão importante:\n\n\n🤔 O que é uma boa representação da informação?\n\n\n\nBoa representação da informação \\(\\Longrightarrow\\) tomar as componentes de \\(\\mathbf{y}\\) que assegurem uma variância similar à de \\(\\mathbf{x}\\)"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#esquematicamente",
    "href": "aulas/analise_comp_princ/comp_princ.html#esquematicamente",
    "title": "Análise de Componentes Principais",
    "section": "Esquematicamente",
    "text": "Esquematicamente\nNestas condições, temos que buscar combinações lineares \\(\\mathbf{y}\\) das variáveis \\(\\mathbf{x}\\) de forma que se maximize a variância\n\n\n\n\n\n\nVariáveis Originais\n\n\n\n\nCombinações Lineares\n\n\n\n\n\n\n\\(X_1\\)\n\n\n\n\n\\(Y_1\\)\n\n\n\n\n\\(X_2\\)\n\n\n\n\n\\(Y_2\\)\n\n\n\n\n\\(\\vdots\\)\n\n\n\n\n\\(\\vdots\\)\n\n\n\n\n\\(X_{r}\\)\n\n\n\\(\\Longrightarrow\\)\n\n\n\\(Y_r\\)\n\n\n\n\n\\(\\vdots\\)\n\n\n\n\n\\(\\vdots\\)\n\n\n\n\n\\(X_p\\)\n\n\n\n\n\\(Y_{p}\\)\n\n\n\n\n\n\n\n\\(\\rm{Var}[\\mathbf{y}]\\): Máxima"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#esquematicamente-1",
    "href": "aulas/analise_comp_princ/comp_princ.html#esquematicamente-1",
    "title": "Análise de Componentes Principais",
    "section": "Esquematicamente",
    "text": "Esquematicamente\nIdeia básica da técnica de Análise de Componentes Principais:\n\n\n\n\n\n\nVariáveis Originais\n\n\n\n\nComponentes Principais\n\n\n\n\n\n\n\\(X_1\\)\n\n\nACP\n\n\n\\(Y_1\\)\n\n\n\n\n\\(X_2\\)\n\n\n\\(\\Longrightarrow\\)\n\n\n\\(Y_2\\)\n\n\n\n\n\\(\\vdots\\)\n\n\n\n\n\\(\\vdots\\)\n\n\n\n\n\\(X_{p}\\)\n\n\n\n\n\\(Y_r\\)\n\n\n\n\n\n\n\n\n\\(\\vdots\\)\n\n\n\n\n\n\n\n\n\\(Y_{p}\\)\n\n\n\n\n\n\n\n\\(r\\) primeiras componentes resumam, por exemplo, 80% do comportamento geral das \\(p\\) variáveis originais"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#principais-objetivos",
    "href": "aulas/analise_comp_princ/comp_princ.html#principais-objetivos",
    "title": "Análise de Componentes Principais",
    "section": "Principais objetivos",
    "text": "Principais objetivos\n\nRedução da dimensionalidade dos dados, projetando-os em uma dimensão \\(r &lt; p\\);"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#principais-objetivos-1",
    "href": "aulas/analise_comp_princ/comp_princ.html#principais-objetivos-1",
    "title": "Análise de Componentes Principais",
    "section": "Principais objetivos",
    "text": "Principais objetivos\n\nObtenção de combinações interpretáveis: determinar índices e produzir escores com base nos resultados avaliados para as \\(p\\) variáveis;"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#principais-objetivos-2",
    "href": "aulas/analise_comp_princ/comp_princ.html#principais-objetivos-2",
    "title": "Análise de Componentes Principais",
    "section": "Principais objetivos",
    "text": "Principais objetivos\n\nDescrição e entendimento da estrutura de correlação entre as variáveis, através de algumas combinações lineares das mesmas."
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#componentes-principais-o-que-são",
    "href": "aulas/analise_comp_princ/comp_princ.html#componentes-principais-o-que-são",
    "title": "Análise de Componentes Principais",
    "section": "Componentes Principais: o que são?",
    "text": "Componentes Principais: o que são?\nAlgebricamente: são combinações lineares das \\(p\\) variáveis originais, \\(X_1, X_2, \\cdots, X_p\\).\n\nGeometricamente: são as coordenadas dos pontos amostrais em um sistema de eixos obtido pela rotação do sistema de eixos original, na direção de variabilidade máxima.\n\n🔎"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#componentes-principais-alguns-comentários",
    "href": "aulas/analise_comp_princ/comp_princ.html#componentes-principais-alguns-comentários",
    "title": "Análise de Componentes Principais",
    "section": "Componentes Principais: alguns comentários",
    "text": "Componentes Principais: alguns comentários\n\nNão pressupõe normalidade dos dados, embora componentes derivadas de populações normais tenham interpretações úteis.\n\n\n\nCom frequência, revela relações insuspeitas. Pode permitir interpretações que não seriam obtidas preliminarmente.\n\n\n\n\nEm algumas aplicações, os componentes da ACP configuram o objetivo final do estudo. Em outras, servem como passo intermediário para realização de outras análises, como regressão, classificação, agrupamento, etc…"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#componentes-principais-como-obtê-los",
    "href": "aulas/analise_comp_princ/comp_princ.html#componentes-principais-como-obtê-los",
    "title": "Análise de Componentes Principais",
    "section": "Componentes Principais: como obtê-los?",
    "text": "Componentes Principais: como obtê-los?\n\nSejam \\(X_1, \\hspace{0.1cm} X_2, \\hspace{0.1cm} \\cdots, \\hspace{0.1cm} X_p\\) as variáveis originais\n\n\n\nA ideia é encontrar um novo conjunto de variáveis \\(Y_1, \\hspace{0.1cm} Y_2, \\hspace{0.1cm} \\cdots, \\hspace{0.1cm} Y_p\\), tais que:\n\n\\[\\textrm{Var}[Y_1] \\geqslant \\textrm{Var}[Y_2] \\geqslant \\cdots \\geqslant \\textrm{Var}[Y_p]\\]\n\n\n\nVamos tomar cada nova variável \\(Y_i\\), \\(i = 1, \\cdots, p\\), como uma combinação linear das variáveis originais \\(\\mathbf{x}\\):\n\n\\[Y_i = a_{i1}X_1 + a_{i2}X_2 + \\cdots + a_{ip}X_p = \\mathbf{a}_i^t \\mathbf{x}\\]"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#componentes-principais-como-obtê-los-1",
    "href": "aulas/analise_comp_princ/comp_princ.html#componentes-principais-como-obtê-los-1",
    "title": "Análise de Componentes Principais",
    "section": "Componentes Principais: como obtê-los?",
    "text": "Componentes Principais: como obtê-los?\n\nPara fixar problemas de escala, adicionamos uma primeira restrição aos vetores \\(\\mathbf{a}_i\\):\n\n\\[\\mathbf{a}_i^t \\mathbf{a}_i = \\displaystyle{ \\sum_{j=1}^p a_{ij}^2} = 1\\]\n\n\nPara evitar que duas variáveis \\(Y_i\\) e \\(Y_k\\), \\(i \\neq k\\), \\(i,k = 1, \\cdots, p\\), compartilhem informação, adicionamos uma segunda restrição aos vetores \\(\\mathbf{a}_i\\):\n\n\\[\\mathbf{a}_i^t \\mathbf{a}_k = \\displaystyle{ \\sum_{j=1}^p a_{ij}a_{kj}}  = 0\\]"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#componentes-principais-como-obtê-los-2",
    "href": "aulas/analise_comp_princ/comp_princ.html#componentes-principais-como-obtê-los-2",
    "title": "Análise de Componentes Principais",
    "section": "Componentes Principais: como obtê-los?",
    "text": "Componentes Principais: como obtê-los?\n\n\n\n💡 Garantia: ortogalidade, componentes não correlacionadas, independência"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#componentes-principais-como-obtê-los-3",
    "href": "aulas/analise_comp_princ/comp_princ.html#componentes-principais-como-obtê-los-3",
    "title": "Análise de Componentes Principais",
    "section": "Componentes Principais: como obtê-los?",
    "text": "Componentes Principais: como obtê-los?\n\nPrimeira Componente Principal\n\n\\[Y_1 = a_{11}X_1 + a_{12}X_2 + \\cdots + a_{1p}X_p = \\boldsymbol{a}_1^t \\mathbf{x}\\]\n\nObjetivo: Encontrar \\(\\boldsymbol{a}_1^t = [a_{11} \\hspace{0.3cm} a_{12} \\hspace{0.3cm} \\cdots \\hspace{0.3cm} a_{1p}]^t\\) tal que:\n\n\\(\\rm{Var}[Y_1]\\) seja máxima\n\n\n\nSujeita à restrição:\n\\[\\boldsymbol{a}_1^t \\boldsymbol{a}_1 = a_{11}^2 + a_{12}^2 + \\cdots + a_{1p}^2 = 1\\]"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#componentes-principais-como-obtê-los-4",
    "href": "aulas/analise_comp_princ/comp_princ.html#componentes-principais-como-obtê-los-4",
    "title": "Análise de Componentes Principais",
    "section": "Componentes Principais: como obtê-los?",
    "text": "Componentes Principais: como obtê-los?\n\nSegunda Componente Principal\n\n\\[Y_2 = a_{21}X_1 + a_{22}X_2 + \\cdots + a_{2p}X_p = \\boldsymbol{a}_2^t \\mathbf{x}\\]\n\nObjetivo: Encontrar \\(\\boldsymbol{a}_2^t = [a_{21} \\hspace{0.3cm} a_{22} \\hspace{0.3cm} \\cdots \\hspace{0.3cm} a_{2p}]^t\\) tal que:\n\n\\(\\rm{Var}[Y_2]\\) seja máxima\n\n\n\nSujeita à restrição:\n\\[\\boldsymbol{a}_2^t \\boldsymbol{a}_2 = a_{21}^2 + a_{22}^2 + \\cdots + a_{2p}^2 = 1\\]\n\\[\\rm{Cov}[Y_1,Y_2] = 0\\]"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#componentes-principais-como-obtê-los-5",
    "href": "aulas/analise_comp_princ/comp_princ.html#componentes-principais-como-obtê-los-5",
    "title": "Análise de Componentes Principais",
    "section": "Componentes Principais: como obtê-los?",
    "text": "Componentes Principais: como obtê-los?\n\ni-ésima Componente Principal\n\n\\[Y_i = a_{i1}X_1 + a_{i2}X_2 + \\cdots + a_{ip}X_p = \\boldsymbol{a}_i^t \\mathbf{x}\\]\n\nObjetivo: Encontrar \\(\\boldsymbol{a}_i^t = [a_{i1} \\hspace{0.3cm} a_{i2} \\hspace{0.3cm} \\cdots \\hspace{0.3cm} a_{ip}]^t\\) tal que:\n\n\\(\\rm{Var}[Y_i]\\) seja máxima\n\n\n\nSujeita à restrição:\n\\[\\boldsymbol{a}_i^t \\boldsymbol{a}_i =  a_{i1}^2 + a_{i2}^2 + \\cdots + a_{ip}^2 = 1\\]\n\\[\\rm{Cov}[Y_,Y_k] = 0, \\text{para  } k &lt; i\\]"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#a-escolha-dos-vetores-boldsymbola_i",
    "href": "aulas/analise_comp_princ/comp_princ.html#a-escolha-dos-vetores-boldsymbola_i",
    "title": "Análise de Componentes Principais",
    "section": "A escolha dos vetores \\(\\boldsymbol{a}_i\\)",
    "text": "A escolha dos vetores \\(\\boldsymbol{a}_i\\)\n\nConsidere o vetor aleatório p-variado \\(\\mathbf{x} = [X_1 \\hspace{0.3cm} X_2 \\hspace{0.3cm} \\cdots \\hspace{0.3cm} X_p]^t\\) com vetor de médias \\(\\boldsymbol{\\mu}\\) e matriz de covariâncias \\(\\boldsymbol{\\Sigma}\\), positiva definida (todos os seus autovalores são positivos), sendo\n\n\\[\\boldsymbol{\\mu} = [\\mu_1 \\hspace{0.3cm} \\mu_2 \\hspace{0.3cm} \\cdots \\hspace{0.3cm} \\mu_p]^t \\hspace{0.5cm} \\textrm{e} \\hspace{0.5cm}\n\\boldsymbol{\\Sigma} = \\left[ \\begin{array}{cccc} \\sigma_{11} & \\sigma_{12} & \\cdots & \\sigma_{1p}\n\\\\ \\sigma_{21} & \\sigma_{22} & \\cdots & \\sigma_{2p}\n\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n\\\\ \\sigma_{p1} & \\sigma_{p2} & \\cdots & \\sigma_{pp} \\end{array} \\right]\\]\n\n\nPara determinação dos componentes principais, com base no que foi exposto, usaremos o seguinte teorema:"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#a-escolha-dos-vetores-boldsymbola_i-1",
    "href": "aulas/analise_comp_princ/comp_princ.html#a-escolha-dos-vetores-boldsymbola_i-1",
    "title": "Análise de Componentes Principais",
    "section": "A escolha dos vetores \\(\\boldsymbol{a}_i\\)",
    "text": "A escolha dos vetores \\(\\boldsymbol{a}_i\\)\nTeorema - Maximização de formas quadráticas: Seja \\(\\boldsymbol{B}\\) uma matriz positiva definida com autovalores \\(\\lambda_1 \\geqslant \\lambda_2 \\geqslant \\cdots \\geqslant \\lambda_p &gt; 0\\) e autovetores associados normalizados \\({\\boldsymbol{e}_1, \\boldsymbol{e}_2, \\cdots, \\boldsymbol{e}_p}\\). Então:\n\\[\\max_{\\mathbf{x} \\neq \\boldsymbol{0}} \\dfrac{\\mathbf{x}^t \\boldsymbol{B} \\mathbf{x}}{\\mathbf{x}^t \\mathbf{x}} =  \\lambda_1, \\text{ obtido quando } \\mathbf{x} = \\boldsymbol{e}_1;\\]\n\\[\\min_{\\mathbf{x} \\neq \\boldsymbol{0}} \\dfrac{\\mathbf{x}^t \\boldsymbol{B} \\mathbf{x}}{\\mathbf{x}^t \\mathbf{x}} =  \\lambda_p, \\text{ obtido quando } \\mathbf{x} = \\boldsymbol{e}_p.\\]\n\n\nAdicionalmente,\n\n\\[\\max_{\\mathbf{x} \\perp \\boldsymbol{e}_1, \\boldsymbol{e}_1, \\cdots, \\boldsymbol{e}_k} \\dfrac{\\mathbf{x}^t \\boldsymbol{B} \\mathbf{x}}{\\mathbf{x}^t \\mathbf{x}} =  \\lambda_{k+1}, \\text{ obtido quando } \\mathbf{x} = \\boldsymbol{e}_{k+1}.\\]"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#a-escolha-dos-vetores-boldsymbola_i-2",
    "href": "aulas/analise_comp_princ/comp_princ.html#a-escolha-dos-vetores-boldsymbola_i-2",
    "title": "Análise de Componentes Principais",
    "section": "A escolha dos vetores \\(\\boldsymbol{a}_i\\)",
    "text": "A escolha dos vetores \\(\\boldsymbol{a}_i\\)\nAssim, no contexto de componentes principais, seja \\(\\mathbf{x} = [X_1 \\hspace{0.3cm} X_2 \\hspace{0.3cm} \\cdots \\hspace{0.3cm} X_p]^t\\) um vetor aleatório. Seja \\(\\boldsymbol{\\Sigma}\\) a matriz de variâncias e covariâncias e \\((\\lambda_1, \\boldsymbol{e}_1)\\), \\((\\lambda_2, \\boldsymbol{e}_2)\\), …, \\((\\lambda_p, \\boldsymbol{e}_p)\\) seus autovalores e autovetores, tal que \\(\\lambda_1 \\geqslant \\lambda_2 \\geqslant \\cdots \\geqslant \\lambda_p &gt; 0\\). Então:\n\\[\\max_{\\boldsymbol{a} \\neq \\boldsymbol{0}} \\dfrac{\\boldsymbol{a}^t \\boldsymbol{\\Sigma} \\boldsymbol{a}}{\\boldsymbol{a}^t \\boldsymbol{a}} = \\max_{\\boldsymbol{a} \\neq \\boldsymbol{0}}(\\boldsymbol{a}^t \\boldsymbol{\\Sigma} \\boldsymbol{a}) = \\lambda_1, \\text{ obtido quando } \\boldsymbol{a} = \\boldsymbol{e}_1;\\]\n\\[\\min_{\\boldsymbol{a} \\neq \\boldsymbol{0}} \\dfrac{\\boldsymbol{a}^t \\boldsymbol{\\Sigma} \\boldsymbol{a}}{\\boldsymbol{a}^t \\boldsymbol{a}} = \\min_{\\boldsymbol{a} \\neq \\boldsymbol{0}}(\\boldsymbol{a}^t \\boldsymbol{\\Sigma} \\boldsymbol{a}) = \\lambda_p, \\text{ obtido quando } \\boldsymbol{a} = \\boldsymbol{e}_p.\\]"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#a-escolha-dos-vetores-boldsymbola_i-3",
    "href": "aulas/analise_comp_princ/comp_princ.html#a-escolha-dos-vetores-boldsymbola_i-3",
    "title": "Análise de Componentes Principais",
    "section": "A escolha dos vetores \\(\\boldsymbol{a}_i\\)",
    "text": "A escolha dos vetores \\(\\boldsymbol{a}_i\\)\n\nAdicionalmente,\n\n\\[\\max_{\\boldsymbol{a} \\perp \\boldsymbol{e}_1, \\boldsymbol{e}_1, \\cdots, \\boldsymbol{e}_k} \\dfrac{\\boldsymbol{a}^t \\boldsymbol{\\Sigma} {\\boldsymbol a}}{\\boldsymbol{a}^t \\boldsymbol{a}} = \\max_{\\boldsymbol{a} \\perp \\boldsymbol{e}_1, \\boldsymbol{e}_1, \\cdots, \\boldsymbol{e}_k}(\\boldsymbol{a}^t \\boldsymbol{\\Sigma} \\boldsymbol{a})= \\lambda_{k+1}, \\text{ obtido quando } \\boldsymbol{a} = \\boldsymbol{e}_{k+1}.\\]"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#a-escolha-dos-vetores-boldsymbola_i-4",
    "href": "aulas/analise_comp_princ/comp_princ.html#a-escolha-dos-vetores-boldsymbola_i-4",
    "title": "Análise de Componentes Principais",
    "section": "A escolha dos vetores \\(\\boldsymbol{a}_i\\)",
    "text": "A escolha dos vetores \\(\\boldsymbol{a}_i\\)\n\nUma escolha interessante para os vetores de constantes \\({\\boldsymbol{a}_i}\\), \\(i = 1, \\cdots, p\\) são os autovetores normalizados \\({\\boldsymbol{e}_i}\\) da matriz \\(\\boldsymbol{\\Sigma}\\).\n\n\n\nDessa forma, podemos definir a \\(i\\)-ésima componente principal da matriz \\(\\boldsymbol{\\Sigma}\\), \\(i = 1, \\cdots, p\\) como sendo\n\n\n\n\\[Y_i = {\\boldsymbol{e}_i^t}\\mathbf{x} =  e_{i1}X_1 + e_{i2}X_2 + \\cdots + e_{ip}X_p\\]"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#componentes-principais-propriedades",
    "href": "aulas/analise_comp_princ/comp_princ.html#componentes-principais-propriedades",
    "title": "Análise de Componentes Principais",
    "section": "Componentes Principais: propriedades",
    "text": "Componentes Principais: propriedades\n\nA esperança e a variância da componente \\(Y_i\\) são respectivamente dadas por:\n\n\\[\n\\begin{eqnarray*}\nE[Y_i] &=& E[e_{i1}X_1 + e_{i2}X_2 + \\cdots + e_{ip}X_p]  \\nonumber \\\\\n&=& e_{i1}E[X_1] + e_{i2}E[X_2] + \\cdots + e_{ip}E[X_p]  \\nonumber \\\\\n&=& e_{i1}\\mu_1 + e_{i2}\\mu_2 + \\cdots + e_{ip}\\mu_p \\nonumber \\\\\n&=& {\\boldsymbol{e}_i^t}{\\boldsymbol{\\mu}} \\nonumber\n\\end{eqnarray*}\n\\]\n\n\\[ \\textrm{Var}[Y_i] = \\textrm{Var}[{\\boldsymbol{e}_i^t}\\mathbf{x}] = {\\boldsymbol{e}_i^t} \\textrm{Var}[\\mathbf{x}] {\\boldsymbol{e}_i}\n=  {\\boldsymbol{e}_i^t} \\boldsymbol{\\Sigma} {\\boldsymbol{e}_i} = {\\boldsymbol{e}_i^t} \\lambda_i {\\boldsymbol{e}_i} =   \n{\\boldsymbol{e}_i^t} {\\boldsymbol{e}_i}\\lambda_i = \\lambda_i \\]"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#na-forma-matricial",
    "href": "aulas/analise_comp_princ/comp_princ.html#na-forma-matricial",
    "title": "Análise de Componentes Principais",
    "section": "Na forma matricial",
    "text": "Na forma matricial\n\nSejam \\(\\boldsymbol{O}\\) a matriz dos autovetores normalizados da matriz \\(\\boldsymbol{\\Sigma}\\), isto é,\n\n\\[\\boldsymbol{O} = \\left[ \\begin{array}{cccc} e_{11} & e_{21} & \\cdots & e_{p1}\n\\\\ e_{12} & e_{22} & \\cdots & e_{p2}\n\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n\\\\ e_{1p} & e_{2p} & \\cdots & e_{pp} \\end{array} \\right] = [{\\boldsymbol{e}_1} \\hspace{0.5cm} {\\boldsymbol{e}_2}\n\\hspace{0.5cm} \\cdots \\hspace{0.5cm} {\\boldsymbol{e}_p}]\\]\ne \\(\\boldsymbol{y}\\) o vetor das componentes principais. Então, \\(\\boldsymbol{y} = \\boldsymbol{O}^t \\mathbf{x}\\) e a matriz de covariâncias de \\(\\boldsymbol{y}\\) será:\n\\[\\textrm{Var}[\\boldsymbol{y}] = \\textrm{Var}[\\boldsymbol{O}^t \\mathbf{x}] =  \\boldsymbol{O}^t \\textrm{Var}[\\mathbf{x}] \\boldsymbol{O} =\n\\boldsymbol{O}^t \\boldsymbol{\\Sigma} \\boldsymbol{O} = \\boldsymbol{\\Lambda}\\]"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#na-forma-matricial-1",
    "href": "aulas/analise_comp_princ/comp_princ.html#na-forma-matricial-1",
    "title": "Análise de Componentes Principais",
    "section": "Na forma matricial",
    "text": "Na forma matricial\nsendo\n\\[\\boldsymbol{\\Lambda} = \\left[ \\begin{array}{cccc} \\lambda_1 & 0 & \\cdots & 0\n\\\\ 0 & \\lambda_2 & \\cdots & 0\n\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n\\\\ 0 & 0 & \\cdots & \\lambda_p \\end{array} \\right] \\]\nou ainda, \\(\\boldsymbol{\\Sigma} = \\boldsymbol{O} \\boldsymbol{\\Lambda} \\boldsymbol{O}^t = \\displaystyle \\sum_{i=1}^p \\lambda_i \\boldsymbol{e}_i \\boldsymbol{e}_i^t\\), uma vez que \\(\\boldsymbol{O}\\) é uma matriz ortogonal tal que \\(\\boldsymbol{O} \\boldsymbol{O}^t = \\boldsymbol{O}^t \\boldsymbol{O} = \\boldsymbol{I}\\). Estes resultados são conhecidos como Teorema da decomposição espectral."
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#variabilidade-explicada",
    "href": "aulas/analise_comp_princ/comp_princ.html#variabilidade-explicada",
    "title": "Análise de Componentes Principais",
    "section": "Variabilidade explicada",
    "text": "Variabilidade explicada\n\n\n\n\n\nVariável\n\n\nVariância\n\n\nComponente\n\n\nVariância\n\n\n\n\n\n\n\\(X_1\\)\n\n\n\\(\\sigma_{11}\\)\n\n\n\\(Y_1\\)\n\n\n\\(\\lambda_1\\)\n\n\n\n\n\\(X_2\\)\n\n\n\\(\\sigma_{22}\\)\n\n\n\\(Y_2\\)\n\n\n\\(\\lambda_2\\)\n\n\n\n\n\\(\\vdots\\)\n\n\n\\(\\vdots\\)\n\n\n\\(\\vdots\\)\n\n\n\\(\\vdots\\)\n\n\n\n\n\\(X_p\\)\n\n\n\\(\\sigma_{pp}\\)\n\n\n\\(Y_p\\)\n\n\n\\(\\lambda_p\\)\n\n\n\n\nTotal\n\n\n\\(\\sigma_T^2\\) = \\(\\displaystyle{\\sum_{j=1}^p \\sigma_{jj}} = \\rm{tr}(\\boldsymbol{\\Sigma})\\)\n\n\nTotal\n\n\n\\(\\lambda_T = \\displaystyle{\\sum_{j=1}^p \\lambda_j} = \\rm{tr}(\\boldsymbol{\\Lambda})\\)\n\n\n\n\n\n\\[\\rm{tr}(\\boldsymbol{\\Sigma}) = \\rm{tr}(\\boldsymbol{O} \\boldsymbol{\\Lambda} \\boldsymbol{O}^t) = \\rm{tr}(\\boldsymbol{\\Lambda} \\boldsymbol{O}^t \\boldsymbol{O}) = \\rm{tr}(\\boldsymbol{\\Lambda} \\boldsymbol{I}) = \\rm{tr}(\\boldsymbol{\\Lambda})\\]\n\n\n\\[\\sigma_T^2 = \\lambda_T\\]"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#variabilidade-explicada-1",
    "href": "aulas/analise_comp_princ/comp_princ.html#variabilidade-explicada-1",
    "title": "Análise de Componentes Principais",
    "section": "Variabilidade explicada",
    "text": "Variabilidade explicada\n\nPode-se então concluir que a j-ésima componente explica\n\n\\[\\displaystyle{\\frac{\\textrm{Var}[Y_j]}{\\textrm{Variância Total de X}}} = \\displaystyle{\\frac{\\lambda_j}{\\textrm{tr}(\\boldsymbol{\\Sigma})}} =\n\\displaystyle{\\frac{\\lambda_j}{\\displaystyle{\\sum_{i=1}^p \\lambda_i}}}\\]\nda variação total original, e ainda, que as \\(r\\) primeiras componentes explicam\n\\[\\displaystyle{\\frac{ \\displaystyle \\sum_{j=1}^r \\textrm{Var}[Y_j]}{\\textrm{Variância Total de X}}} = \\displaystyle{\\frac{\\displaystyle \\sum_{j=1}^r \\lambda_j}{\\textrm{tr}(\\boldsymbol{\\Sigma})}} =\n\\displaystyle{\\frac{\\displaystyle \\sum_{j=1}^r \\lambda_j}{\\displaystyle{\\sum_{i=1}^p \\lambda_i}}}\\]\nda variação total."
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#variabilidade-explicada-2",
    "href": "aulas/analise_comp_princ/comp_princ.html#variabilidade-explicada-2",
    "title": "Análise de Componentes Principais",
    "section": "Variabilidade explicada",
    "text": "Variabilidade explicada\n\nBusca-se analisar um conjunto menor de variáveis sem perder muita informação sobre a estrutura de variabilidade original\n\n\n\nAproximação de \\(\\boldsymbol{\\Sigma}\\): Analisando as \\(r\\) primeiras componentes principais\n\n\\[\\boldsymbol{\\Sigma} \\approx \\displaystyle \\sum_{i=1}^r \\lambda_i \\boldsymbol{e}_i \\boldsymbol{e}_i^t\\]\n\n\n\nCada parcela da soma envolve uma matriz de dimensão \\(p \\times p\\) correspondente apenas à informação da \\(j\\)-ésima componente principal"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#correlação-com-as-variáveis-originais",
    "href": "aulas/analise_comp_princ/comp_princ.html#correlação-com-as-variáveis-originais",
    "title": "Análise de Componentes Principais",
    "section": "Correlação com as variáveis originais",
    "text": "Correlação com as variáveis originais\n\nOs coeficientes de correlação entre a \\(j\\)-ésima variável e a \\(i\\)-ésima componente principal é dada por:\n\n\\[\\rho_{Y_i,X_j} = \\displaystyle{\\frac{e_{ij} \\sqrt{\\lambda_i}}{\\sqrt{\\sigma_{jj}}}}\\]\n\n\nAs correlações medem unicamente a importância de uma variável individual sem considerar a influência das demais. Não medem a importância de \\(X_i\\) na presença de outras variáveis.\n\n\n\n\nOs coeficientes (cargas) dos componentes (\\(e_{ij}\\)), seus sinais e magnitudes, permitem interpretar os componentes e avaliar a importância das variáveis em sua constituição."
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#estimação-das-componentes-principais",
    "href": "aulas/analise_comp_princ/comp_princ.html#estimação-das-componentes-principais",
    "title": "Análise de Componentes Principais",
    "section": "Estimação das Componentes Principais",
    "text": "Estimação das Componentes Principais\n\nEm geral, \\(\\boldsymbol{\\Sigma}\\) é estimada por \\(\\boldsymbol{S}\\)\n\n\\[\\boldsymbol{S} = \\left[ \\begin{array}{cccc} s_{11} & s_{12} & \\cdots & s_{1p}\n\\\\ s_{21} & s_{22} & \\cdots & s_{2p}\n\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n\\\\ s_{p1} & s_{p2} & \\cdots & s_{pp} \\end{array} \\right]\\]\n\n\nAutovalores de \\(\\boldsymbol{S}\\): \\(\\hat{\\lambda}_1, \\hat{\\lambda}_2, \\cdots, \\hat{\\lambda}_p\\)\n\n\n\n\nAutovetores de \\(\\boldsymbol{S}\\): \\(\\hat{\\boldsymbol{e}}_1, \\hat{\\boldsymbol{e}}_2, \\cdots, \\hat{\\boldsymbol{e}}_p\\)"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#estimação-das-componentes-principais-1",
    "href": "aulas/analise_comp_princ/comp_princ.html#estimação-das-componentes-principais-1",
    "title": "Análise de Componentes Principais",
    "section": "Estimação das Componentes Principais",
    "text": "Estimação das Componentes Principais\n\nEstimação da \\(j\\)-ésima componente principal de \\(\\boldsymbol{S}\\):\n\n\\[\\hat{Y}_j = {\\hat{\\boldsymbol{e}}_j^t}\\mathbf{x} =  \\hat{e}_{j1}X_1 + \\hat{e}_{j2}X_2 + \\cdots + \\hat{e}_{jp}X_p, \\,\\,\\,\\,\\,\\, j = 1, 2, \\cdots, p\\]\n\n\nComponentes principais amostrais - Propriedades\n\nVariância: \\(\\text{Var}(\\hat{Y}_j) = \\hat{\\lambda}_j\\)\nCovariância entre as componentes: \\(\\text{Cov}(\\hat{Y}_j, \\hat{Y}_k) = 0, \\,\\,\\, j \\neq k\\)\nVariância total estimada explicada pela componente: \\[\\dfrac{\\text{Var}(\\hat{Y}_j)}{\\text{Variância total estimada de } \\mathbf{x}} = \\dfrac{\\hat{\\lambda}_j}{\\text{tr}(\\boldsymbol{S})} =  \\dfrac{\\hat{\\lambda}_j}{\\sum \\limits_{i=1}^p \\hat{\\lambda}_i}\\]"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#estimação-das-componentes-principais-2",
    "href": "aulas/analise_comp_princ/comp_princ.html#estimação-das-componentes-principais-2",
    "title": "Análise de Componentes Principais",
    "section": "Estimação das Componentes Principais",
    "text": "Estimação das Componentes Principais\n\nCorrelação estimada entre componente e variável: \\[r_{\\hat{Y}_j,\\hat{X}_k} = \\displaystyle{\\frac{\\hat{e}_{jk} \\sqrt{\\hat{\\lambda}_j}}{\\sqrt{\\sigma_{kk}}}}\\]\n\n\n\nDecomposição espectral de \\(\\boldsymbol{S}\\):\n\n\\[\\boldsymbol{S} = \\displaystyle \\sum_{j=1}^p \\hat{\\lambda}_j  \\hat{\\boldsymbol{e}}_j  \\hat{\\boldsymbol{e}}_j^t\\]"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#estimação-das-componentes-principais-3",
    "href": "aulas/analise_comp_princ/comp_princ.html#estimação-das-componentes-principais-3",
    "title": "Análise de Componentes Principais",
    "section": "Estimação das Componentes Principais",
    "text": "Estimação das Componentes Principais\n\nAproximação de \\(\\boldsymbol{S}\\) pelas \\(r\\) primeiras componentes\n\n\\[\\boldsymbol{S} \\approx \\displaystyle \\sum_{j=1}^r \\hat{\\lambda}_j \\hat{\\boldsymbol{e}}_j  \\hat{\\boldsymbol{e}}_j^t\\]\n\n\nEscores das componentes\n\nValor das componentes para cada elemento amostral\nNa prática, o uso das componentes relevantes se dá através dos escores"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#exemplo-mingoti2007",
    "href": "aulas/analise_comp_princ/comp_princ.html#exemplo-mingoti2007",
    "title": "Análise de Componentes Principais",
    "section": "Exemplo: (Mingoti,2007)",
    "text": "Exemplo: (Mingoti,2007)\n\n12 empresas, 3 variáveis: ganho bruto (\\(X_1\\)), ganho líquido (\\(X_2\\)) e patrimônio acumulado (\\(X_3\\))\n\n\n\n\n\n\nEmpresa\n\n\nGanho bruto \\((X_1)\\)\n\n\nGanho líquido \\((X_2)\\)\n\n\nPatrimônio \\((X_3)\\)\n\n\n\n\n\n\nE1\n\n\n9893\n\n\n564\n\n\n17689\n\n\n\n\nE2\n\n\n8776\n\n\n389\n\n\n17359\n\n\n\n\nE3\n\n\n13572\n\n\n1103\n\n\n18597\n\n\n\n\nE4\n\n\n6455\n\n\n743\n\n\n8745\n\n\n\n\nE5\n\n\n5129\n\n\n203\n\n\n14397\n\n\n\n\nE6\n\n\n5432\n\n\n215\n\n\n3467\n\n\n\n\nE7\n\n\n3807\n\n\n385\n\n\n4679\n\n\n\n\nE8\n\n\n3423\n\n\n187\n\n\n6754\n\n\n\n\nE9\n\n\n3708\n\n\n127\n\n\n2275\n\n\n\n\nE10\n\n\n3294\n\n\n297\n\n\n6754\n\n\n\n\nE11\n\n\n5433\n\n\n432\n\n\n5589\n\n\n\n\nE12\n\n\n6287\n\n\n451\n\n\n8972"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#primeiro-exemplo-mingoti2007",
    "href": "aulas/analise_comp_princ/comp_princ.html#primeiro-exemplo-mingoti2007",
    "title": "Análise de Componentes Principais",
    "section": "Primeiro Exemplo: (Mingoti,2007)",
    "text": "Primeiro Exemplo: (Mingoti,2007)\n\nload &lt;- function(pkg){\n  new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg))\n    install.packages(new.pkg, dependencies = TRUE)\n  sapply(pkg, require, character.only = TRUE)\n} \n\n## Pacotes utilizados nessa análise\n\npackages = c(\"tidyverse\", \"factoextra\", \"psych\", \"gridExtra\")\nload(packages)\n\n tidyverse factoextra      psych  gridExtra \n      TRUE       TRUE       TRUE       TRUE \n\n\n\ndados &lt;- read.table(\"https://raw.githubusercontent.com/tiagomartin/est014/refs/heads/master/dados/empresas.txt\", row.names = 1, header = TRUE) \ndados %&gt;% \n  str()\n\n'data.frame':   12 obs. of  3 variables:\n $ Granho_Bruto : int  9893 8776 13572 6455 5129 5432 3807 3423 3708 3294 ...\n $ Ganho_Liquido: int  564 389 1103 743 203 215 385 187 127 297 ...\n $ Patrimonio   : int  17689 17359 18597 8745 14397 3467 4679 6754 2275 6754 ..."
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#primeiro-exemplo-mingoti2007-1",
    "href": "aulas/analise_comp_princ/comp_princ.html#primeiro-exemplo-mingoti2007-1",
    "title": "Análise de Componentes Principais",
    "section": "Primeiro Exemplo: (Mingoti,2007)",
    "text": "Primeiro Exemplo: (Mingoti,2007)\n\nx_barra = dados %&gt;% \n  colMeans()\n\nx_barra\n\n Granho_Bruto Ganho_Liquido    Patrimonio \n    6267.4167      424.6667     9606.4167 \n\nS = dados %&gt;% \n  var()\n\nS\n\n              Granho_Bruto Ganho_Liquido Patrimonio\nGranho_Bruto     9550608.6     706121.06 14978232.5\nGanho_Liquido     706121.1      76269.52   933915.1\nPatrimonio      14978232.5     933915.06 34408113.0"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#primeiro-exemplo-mingoti2007-2",
    "href": "aulas/analise_comp_princ/comp_princ.html#primeiro-exemplo-mingoti2007-2",
    "title": "Análise de Componentes Principais",
    "section": "Primeiro Exemplo: (Mingoti,2007)",
    "text": "Primeiro Exemplo: (Mingoti,2007)\n\ndados %&gt;% \n  boxplot()"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#primeiro-exemplo-mingoti2007-3",
    "href": "aulas/analise_comp_princ/comp_princ.html#primeiro-exemplo-mingoti2007-3",
    "title": "Análise de Componentes Principais",
    "section": "Primeiro Exemplo: (Mingoti,2007)",
    "text": "Primeiro Exemplo: (Mingoti,2007)\n\n## Analise de Componentes Principais utilizando a matriz de covariancias (Nao aconselhavel, neste caso)\nacp_S = prcomp(dados)\n\n## Proporcao da variacao explicada\nsummary(acp_S)\n\nImportance of components:\n                             PC1       PC2       PC3\nStandard deviation     6440.0615 1.594e+03 145.23266\nProportion of Variance    0.9418 5.767e-02   0.00048\nCumulative Proportion     0.9418 9.995e-01   1.00000\n\n## Loadings (cargas)\nacp_S$rotation\n\n                      PC1         PC2         PC3\nGranho_Bruto  -0.42509725 -0.89970680 -0.09909593\nGanho_Liquido -0.02766083 -0.09651661  0.99494694\nPatrimonio    -0.90472493  0.42569029  0.01614231\n\n## Opcional: trocar sinal da primeira componente e escores\nacp_S$rotation = -acp_S$rotation\nacp_S$x = -acp_S$x\n\n## Loadings (cargas)\nacp_S$rotation\n\n                     PC1         PC2         PC3\nGranho_Bruto  0.42509725  0.89970680  0.09909593\nGanho_Liquido 0.02766083  0.09651661 -0.99494694\nPatrimonio    0.90472493 -0.42569029 -0.01614231"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#primeiro-exemplo-mingoti2007-4",
    "href": "aulas/analise_comp_princ/comp_princ.html#primeiro-exemplo-mingoti2007-4",
    "title": "Análise de Componentes Principais",
    "section": "Primeiro Exemplo: (Mingoti,2007)",
    "text": "Primeiro Exemplo: (Mingoti,2007)\n\\[Y_1 = 0,425 \\times \\text{GB} + 0,028 \\times \\text{GL} + 0,905 \\times \\text{PA} \\Longrightarrow\n94,18\\% \\text{ da informação total de } \\mathbf{x}\\]\n\n\nQual a variável mais importante para \\(Y_1\\)?\n\n\n\nPatrimônio\n\n\n\n\n\n\n\n\n\n\n\n\n🤔Será?"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#componentes-principais-variáveis-padronizadas",
    "href": "aulas/analise_comp_princ/comp_princ.html#componentes-principais-variáveis-padronizadas",
    "title": "Análise de Componentes Principais",
    "section": "Componentes Principais: variáveis padronizadas",
    "text": "Componentes Principais: variáveis padronizadas\n\nPadronização do vetor aleatório \\(\\mathbf{x}\\):\n\n\\[\\boldsymbol{z} = \\boldsymbol{D}^{-1}(\\mathbf{x} - \\boldsymbol{\\mu})\\]\n\n\n\\(\\boldsymbol{D}\\): matriz diagonal de desvios-padrão\n\n\n\n\nVariável padronizada: \\(Z_i = \\dfrac{X_i - \\mu_i}{\\sqrt{\\sigma_{ii}}}\\)\n\n\n\n\nMatriz de covariâncias de \\(\\boldsymbol{z}\\)\n\n\\[\\text{Cov}(\\boldsymbol{z}) = \\boldsymbol{D}^{-1}\\boldsymbol{\\Sigma}  \\boldsymbol{D}^{-1} = \\boldsymbol{P} = \\text{Cor}(\\mathbf{x})\\]\n\n\n\nComponentes principais de \\(\\boldsymbol{z}\\): obtidas dos autovalores e autovetores de \\(\\boldsymbol{P}\\)"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#componentes-principais-variáveis-padronizadas-1",
    "href": "aulas/analise_comp_princ/comp_princ.html#componentes-principais-variáveis-padronizadas-1",
    "title": "Análise de Componentes Principais",
    "section": "Componentes Principais: variáveis padronizadas",
    "text": "Componentes Principais: variáveis padronizadas\n\nA \\(j\\)-ésima componente principal da matriz \\(\\boldsymbol{P}\\):\n\n\\[Y_j = \\boldsymbol{e}_j^t \\boldsymbol{z} =  \\boldsymbol{e}_j^t \\boldsymbol{D}^{-1}(\\mathbf{x} - \\boldsymbol{\\mu}) = {e}_{j1}Z_1 + {e}_{j2}Z_2 + \\cdots + {e}_{jp}Z_p\\]\nsendo \\(\\boldsymbol{e}_j\\), o \\(j\\)-ésimo autovetor da matriz \\(\\boldsymbol{P}\\), \\(j = 1, \\cdots p\\).\n\n\nVariância total de \\(\\boldsymbol{P}\\)\n\n\\[\\sum \\limits_{j=1}^p \\text{Var}(Y_j) = \\sum \\limits_{j=1}^p \\text{Var}(Z_j) = p\\]"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#componentes-principais-variáveis-padronizadas-2",
    "href": "aulas/analise_comp_princ/comp_princ.html#componentes-principais-variáveis-padronizadas-2",
    "title": "Análise de Componentes Principais",
    "section": "Componentes Principais: variáveis padronizadas",
    "text": "Componentes Principais: variáveis padronizadas\n\nProporção da variância populacional padronizada devido à \\(j\\)-ésima componente\n\n\\[\\dfrac{\\text{Var}({Y}_j)}{\\text{Variância total de } \\boldsymbol{z}} = \\dfrac{{\\lambda}_j}{\\text{tr} (\\boldsymbol{P})} =  \\dfrac{{\\lambda}_j}{p}\\]\n\n\nCorrelação entre componente \\(Y_j\\) e a variável padronizada \\(Z_k\\):\n\n\\[\\rho_{Y_j,Z_k} = e_{jk} \\sqrt{\\lambda_j}\\]"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#observações",
    "href": "aulas/analise_comp_princ/comp_princ.html#observações",
    "title": "Análise de Componentes Principais",
    "section": "Observações",
    "text": "Observações\n\nAs componentes principais não são invariantes à mudanças de escala. Os resultados são diferentes quando se faz a análise utilizando a matriz de covariâncias e a matriz de correlações.\n\n\n\nAs componentes obtidas a partir da matriz de covariâncias são influenciadas pelas variáveis de maior variância. A matriz de correlações, em geral, é a melhor opção quando as variâncias são muito heterogêneas.\n\n\n\n\nUm valor pequeno incomum para o último autovalor da matriz de covariâncias (ou correlação) amostral pode indicar uma dependência linear não detectada no conjunto de dados.\n\n\n\n\nValores grandes de autovalores (e correspondentes autovetores) são importantes em uma análise."
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#quantas-componentes-devem-ser-retidas",
    "href": "aulas/analise_comp_princ/comp_princ.html#quantas-componentes-devem-ser-retidas",
    "title": "Análise de Componentes Principais",
    "section": "Quantas componentes devem ser retidas?",
    "text": "Quantas componentes devem ser retidas?\n\nCritério de Kaiser: Trata-se de uma regra prática (heurística) para decidir quantas componentes principais manter em uma PCA extraída através da matriz de correlações. Ele diz:\n\n\nRetenha apenas as componentes associadas a um autovalor maior que 1.\n\n\n\\(\\lambda = 1\\) → a componente explica tanta variabilidade quanto 1 variável original\n\\(\\lambda &gt; 1\\) → explica mais variabilidade do que qualquer variável isolada\n\\(\\lambda &lt; 1\\) → explica menos variabilidade do que uma única variável → então não “vale a pena”"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#quantas-componentes-devem-ser-retidas-1",
    "href": "aulas/analise_comp_princ/comp_princ.html#quantas-componentes-devem-ser-retidas-1",
    "title": "Análise de Componentes Principais",
    "section": "Quantas componentes devem ser retidas?",
    "text": "Quantas componentes devem ser retidas?\n\nMédia dos autovalores: É um critério alternativo quando a PCA é feita sobre matriz de covariâncias.\n\n\nSe PCA foi feita na matriz de covariâncias, retenha PCs com autovalores maiores que a média dos autovalores.\n\nEntão, se uma componente tem autovalor:\n\\[\\lambda_k &gt; \\dfrac{\\sum \\limits_{j=1}^p \\lambda_j}{p}\\]\nela retém mais informação do que a variância média por dimensão do espaço original."
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#quantas-componentes-devem-ser-retidas-2",
    "href": "aulas/analise_comp_princ/comp_princ.html#quantas-componentes-devem-ser-retidas-2",
    "title": "Análise de Componentes Principais",
    "section": "Quantas componentes devem ser retidas?",
    "text": "Quantas componentes devem ser retidas?\n\nScreeplot: É um gráfico que coloca, no eixo X, o número da componente principal e, no eixo Y, o autovalor (ou a variância explicada).\n\n\nO ponto onde a curva deixa de cair abruptamente e começa a “horizontalizar” é onde você para de manter componentes.\n\n\nA ideia é identificar o “cotovelo” da curva.\n\nantes do cotovelo: cada componente adiciona bastante variância\ndepois do cotovelo: os autovalores ficam “quase uma linha reta” → só ruído"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#quantas-componentes-devem-ser-retidas-3",
    "href": "aulas/analise_comp_princ/comp_princ.html#quantas-componentes-devem-ser-retidas-3",
    "title": "Análise de Componentes Principais",
    "section": "Quantas componentes devem ser retidas?",
    "text": "Quantas componentes devem ser retidas?"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#quantas-componentes-devem-ser-retidas-4",
    "href": "aulas/analise_comp_princ/comp_princ.html#quantas-componentes-devem-ser-retidas-4",
    "title": "Análise de Componentes Principais",
    "section": "Quantas componentes devem ser retidas?",
    "text": "Quantas componentes devem ser retidas?\n\nReter o número de componentes principais que acumulem pelo menos certa porcentagem da variabilidade total dos dados, digamos 70%."
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#quantas-componentes-devem-ser-retidas-5",
    "href": "aulas/analise_comp_princ/comp_princ.html#quantas-componentes-devem-ser-retidas-5",
    "title": "Análise de Componentes Principais",
    "section": "Quantas componentes devem ser retidas?",
    "text": "Quantas componentes devem ser retidas?\n\nParallel Analysis (Horn, 1965): É o método moderno mais recomendado para decidir quantos componentes reter em PCA.\n\n\nCompare seus autovalores reais com autovalores esperados ao acaso."
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#quantas-componentes-devem-ser-retidas-6",
    "href": "aulas/analise_comp_princ/comp_princ.html#quantas-componentes-devem-ser-retidas-6",
    "title": "Análise de Componentes Principais",
    "section": "Quantas componentes devem ser retidas?",
    "text": "Quantas componentes devem ser retidas?\n\nO procedimento envolve:\n\nCalcular os autovalores dos dados originais.\nGerar dados aleatórios com a mesma estrutura dos dados originais.\nCalcular autovalores para os dados aleatórios, repetindo o processo muitas vezes para obter uma distribuição amostral.\nComparar os autovalores reais com os autovalores médios ou percentis (como o 95º) dos dados simulados.\nReter os componentes cujos autovalores reais são maiores do que os autovalores aleatórios correspondentes."
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#quantas-componentes-devem-ser-retidas-7",
    "href": "aulas/analise_comp_princ/comp_princ.html#quantas-componentes-devem-ser-retidas-7",
    "title": "Análise de Componentes Principais",
    "section": "Quantas componentes devem ser retidas?",
    "text": "Quantas componentes devem ser retidas?\n\n\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  1"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#quantas-componentes-devem-ser-retidas-8",
    "href": "aulas/analise_comp_princ/comp_princ.html#quantas-componentes-devem-ser-retidas-8",
    "title": "Análise de Componentes Principais",
    "section": "Quantas componentes devem ser retidas?",
    "text": "Quantas componentes devem ser retidas?\n\nInterpretação desse gráfico\n\nlinha azul = autovalores reais dos seus dados\nlinhas vermelha / pontilhada = autovalores esperados pelo acaso (parallel analysis)\n\n\n\n\nRegra: retenha apenas os componentes cuja linha azul está acima da linha vermelha.\n\nParallel Analysis (Horn) está dizendo: retenha 1 componente principal."
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#interpretação-das-componentes-principais",
    "href": "aulas/analise_comp_princ/comp_princ.html#interpretação-das-componentes-principais",
    "title": "Análise de Componentes Principais",
    "section": "Interpretação das componentes principais",
    "text": "Interpretação das componentes principais\n\nEm geral, quando existe uma alta correlação positiva entre todas as variáveis, os sinais associados às variáveis coincidem na primeira componente principal.\n\n\n\nNeste caso, a primeira componente principal pode ser interpretada como um índice global, calculado como uma média ponderada de todas as variáveis.\n\n\n\n\nO restante das componentes, normalmente possuem pesos negativos e positivos e são interpretadas como um contraste entre grupos de variáveis."
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo",
    "href": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo",
    "title": "Análise de Componentes Principais",
    "section": "Voltando ao Exemplo",
    "text": "Voltando ao Exemplo\n\n12 empresas, 3 variáveis: ganho bruto (\\(X_1\\)), ganho líquido (\\(X_2\\)) e patrimônio acumulado (\\(X_3\\))\n\n\n\n\n\n\nEmpresa\n\n\nGanho bruto \\((X_1)\\)\n\n\nGanho líquido \\((X_2)\\)\n\n\nPatrimônio \\((X_3)\\)\n\n\n\n\n\n\nE1\n\n\n9893\n\n\n564\n\n\n17689\n\n\n\n\nE2\n\n\n8776\n\n\n389\n\n\n17359\n\n\n\n\nE3\n\n\n13572\n\n\n1103\n\n\n18597\n\n\n\n\nE4\n\n\n6455\n\n\n743\n\n\n8745\n\n\n\n\nE5\n\n\n5129\n\n\n203\n\n\n14397\n\n\n\n\nE6\n\n\n5432\n\n\n215\n\n\n3467\n\n\n\n\nE7\n\n\n3807\n\n\n385\n\n\n4679\n\n\n\n\nE8\n\n\n3423\n\n\n187\n\n\n6754\n\n\n\n\nE9\n\n\n3708\n\n\n127\n\n\n2275\n\n\n\n\nE10\n\n\n3294\n\n\n297\n\n\n6754\n\n\n\n\nE11\n\n\n5433\n\n\n432\n\n\n5589\n\n\n\n\nE12\n\n\n6287\n\n\n451\n\n\n8972"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-1",
    "href": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-1",
    "title": "Análise de Componentes Principais",
    "section": "Voltando ao Exemplo",
    "text": "Voltando ao Exemplo"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-2",
    "href": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-2",
    "title": "Análise de Componentes Principais",
    "section": "Voltando ao Exemplo",
    "text": "Voltando ao Exemplo\n\ndados %&gt;% \n  scale(center=TRUE, scale=TRUE) \n\n    Granho_Bruto Ganho_Liquido Patrimonio\nE1   1.173173832    0.50452133  1.3779065\nE2   0.811732638   -0.12914781  1.3216486\nE3   2.363632341    2.45622228  1.5327010\nE4   0.060698607    1.15267434 -0.1468530\nE5  -0.368371244   -0.80264757  0.8166914\nE6  -0.270325871   -0.75919598 -1.0466384\nE7  -0.796146767   -0.14363167 -0.8400185\nE8  -0.920402290   -0.86058304 -0.4862757\nE9  -0.828181394   -1.07784103 -1.2498488\nE10 -0.962144379   -0.46227672 -0.4862757\nE11 -0.270002289    0.02655375 -0.6848831\nE12  0.006336816    0.09535212 -0.1081544\nattr(,\"scaled:center\")\n Granho_Bruto Ganho_Liquido    Patrimonio \n    6267.4167      424.6667     9606.4167 \nattr(,\"scaled:scale\")\n Granho_Bruto Ganho_Liquido    Patrimonio \n    3090.4059      276.1694     5865.8429"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-3",
    "href": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-3",
    "title": "Análise de Componentes Principais",
    "section": "Voltando ao Exemplo",
    "text": "Voltando ao Exemplo\n\ndados %&gt;% \n  scale(center=TRUE, scale=TRUE) %&gt;% \n  boxplot()"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-4",
    "href": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-4",
    "title": "Análise de Componentes Principais",
    "section": "Voltando ao Exemplo",
    "text": "Voltando ao Exemplo\n\n## Analise de Componentes Principais utilizando a matriz de correlacoes (Mais aconselhavel, neste caso)\nacp_R = prcomp(dados, scale. = TRUE)\n\n## Proporcao da variacao explicada\nsummary(acp_R)\n\nImportance of components:\n                          PC1    PC2     PC3\nStandard deviation     1.5788 0.6508 0.28978\nProportion of Variance 0.8308 0.1412 0.02799\nCumulative Proportion  0.8308 0.9720 1.00000\n\n## Loadings (cargas)\nacp_R$rotation\n\n                     PC1          PC2        PC3\nGranho_Bruto  -0.6167027  0.001267206  0.7871952\nGanho_Liquido -0.5567945  0.706196936 -0.4373395\nPatrimonio    -0.5564690 -0.708014324 -0.4348080\n\n## Opcional: trocar sinal da primeira componente e escores\nacp_R$rotation = -acp_R$rotation\nacp_R$x = -acp_R$x\n\n## Loadings (cargas)\nacp_R$rotation\n\n                    PC1          PC2        PC3\nGranho_Bruto  0.6167027 -0.001267206 -0.7871952\nGanho_Liquido 0.5567945 -0.706196936  0.4373395\nPatrimonio    0.5564690  0.708014324  0.4348080"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-5",
    "href": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-5",
    "title": "Análise de Componentes Principais",
    "section": "Voltando ao Exemplo",
    "text": "Voltando ao Exemplo\n\n## Escores das componentes principais\nYr = acp_R$x\nYr %&gt;% head()\n\n          PC1        PC2        PC3\nE1  1.7711764  0.6177995 -0.1037449\nE2  1.1641454  1.0259213 -0.1208101\nE3  3.6781699 -0.6523976 -0.1200063\nE4  0.5975165 -0.9180660  0.3924755\nE5 -0.2196218  1.1455233  0.2940545\nE6 -1.1718486 -0.2045506 -0.5743139\n\n\n\n## Matriz de correlacoes entre variaveis originais e componentes principais\nRyx_R = cor(dados,Yr)\nRyx_R\n\n                    PC1           PC2        PC3\nGranho_Bruto  0.9736351 -0.0008246563 -0.2281098\nGanho_Liquido 0.8790534 -0.4595699002  0.1267302\nPatrimonio    0.8785396  0.4607525968  0.1259966"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-6",
    "href": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-6",
    "title": "Análise de Componentes Principais",
    "section": "Voltando ao Exemplo",
    "text": "Voltando ao Exemplo\n\nfa.parallel(dados %&gt;% scale(center=TRUE, scale=TRUE), fa=\"pc\", n.iter=1000)\n\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  1"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-7",
    "href": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-7",
    "title": "Análise de Componentes Principais",
    "section": "Voltando ao Exemplo",
    "text": "Voltando ao Exemplo\n\\[Z_{GB} = \\dfrac{\\text{Ganho Bruto} - \\overline{\\text{Ganho Bruto}}}{s_{\\text{Ganho Bruto}}}\\]\n\\[Z_{GL} = \\dfrac{\\text{Ganho Líquido} - \\overline{\\text{Ganho Líquido}}}{s_{\\text{Ganho Líquido}}}\\]\n\\[Z_{PA} = \\dfrac{\\text{Patrimônio} - \\overline{\\text{Patrimônio}}}{s_{\\text{Patrimônio}}}\\]"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-8",
    "href": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-8",
    "title": "Análise de Componentes Principais",
    "section": "Voltando ao Exemplo",
    "text": "Voltando ao Exemplo\n\\[Y_1 =  0,617 \\times Z_{GB} + 0,557 \\times Z_{GL} + 0,556 \\times Z_{PA} \\Longrightarrow\n83,08\\% \\text{ da informação total de } \\boldsymbol{z}\\]\n\n\nInterpretação: É basicamente um índice de desempenho global da empresa. O coeficiente de maior grandeza numérica desta componente é relativo a ganho bruto enquanto que os demais coeficientes são aproximadamente iguais. Quanto maior os valores de ganhos brutos e líquido e patrimônio da empresa, maior será o valor numérico da componente. Além disso, todas as três variáveis possui alta correlação com essa componente, indicando serem importantes na composição da mesma."
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-9",
    "href": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-9",
    "title": "Análise de Componentes Principais",
    "section": "Voltando ao Exemplo",
    "text": "Voltando ao Exemplo\n\\[Y_2 =  -0,001 \\times Z_{GB} - 0,706 \\times Z_{GL} + 0,708 \\times Z_{PA} \\Longrightarrow\n14,12\\% \\text{ da informação total de } \\boldsymbol{z}\\]\n\n\nInterpretação: É uma comparação entre as variáveis ganho líquido e patrimônio, sendo que essas duas variáveis possuem igual importância na composição da mesma. Valores próximos de zero dessa componente indicam empresas com um certo equilíbrio entre ganho líquido e patrimônio acumulado no período."
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-10",
    "href": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-10",
    "title": "Análise de Componentes Principais",
    "section": "Voltando ao Exemplo",
    "text": "Voltando ao Exemplo\n\nfviz_pca_biplot(acp_R, repel = TRUE,\n                col.var = \"#2E9FDF\", # Variables color\n                col.ind = \"#696969\"  # Individuals color\n                )"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-11",
    "href": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-11",
    "title": "Análise de Componentes Principais",
    "section": "Voltando ao Exemplo",
    "text": "Voltando ao Exemplo\nInterpretação do biplot\n\nAs setas indicam em que direção a variável aumenta. os indivíduos que estão posicionados no sentido da seta são os que têm valores maiores naquela variável.\n\n\n\nÂngulo entre a seta (variável) e o eixo PC1: O cosseno do ângulo entre a variável e a PC é exatamente o loading.\n\n\\[\\cos(\\theta_j, PC_i) = \\text{loading}_{X_j, PC_i}\\]"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-12",
    "href": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-12",
    "title": "Análise de Componentes Principais",
    "section": "Voltando ao Exemplo",
    "text": "Voltando ao Exemplo\nentão:\n\nse a seta está quase colada no eixo \\(PC_i\\)\n\nloading próximo de +1 → variável altamente alinhada com a componente\né variável que define o componente principal\n\nse a seta faz um ângulo grande (perto de 90°) com \\(PC_i\\)\n\nloading \\(\\approx 0\\) → essa variável não contribui para a componente\n\nse a seta aponta para o lado oposto da \\(PC_i\\) (180°)\n\nloading \\(\\approx -1\\) → variável altamente alinhada negativamente"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-13",
    "href": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-13",
    "title": "Análise de Componentes Principais",
    "section": "Voltando ao Exemplo",
    "text": "Voltando ao Exemplo\n\nObservação: Quando a PCA é feita na matriz de correlações, os loadings são exatamente as correlações entre as variáveis originais e as componentes principais. Quando usamos matriz de covariâncias, os loadings refletem contribuição em variância e o cosseno do ângulo não corresponde numericamente à correlação."
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-14",
    "href": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-14",
    "title": "Análise de Componentes Principais",
    "section": "Voltando ao Exemplo",
    "text": "Voltando ao Exemplo\n\nPC1 (Dim1 = 83.1%): Praticamente toda a informação relevante está aqui.\n\n\nPC1 representa um eixo geral de nível financeiro / escala econômica. Quanto maior Patrimônio / Ganho Bruto / Ganho Líquido → mais à direita o indivíduo aparece.\n\n\nTodas variáveis apontam para a direita, e com ângulos semelhantes → altíssima correlação entre elas"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-15",
    "href": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-15",
    "title": "Análise de Componentes Principais",
    "section": "Voltando ao Exemplo",
    "text": "Voltando ao Exemplo\n\nQuem está mais pra direita (E1, E2, E3) → são aqueles com valores altos nessas variáveis\nQuem está mais pra esquerda (E8, E9, E10…) → são os com valores baixos\nA empresa E12 posiciona-se muito próxima à origem do plano principal, indicando um perfil mediano em todas as variáveis financeiras consideradas.\n\nEla não apresenta características extremas nem para valores altos, nem para valores baixos, sendo portanto uma empresa altamente representativa do centro da distribuição.\n\n\n\n\nPC2 (Dim2 = 14.1%): quase não traz informação nova → setas não sobem muito, elas estão quase horizontais\n\nnão existe uma segunda dimensão “conceitual” forte"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#voltando-a-exemplo",
    "href": "aulas/analise_comp_princ/comp_princ.html#voltando-a-exemplo",
    "title": "Análise de Componentes Principais",
    "section": "Voltando a Exemplo",
    "text": "Voltando a Exemplo\n\n# Contribuições das variáveis para a PC1\nfviz_contrib(acp_R, choice = \"var\", axes = 1)"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-16",
    "href": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-16",
    "title": "Análise de Componentes Principais",
    "section": "Voltando ao Exemplo",
    "text": "Voltando ao Exemplo\n\nEsse gráfico mostra o quanto cada variável explica/contribui para a formação do primeiro componente principal.\n\nGanho Bruto contribuiu um pouco mais\nGanho Liquido e Patrimonio contribuem praticamente igual e muito próximo\n\n\n\n\nA linha vermelha tracejada é a contribuição média esperada (se todas contribuíssem igual).\n\nComo temos 3 variáveis → contribuição média = \\(100\\%/3 \\approx 33.33\\%\\)"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-17",
    "href": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-17",
    "title": "Análise de Componentes Principais",
    "section": "Voltando ao Exemplo",
    "text": "Voltando ao Exemplo\n\nfviz_contrib(acp_R, choice = \"ind\", axes = 1:2)"
  },
  {
    "objectID": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-18",
    "href": "aulas/analise_comp_princ/comp_princ.html#voltando-ao-exemplo-18",
    "title": "Análise de Componentes Principais",
    "section": "Voltando ao Exemplo",
    "text": "Voltando ao Exemplo\n\nEsse gráfico mostra quais indivíduos contribuem mais para definir o plano principal da PC, isto é, quais observações estão orientando a direção das componentes principais.\n\n\n\nA empresa E3 é a maior influenciadora da PCA: sua contribuição para o plano principal é muito superior à média (linha vermelha), indicando que ela é um caso extremo na direção da primeira componente. Esse ponto está orientando de forma dominante a estrutura da análise."
  },
  {
    "objectID": "analise_dados/brands.html",
    "href": "analise_dados/brands.html",
    "title": "Mapas Perceptuais com PCA: estudo de marcas",
    "section": "",
    "text": "Nesta análise, vamos construir mapas perceptuais de marcas por meio da Análise de Componentes Principais (PCA). O objetivo é reduzir nove atributos avaliados em escala de 1 a 10 para poucas dimensões interpretáveis, preservando a maior parte possível da variabilidade e oferecendo insights sobre similaridade entre marcas.\nVamos usar a base brand.csv (100 respondentes, 10 marcas a–j, atributos: perform, leader, latest, fun, serious, bargain, value, trendy, rebuy).\n\nCaracterísticas das marcas: perguntas\n\n\n\n\n\n\nAtributo\n\n\nExemplo de pergunta\n\n\n\n\n\n\nperform\n\n\nMarca tem um forte desempenho?\n\n\n\n\nleader\n\n\nMarca é líder no mercado?\n\n\n\n\nlatest\n\n\nMarca tem os produtos mais recentes?\n\n\n\n\nfun\n\n\nMarca é divertida?\n\n\n\n\nserious\n\n\nMarca é séria?\n\n\n\n\nbargain\n\n\nProdutos da marca são uma pechincha?\n\n\n\n\nvalue\n\n\nProdutos da marca possuem um bom valor?\n\n\n\n\ntrendy\n\n\nMarca está na moda?\n\n\n\n\nrebuy\n\n\nEu compraria a marca novamente?"
  },
  {
    "objectID": "analise_dados/brands.html#leitura-dos-dados",
    "href": "analise_dados/brands.html#leitura-dos-dados",
    "title": "Mapas Perceptuais com PCA: estudo de marcas",
    "section": "2.1 Leitura dos dados",
    "text": "2.1 Leitura dos dados\n\npath &lt;- \"https://raw.githubusercontent.com/tiagomartin/est014/refs/heads/master/dados/brand.csv\"\nbrands &lt;- read_csv(path, show_col_types = FALSE) %&gt;%\n  clean_names() \n\nbrands %&gt;% \n  glimpse()\n\nRows: 1,000\nColumns: 10\n$ perform &lt;dbl&gt; 2, 1, 2, 1, 1, 2, 1, 2, 2, 3, 2, 1, 3, 1, 3, 3, 2, 3, 2, 2, 2,…\n$ leader  &lt;dbl&gt; 4, 1, 3, 6, 1, 8, 1, 1, 1, 1, 1, 2, 3, 5, 1, 2, 1, 8, 5, 3, 2,…\n$ latest  &lt;dbl&gt; 8, 4, 5, 10, 5, 9, 5, 7, 8, 9, 5, 7, 10, 7, 3, 7, 6, 9, 10, 9,…\n$ fun     &lt;dbl&gt; 8, 7, 9, 8, 8, 5, 7, 5, 10, 8, 6, 7, 10, 10, 6, 6, 7, 9, 8, 10…\n$ serious &lt;dbl&gt; 2, 1, 2, 3, 1, 3, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 4, 2, 2, 1,…\n$ bargain &lt;dbl&gt; 9, 1, 9, 4, 9, 8, 5, 8, 7, 3, 1, 3, 3, 1, 3, 10, 1, 7, 6, 2, 5…\n$ value   &lt;dbl&gt; 7, 1, 5, 5, 9, 7, 1, 7, 7, 3, 1, 2, 3, 3, 4, 5, 3, 10, 10, 6, …\n$ trendy  &lt;dbl&gt; 4, 2, 1, 2, 1, 1, 1, 7, 5, 4, 1, 1, 3, 3, 4, 1, 5, 4, 5, 5, 3,…\n$ rebuy   &lt;dbl&gt; 6, 2, 6, 1, 1, 2, 1, 1, 1, 1, 2, 5, 3, 1, 2, 3, 1, 2, 2, 2, 2,…\n$ brand   &lt;chr&gt; \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a…\n\nbrands %&gt;% \n  summary()\n\n    perform           leader           latest            fun        \n Min.   : 1.000   Min.   : 1.000   Min.   : 1.000   Min.   : 1.000  \n 1st Qu.: 1.000   1st Qu.: 2.000   1st Qu.: 4.000   1st Qu.: 4.000  \n Median : 4.000   Median : 4.000   Median : 7.000   Median : 6.000  \n Mean   : 4.488   Mean   : 4.417   Mean   : 6.195   Mean   : 6.068  \n 3rd Qu.: 7.000   3rd Qu.: 6.000   3rd Qu.: 9.000   3rd Qu.: 8.000  \n Max.   :10.000   Max.   :10.000   Max.   :10.000   Max.   :10.000  \n    serious          bargain           value            trendy     \n Min.   : 1.000   Min.   : 1.000   Min.   : 1.000   Min.   : 1.00  \n 1st Qu.: 2.000   1st Qu.: 2.000   1st Qu.: 2.000   1st Qu.: 3.00  \n Median : 4.000   Median : 4.000   Median : 4.000   Median : 5.00  \n Mean   : 4.323   Mean   : 4.259   Mean   : 4.337   Mean   : 5.22  \n 3rd Qu.: 6.000   3rd Qu.: 6.000   3rd Qu.: 6.000   3rd Qu.: 7.00  \n Max.   :10.000   Max.   :10.000   Max.   :10.000   Max.   :10.00  \n     rebuy           brand          \n Min.   : 1.000   Length:1000       \n 1st Qu.: 1.000   Class :character  \n Median : 3.000   Mode  :character  \n Mean   : 3.727                     \n 3rd Qu.: 5.000                     \n Max.   :10.000                     \n\n\nDe forma geral, observa-se que os atributos mais “positivos” em média são os relacionados à modernidade/diversão da marca (latest ≈ 6,2 e fun ≈ 6,1), enquanto atributos ligados a recompra (rebuy ≈ 3,7) e preço/valor percebido (bargain e value ≈ 4,3) apresentam avaliações mais baixas. Além disso, vários atributos têm 1º quartil baixo (ex.: perform Q1 = 1; bargain/value Q1 = 2), sugerindo que há uma parcela de consumidores que de fato avalia marca(s) de forma bem negativa em alguns aspectos. Esse padrão indica que a PCA provavelmente encontrará uma dimensão relacionada à inovação/modernidade e outra relacionada a valor/preço e recompra, uma vez que esses conjuntos de atributos apresentam distribuições centrais distintas e potencialmente estruturadas em blocos de correlação."
  },
  {
    "objectID": "analise_dados/brands.html#visualização-exploratória",
    "href": "analise_dados/brands.html#visualização-exploratória",
    "title": "Mapas Perceptuais com PCA: estudo de marcas",
    "section": "2.2 Visualização exploratória",
    "text": "2.2 Visualização exploratória\n\nattr_names &lt;- setdiff(names(brands), \"brand\")\n\nX &lt;- brands %&gt;% select(all_of(attr_names))\n\nX_long &lt;- brands %&gt;%\npivot_longer(all_of(attr_names),\nnames_to = \"atributo\", values_to = \"nota\")\n\n# Guardar a coluna de marca (fator) separada\nbrand_fac &lt;- brands %&gt;% mutate(brand = as.factor(brand)) %&gt;% pull(brand)\n\n# Padronizar as variáveis\nX_scaled &lt;- scale(X)\ndf_scaled &lt;- data.frame(brand_fac = brand_fac, X_scaled)\n\n# histogramas facetados\np_hist &lt;- ggplot(X_long, aes(nota)) +\ngeom_histogram(bins = 9, color = \"white\") +\nfacet_wrap(~ atributo, scales=\"free_y\") +\nlabs(title = \"Distribuição dos atributos (histogramas)\") +\ntheme_minimal()\n\n\np_hist\n\n\n\n\n\n\n\n\nObserva-se que a maior parte dos atributos tem uma massa grande de notas baixas (1 a 4), exceto latest, fun e (em menor grau) trendy, que apresentam caudas mais fortes para notas altas. Isso sugere que a percepção média de “inovação/modernidade/diversão” tende a ser mais positiva do que a percepção de “valor/preço/desempenho”. Além disso, rebuy é visivelmente o mais “puxado” para notas muito baixas, indicando que a disposição em recomprar é o atributo mais crítico (e possivelmente o mais difícil de conquistar), o que antecipa que a PCA provavelmente vai separar dimensões ligadas a modernidade/diversão versus valor/preço e recompra.\n\n# boxplots facetados por atributo\np_box &lt;- ggplot(X_long, aes(x = atributo, y = nota)) +\ngeom_boxplot() +\ncoord_flip() +\nlabs(title = \"Distribuição dos atributos (boxplots)\") +\ntheme_minimal()\n\n\np_box\n\n\n\n\n\n\n\n\nO boxplot reforça o padrão visto nos histogramas: os atributos ligados a “inovação/diversão” (latest, fun, trendy) apresentam medianas mais altas e intervalos interquartílicos deslocados para a direita, enquanto atributos relacionados a valor/preço (bargain, value) e especialmente rebuy permanecem com centrais mais baixas. Além disso, há bastante dispersão em todos os atributos, sugerindo heterogeneidade real na percepção das marcas. Esse contraste estrutural entre atributos “mais modernos” e atributos “valor/recompra” é exatamente o tipo de padrão latente que a PCA tende a capturar, separando dimensões latentes distintas no mapa perceptual."
  },
  {
    "objectID": "analise_dados/brands.html#matriz-de-correlações",
    "href": "analise_dados/brands.html#matriz-de-correlações",
    "title": "Mapas Perceptuais com PCA: estudo de marcas",
    "section": "2.3 Matriz de correlações",
    "text": "2.3 Matriz de correlações\n\ncormat &lt;- cor(X)\ncorrplot(cormat, method = \"color\", type = \"upper\", addCoef.col = \"black\",\ntl.col = \"black\", tl.srt = 45, number.cex = .6,\ntitle = \"Matriz de correlações entre atributos\")\n\n\n\n\n\n\n\n\nA matriz de correlações mostra claramente dois sub-blocos relevantes: (i) bargain e value são fortemente correlacionados (0,74), compondo um núcleo evidente de “valor/preço percebido”; (ii) latest e trendy também se correlacionam positivamente (0,63), sugerindo um eixo de “modernidade/moda”. Além disso, rebuy correlaciona moderadamente com value (0,51) e perform (0,31), indicando que intenção de recompra parece ancorar-se tanto em percepção de valor quanto em desempenho.\n\ntidy_cors &lt;- X %&gt;%\n  correlate() %&gt;%\n  stretch()\ngraph_cors &lt;- tidy_cors %&gt;%\n  filter(abs(r) &gt; .3) %&gt;%\n  graph_from_data_frame(directed = FALSE)\n\nggraph(graph_cors) + geom_edge_link() + geom_node_point() +   ## Variaveis aparentam se agrupar em dois grupos\n  geom_node_text(aes(label = name))\n\n\n\n\n\n\n\n\nA visualização em rede ajuda a enxergar que os atributos não formam um único bloco homogêneo: existe um cluster bem definido formado por latest, trendy, value e bargain, o que reforça a ideia de um eixo “moda/modernidade/valor”. Em paralelo, perform, leader e serious aparecem mais conectados entre si, sugerindo outra dimensão ligada a “desempenho/liderança/seriedade”. rebuy fica numa posição intermediária, fazendo a ponte entre esses dois polos, o que é coerente com a lógica de que intenção de recompra é influenciada tanto pela percepção de modernidade quanto pela percepção de valor/desempenho."
  },
  {
    "objectID": "analise_dados/brands.html#loadings-correlações-e-interpretação-geométrica",
    "href": "analise_dados/brands.html#loadings-correlações-e-interpretação-geométrica",
    "title": "Mapas Perceptuais com PCA: estudo de marcas",
    "section": "3.1 Loadings, correlações e interpretação geométrica",
    "text": "3.1 Loadings, correlações e interpretação geométrica\nTemos que as componentes encontradas são dadas por:\n\n## Loadings (cargas)\npca$rotation\n\n               PC1         PC2         PC3         PC4         PC5        PC6\nperform  0.2374679  0.41991179  0.03854006 -0.52630873 -0.46793435  0.3370676\nleader   0.2058257  0.52381901 -0.09512739 -0.08923461  0.29452974  0.2968860\nlatest  -0.3703806  0.20145317 -0.53273054  0.21410754 -0.10586676  0.1742059\nfun     -0.2510601 -0.25037973 -0.41781346 -0.75063952  0.33149429 -0.1405367\nserious  0.1597402  0.51047254 -0.04067075  0.09893394  0.55515540 -0.3924874\nbargain  0.3991731 -0.21849698 -0.48989756  0.16734345  0.01257429  0.1393966\nvalue    0.4474562 -0.18980822 -0.36924507  0.15118500  0.06327757  0.2195327\ntrendy  -0.3510292  0.31849032 -0.37090530  0.16764432 -0.36649697 -0.2658186\nrebuy    0.4390184  0.01509832 -0.12461593 -0.13031231 -0.35568769 -0.6751400\n                 PC7         PC8         PC9\nperform -0.364179109 -0.14444718 -0.05223384\nleader   0.613674301  0.28766118  0.17889453\nlatest   0.185480310 -0.64290436 -0.05750244\nfun      0.007114761  0.07461259 -0.03153306\nserious -0.445302862 -0.18354764 -0.09072231\nbargain -0.288264900  0.05789194  0.64720849\nvalue   -0.017163011  0.14829295 -0.72806108\ntrendy  -0.153572108  0.61450289 -0.05907022\nrebuy    0.388656160 -0.20210688  0.01720236\n\n\n\\[Y_1 = 0,24 \\times Z_{\\text{perform}} + 0,21 \\times Z_{\\text{leader}} - 0,37 \\times Z_{\\text{latest}} - 0,25 \\times Z_{\\text{fun}} + 0,16 \\times Z_{\\text{serious}} + 0,40 \\times Z_{\\text{bargain}} + 0,45 \\times Z_{\\text{value}} - 0,35 \\times Z_{\\text{trendy}} + 0,44 \\times Z_{\\text{rebuy}}\\]\ne,\n\\[Y_2 = 0,42 \\times Z_{\\text{perform}} + 0,52 \\times Z_{\\text{leader}} + 0,20 \\times Z_{\\text{latest}} - 0,25 \\times Z_{\\text{fun}} + 0,51 \\times Z_{\\text{serious}} - 0,22 \\times Z_{\\text{bargain}} - 0,19 \\times Z_{\\text{value}} + 0,32 \\times Z_{\\text{trendy}} + 0,02 \\times Z_{\\text{rebuy}}\\]\nObservando as loadings, \\(Y_1\\) tem cargas altas positivas em value, bargain, rebuy e perform, indicando que esta dimensão parece sintetizar uma percepção geral de benefício percebido/custo-benefício/probabilidade de recompra (mais “valor concreto” da marca). Já \\(Y_2\\) tem cargas altas em leader, serious e perform, indicando que essa dimensão se aproxima mais de um eixo de prestígio/liderança/seriedade. Em síntese: \\(Y_1\\) capta “valor concreto oferecido”, \\(Y_2\\) capta “status institucional da marca”.\n\nfviz_pca_biplot(pca, repel = TRUE,\n                col.var = \"#2E9FDF\", # Variables color\n                col.ind = \"#696969\"  # Individuals color\n                )\n\n\n\n\n\n\n\n\nNo biplot com indivíduos, o padrão fica difícil de visualizar devido ao volume de pontos, sugerindo a execução da PCA usando avaliacoes agregadas por marca.\n\n## Medias dos atributos por marca\nbrand.mean &lt;- aggregate(. ~brand_fac, data = df_scaled, mean)\nrownames(brand.mean) &lt;- brand.mean[, 1]  # usar marca para os nomes das linhas\nbrand.mean &lt;- brand.mean[, -1] # remover coluna de nome de marca\nbrand.mean\n\n\n  \n\n\n\n\npca_mu &lt;- prcomp(brand.mean, scale = TRUE)\npca_mu %&gt;% \n  summary()\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6     PC7\nStandard deviation     2.1345 1.7349 0.7690 0.61498 0.50983 0.36662 0.21506\nProportion of Variance 0.5062 0.3345 0.0657 0.04202 0.02888 0.01493 0.00514\nCumulative Proportion  0.5062 0.8407 0.9064 0.94842 0.97730 0.99223 0.99737\n                           PC8     PC9\nStandard deviation     0.14588 0.04867\nProportion of Variance 0.00236 0.00026\nCumulative Proportion  0.99974 1.00000\n\n\nRealizada nova padronização, as médias agregadas têm escala um pouco diferente que os dados padronizados. A PCA neste contexto é extremamente dominante nas duas primeiras dimensões, isto é, \\(Y_1\\) sozinha já explica 50,6% da variância total, e \\(Y_1 + Y_2\\) explicam 84,1%. Ou seja, praticamente toda a estrutura relevante dos 9 atributos pode ser projetada com segurança em apenas duas dimensões.\n\nfviz_pca_biplot(pca_mu, repel = TRUE,\n                col.var = \"#2E9FDF\", # Variables color\n                col.ind = \"#696969\"  # Individuals color\n                )\n\n\n\n\n\n\n\n\nObservamos aqui o mesmo agrupamento global de atributos e estrutura de associações. A Posição das variáveis nas componentes é consistente com ACP com todas as observações, de forma que podemos prosseguir com a interpretação do gráfico. No mapa perceptual, o eixo horizontal (\\(Y_1 \\approx 50,6\\%\\)) separa claramente marcas percebidas como mais “valor/custo-benefício/recompra” à direita (“f” e “g”) versus marcas com avaliações mais baixas nesses atributos à esquerda (“a”, “d”, “j”). Já o eixo vertical (\\(Y_2 \\approx 33,4\\%\\)) separa marcas mais “sérias/líderes/desempenho” na parte superior (“b” e “c”) versus marcas associadas a um perfil mais “divertido/jovem/informal” na parte inferior (“a” e “j”). Assim, as duas dimensões encontradas sintetizam de forma bem clara duas tensões estratégicas: valor concreto (\\(Y_1\\)) e prestígio vs. diversão (\\(Y_2\\)).\nA marca “e” aparece exatamente próxima ao centro do mapa perceptual, sugerindo que ela não se destaca nem positivamente nem negativamente em nenhuma das duas dimensões principais. Em outras palavras, ela tende a ser percebida como “mediana” ou “neutra”, não é vista como uma marca particularmente moderna/divertida, nem como uma marca de alto prestígio/seriedade, e também não se posiciona como forte em valor/custo-benefício. Ela estaria, portanto, no espaço de “marca genérica”, ou seja, não agride, mas também não diferencia.\nIsso pode ser bom ou ruim: há setores em que o consumidor valoriza o “não ter arestas”, ou seja, confiança, regularidade, estabilidade, não polêmica. Por exemplo, serviços financeiros conservadores, empresas B2B que vendem confiabilidade. Em um caso assim, estar no centro pode até ser seguro. Entretanto, se o jogo competitivo naquele setor depende de posicionamento claro (ex.: “premium”, “inovadora”, “low-cost”, “divertida”, etc.), então estar no centro é perigoso. A marca “e” pode estar sendo percebida como sem personalidade. Isso é típico de marcas que não conseguem defender um território mental.\nUma alternativa para a marca “e” seria, na prática, intervir nas percepções associadas à marca (por comunicação, experiência, produto ou preço) para que ela passe a ser percebida de forma mais alinhada a um território desejado, por exemplo, se o interesse é em realizar um deslocamento da marca “e” → “c” no espaço dos atributos padronizados:\n\n## Diferenças entre a marca c e e\nbrand.mean[\"c\",] - brand.mean[\"e\",]\n\n\n  \n\n\n\nO vetor de diferença mostra que “c” é muito mais forte que e em serious (+1,18), perform (+1,21) e leader (+0,97). Estes são os atributos que definem o território de “c”: seriedade, liderança e desempenho. Por outro lado, “c” perde claramente em fun (−1,14), bargain (−1,16) e value (−0,86). Portanto, se a marca “e” quiser migrar na direção do território perceptual de “c”, ela precisa subir principalmente em seriedade, performance e liderança, uma vez que é exatamente aí que “c” se diferencia. Contudo, ela não deveria tentar deslocar ao mesmo tempo a sua eventual vantagem em diversão ou custo-benefício, pois “c” não compete nesses atributos.\nSe o interesse é não seguir outra marca, mas obter espaço diferenciado, podemos deslocar nossa marca para áreas “vazias” no mapa. Agora estamos comparando a marca “e” a um cluster-alvo (“b”, “c”, “f”, “g”) que, pelo mapa, parece ser o cluster “das marcas vencedoras” em prestígio + valor concreto.\n\n## Gap value-leader: Assumindo que o gap reflete aproximadamente a media dessas 4 marcas\ncolMeans(brand.mean[c(\"b\", \"c\", \"f\", \"g\"), ]) - brand.mean[\"e\", ]\n\n\n  \n\n\n\nEsse vetor gerado é o gap necessário para que a marca “e” se deslocasse em direção ao centro de gravidade das marcas de melhor desempenho perceptual. Para aproximar-se do “centro de massa” das marcas superiores (“b”,“c”,“f”,“g”), a marca “e” deveria priorizar aumento de desempenho (perform: +1,17), maior seriedade (serious: +0,57) e maior intenção de recompra (rebuy: +0,67). Além disso, reduzir ênfase em latest e fun."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sobre",
    "section": "",
    "text": "Disciplina oferecida no semestre 2025/2"
  },
  {
    "objectID": "analises.html",
    "href": "analises.html",
    "title": "Análise de dados",
    "section": "",
    "text": "Descrição e Visualização da base de dados Iris",
    "crumbs": [
      "Análise de dados"
    ]
  },
  {
    "objectID": "analise_dados/descr_vis_iris.html",
    "href": "analise_dados/descr_vis_iris.html",
    "title": "Descrição e visualização de dados multivariados utilizando a base Iris",
    "section": "",
    "text": "A base iris (FISHER (1936)) é um dos conjuntos de dados mais clássicos e didáticos da estatística. Ela contém 150 observações (flores) de três espécies, setosa, versicolor e virginica, medidas em quatro variáveis contínuas:\n\nSepal.Length (cm)\n\nSepal.Width (cm)\n\nPetal.Length (cm)\n\nPetal.Width (cm)\n\n\n\n\n\n\nNosso objetivo é revisar conceitos fundamentais de Estatística Multivariada:\n\n\nVetor de médias\n\nMatrizes de covariância e correlação\n\nVariância total e generalizada\n\nMatrizes de distância (euclidiana e de Mahalanobis)\n\nVisualizações (pares, correlograma, heatmaps)"
  },
  {
    "objectID": "analise_dados/descr_vis_iris.html#distância-euclidiana",
    "href": "analise_dados/descr_vis_iris.html#distância-euclidiana",
    "title": "Descrição e visualização de dados multivariados utilizando a base Iris",
    "section": "6.1 Distância Euclidiana",
    "text": "6.1 Distância Euclidiana\n\nD_euclid &lt;- euclidean_dist(data = X)\nas.matrix(D_euclid)[1:6, 1:6]\n\n          1         2        3         4         5         6\n1 0.0000000 0.5385165 0.509902 0.6480741 0.1414214 0.6164414\n2 0.5385165 0.0000000 0.300000 0.3316625 0.6082763 1.0908712\n3 0.5099020 0.3000000 0.000000 0.2449490 0.5099020 1.0862780\n4 0.6480741 0.3316625 0.244949 0.0000000 0.6480741 1.1661904\n5 0.1414214 0.6082763 0.509902 0.6480741 0.0000000 0.6164414\n6 0.6164414 1.0908712 1.086278 1.1661904 0.6164414 0.0000000\n\n\n\n📏 Distância Euclidiana representa a distância geométrica entre dois pontos."
  },
  {
    "objectID": "analise_dados/descr_vis_iris.html#distância-de-karl-pearson",
    "href": "analise_dados/descr_vis_iris.html#distância-de-karl-pearson",
    "title": "Descrição e visualização de dados multivariados utilizando a base Iris",
    "section": "6.2 Distância de Karl Pearson",
    "text": "6.2 Distância de Karl Pearson\n\nD_karlP &lt;- scaled_euclidean_dist(data = X)\nas.matrix(D_karlP)[1:6, 1:6]\n\n          1         2         3         4         5         6\n1 0.0000000 1.1722914 0.8427840 1.0999999 0.2592702 1.0349769\n2 1.1722914 0.0000000 0.5216255 0.4325508 1.3818560 2.1739229\n3 0.8427840 0.5216255 0.0000000 0.2829432 0.9882608 1.8477070\n4 1.0999999 0.4325508 0.2829432 0.0000000 1.2459861 2.0937597\n5 0.2592702 1.3818560 0.9882608 1.2459861 0.0000000 0.8971079\n6 1.0349769 2.1739229 1.8477070 2.0937597 0.8971079 0.0000000\n\n\n\n📏 Distância de Karl Pearson leva em conta as diferenças de escala."
  },
  {
    "objectID": "analise_dados/descr_vis_iris.html#distância-de-mahalanobis",
    "href": "analise_dados/descr_vis_iris.html#distância-de-mahalanobis",
    "title": "Descrição e visualização de dados multivariados utilizando a base Iris",
    "section": "6.3 Distância de Mahalanobis",
    "text": "6.3 Distância de Mahalanobis\n\nD_mahal &lt;- mahalanobis_dist(data = X)\nas.matrix(D_mahal)[1:6, 1:6]\n\n          1         2         3         4         5         6\n1 0.0000000 1.3544572 0.9687298 1.4057253 0.5899110 1.1382566\n2 1.3544572 0.0000000 0.9697905 1.4527546 1.8106890 2.4484066\n3 0.9687298 0.9697905 0.0000000 0.7170009 1.1253440 1.9227214\n4 1.4057253 1.4527546 0.7170009 0.0000000 1.3293220 2.2425573\n5 0.5899110 1.8106890 1.1253440 1.3293220 0.0000000 0.9446158\n6 1.1382566 2.4484066 1.9227214 2.2425573 0.9446158 0.0000000\n\n\n\n📏 Distância de Mahalanobis leva em conta correlações entre variáveis e diferenças de escala."
  },
  {
    "objectID": "analise_dados/descr_vis_iris.html#gráfico-de-pares-por-espécie",
    "href": "analise_dados/descr_vis_iris.html#gráfico-de-pares-por-espécie",
    "title": "Descrição e visualização de dados multivariados utilizando a base Iris",
    "section": "7.1 Gráfico de pares por espécie",
    "text": "7.1 Gráfico de pares por espécie\n\nGGally::ggpairs(bind_cols(X, Species = y),\n                columns = 1:4, aes(color = Species, alpha = 0.8)) + theme_bw()\n\n\n\n\n\n\n\n\n\n💡 Interpretação: O “pairs plot” da iris mostra que setosa tem pétalas muito pequenas e sépalas mais largas, enquanto versicolor e virginica apresentam pétalas maiores (com alguma sobreposição), e em Sepal.Width a espécie setosa desloca-se à direita, versicolor à esquerda e virginica fica intermediária. As correlações globais destacam Petal.Length vs. Petal.Width como fortíssima (≈0,96) e Sepal.Length bem associado às pétalas; já Sepal.Width aparece negativamente correlacionado no agregado. Contudo, por espécie as relações com Sepal.Width tendem a ser positivas, e os dispersogramas envolvendo medidas de pétala exibem a melhor separação entre espécies. Em síntese, as pétalas dominam a estrutura e a discriminação, com Sepal.Width fornecendo informação complementar."
  },
  {
    "objectID": "analise_dados/descr_vis_iris.html#correlograma",
    "href": "analise_dados/descr_vis_iris.html#correlograma",
    "title": "Descrição e visualização de dados multivariados utilizando a base Iris",
    "section": "7.2 Correlograma",
    "text": "7.2 Correlograma\n\nggcorrplot(R, hc.order = TRUE, type = \"lower\",\n           lab = TRUE, tl.cex = 10,\n           title = \"Matriz de Correlações - iris\")\n\n\n\n\n\n\n\n\n\n💡 Interpretação: O mapa de correlações da iris evidencia três padrões centrais: (1) forte associação positiva entre as medidas de pétala: Petal.Length vs. Petal.Width ≈ 0,96 e também de Sepal.Length com as pétalas (≈ 0,87 e 0,82), indicando redundância informacional e provável multicolinearidade; (2) Sepal.Width apresenta correlações negativas com as demais variáveis (≈ −0,12 com Sepal.Length, −0,43 com Petal.Length e −0,37 com Petal.Width), sugerindo um eixo de variação em sentido oposto ao das pétalas; e (3) como consequência, em tarefas de PCA ou classificação, as pétalas tendem a dominar a separação entre espécies, enquanto Sepal.Width adiciona sinal complementar."
  },
  {
    "objectID": "analise_dados/descr_vis_iris.html#heatmap-das-correlações",
    "href": "analise_dados/descr_vis_iris.html#heatmap-das-correlações",
    "title": "Descrição e visualização de dados multivariados utilizando a base Iris",
    "section": "7.3 Heatmap das correlações",
    "text": "7.3 Heatmap das correlações\n\npheatmap(R, cluster_rows = TRUE, cluster_cols = TRUE,\n         main = \"Heatmap da Matriz de Correlações (iris)\")\n\n\n\n\n\n\n\n\n\n💡 Interpretação: O heatmap das correlações da iris confirma dois blocos de variáveis: (i) Petal.Length e Petal.Width fortemente positivas entre si (vermelho intenso) e também bem alinhadas com Sepal.Length (vermelho), formando um grupo altamente correlacionado que indica redundância e tende a dominar a variação; (ii) Sepal.Width aparece em azul frente às demais, mostrando correlação negativa moderada, o que a isola no dendrograma e sugere um eixo complementar de informação. Em termos práticos, as pétalas são as melhores para discriminar espécies (e podem sofrer multicolinearidade), enquanto Sepal.Width acrescenta sinal em direção oposta."
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#distribuição-normal-multivariada",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#distribuição-normal-multivariada",
    "title": "Distribuição Normal Multivariada",
    "section": "Distribuição Normal Multivariada",
    "text": "Distribuição Normal Multivariada\n\nA distribuição normal multivariada é uma generalização da distribuição normal univariada, para o caso \\(p\\)-dimensional.\n\n\n\nDiversas técnicas multivariadas baseiam-se na distribuição normal multivariada e em suas propriedades.\n\n\n\n\nBasicamente, a distribuição normal multivariada é importante por dois possíveis fatores:\n\nConfigura um modelo probabilístico adequado para o fenômeno sob estudo;\nCorresponde à distribuição (ao menos aproximada) de um grande número de estatísticas e estimadores."
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#distribuição-normal-multivariada-1",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#distribuição-normal-multivariada-1",
    "title": "Distribuição Normal Multivariada",
    "section": "Distribuição Normal Multivariada",
    "text": "Distribuição Normal Multivariada\n\n\nCaso univariado: Uma variável aleatória \\(X\\) tem distribuição normal univariada com média \\(\\mu\\) e variância \\(\\sigma^2\\), o que denotamos por \\(X \\sim Normal(\\mu, \\sigma^2)\\), se sua função densidade de probabilidade é dada por:\n\n\\[f_X(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left\\{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right\\}, \\,\\,\\,  -\\infty &lt; \\mu &lt; \\infty \\,\\,\\, \\text{e} \\,\\,\\, \\sigma &gt; 0\\]\n\n\nObserve a seguinte reescrita do termo presente no expoente:\n\n\\[ \\left( \\displaystyle{\\frac{x - \\mu}{\\sigma}} \\right)^2 = \\displaystyle{\\frac{(x - \\mu)^2}{\\sigma^2}}  = (x - \\mu) \\displaystyle{\\frac{1}{\\sigma^2}} (x - \\mu) = (x - \\mu) (\\sigma^2)^{-1} (x - \\mu),\\]\nque corresponde à distância quadrática entre \\(x\\) e \\(\\mu\\) em unidades de \\(\\sigma\\)."
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#distribuição-normal-multivariada-2",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#distribuição-normal-multivariada-2",
    "title": "Distribuição Normal Multivariada",
    "section": "Distribuição Normal Multivariada",
    "text": "Distribuição Normal Multivariada\n\nConsidere agora \\(\\mathbf{x}\\) um vetor aleatório \\(p\\)-dimensional, com vetor de médias \\(\\mathbf{\\mu}\\) e matriz de covariâncias \\(\\mathbf{\\Sigma}\\).\n\n\n\nAssumindo que \\(\\mathbf{\\Sigma}\\) é uma matriz \\(p \\times p\\), positiva definida, a seguinte expressão define o quadrado da distância de Mahalanobis entre \\(\\mathbf{x}\\) e \\(\\mathbf{\\mu}\\):\n\n\\[(\\mathbf{x} - \\mathbf{\\mu})^t\\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu})\\]\n\n\n\nA distribuição normal multivariada é obtida substituindo o expoente original pela distância generalizada, para o caso \\(p\\)-dimensional, e usando como constante de normalização \\((2\\pi)^{p/2} |\\mathbf{\\Sigma}|^{-1/2}\\)."
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#distribuição-normal-multivariada-3",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#distribuição-normal-multivariada-3",
    "title": "Distribuição Normal Multivariada",
    "section": "Distribuição Normal Multivariada",
    "text": "Distribuição Normal Multivariada\n\nUm vetor aleatório \\(p\\)-dimensional \\(\\mathbf{x} = (X_1, X_2, \\cdots, X_p)^t\\) tem distribuição normal \\(p\\)-variada, com vetor de médias \\(\\mathbf{\\mu}\\) e matriz de covariâncias \\(\\mathbf{\\Sigma}\\) se a função densidade de probabilidade conjunta for dada por:\n\n\\[f(\\mathbf{x}) = \\frac{1}{(2\\pi)^\\frac{p}{2}\\left|\\mathbf{\\Sigma} \\right|^\\frac{1}{2}} \\exp \\left\\{-\\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu})^t \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}) \\right\\}\\]\npara \\(-\\infty &lt; x_i &lt; \\infty, \\,\\,\\, i = 1,2,\\cdots, p\\)\n\n\nVamos denotar a normalidade multivariada na forma \\(\\mathbf{x} \\sim \\mathcal{N}_p(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\)."
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#distribuição-normal-multivariada-4",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#distribuição-normal-multivariada-4",
    "title": "Distribuição Normal Multivariada",
    "section": "Distribuição Normal Multivariada",
    "text": "Distribuição Normal Multivariada\n\nComo caso particular da distribuição normal multivariada temos a normal bivariada \\((p=2)\\).\n\n\n\nSeja \\(\\mathbf{x} = (X_1, X_2)^t\\) um vetor aleatório com distribuição normal de média \\(\\mathbf{\\mu} = (\\mu_1, \\mu_2)^t\\) e matriz de covariâncias:\n\n\\[\\mathbf{\\Sigma} = \\left[ \\begin{array}{cc} \\sigma_{11} & \\sigma_{12}  \\\\ \\sigma_{21} & \\sigma_{22} \\end{array} \\right]\\]\n\n\n\nA função densidade de probabilidade de \\(\\mathbf{x}\\) fica dada por:\n\n\n\\[f_{\\mathbf{x}}(\\mathbf{x}) = \\displaystyle{\\frac{1}{2 \\pi \\sigma_1 \\sigma_2 \\sqrt{1-\\rho^2}}} \\exp \\left\\{ \\displaystyle{-\\frac{1}{2(1-\\rho^2)}} \\left[ \\left(\\frac{x_1 - \\mu_1}{\\sigma_1}\\right)^2 + \\left(\\frac{x_2 - \\mu_2}{\\sigma_2}\\right)^2 - 2\\rho \\left( \\frac{x_1 - \\mu_1}{\\sigma_1}\\right) \\left(\\frac{x_2 - \\mu_2}{\\sigma_2}\\right)\\right] \\right\\}\\]"
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#distribuição-normal-multivariada-5",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#distribuição-normal-multivariada-5",
    "title": "Distribuição Normal Multivariada",
    "section": "Distribuição Normal Multivariada",
    "text": "Distribuição Normal Multivariada"
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#distribuição-normal-multivariada-6",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#distribuição-normal-multivariada-6",
    "title": "Distribuição Normal Multivariada",
    "section": "Distribuição Normal Multivariada",
    "text": "Distribuição Normal Multivariada\n\n\n\n\n\n\nTip\n\n\nÀ medida que \\(\\rho\\) varia de -1 a 1:\n\n\n\\(\\rho = 0\\) → formato circular, variáveis independentes.\n\n\n\\(\\rho &gt; 0\\) → elipses inclinadas na diagonal positiva.\n\n\n\\(\\rho &lt; 0\\) → elipses inclinadas na diagonal negativa.\n\n\n\\(\\rho\\) → \\(\\pm 1\\) → a densidade colapsa em uma linha, indicando dependência perfeita."
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#distribuição-normal-multivariada-7",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#distribuição-normal-multivariada-7",
    "title": "Distribuição Normal Multivariada",
    "section": "Distribuição Normal Multivariada",
    "text": "Distribuição Normal Multivariada\n\nSe \\(X_1, X_2, \\cdots, X_p\\) forem independentes, então \\(f(\\mathbf{x}) = f(x_1)f(x_2)\\cdots f(x_n)\\), tal que \\(X_i \\sim \\mathcal{N}(\\mu, \\sigma^2)\\).\n\n\n\n\nContornos da normal multivariada: são elipsoides que compreendem todo \\(\\mathbf{x} \\in \\mathbb{R}^p\\) tal que \\((\\mathbf{x} - \\mathbf{\\mu})^t\\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}) = c^2\\).\n\n\n\n\nOs elipsoides (contornos) gerados tem centro em \\(\\mathbf{\\mu}\\) e a direção dada pelos autovetores de \\(\\mathbf{\\Sigma}\\), com eixos \\(\\pm c \\sqrt{\\lambda_i} \\mathbf{e}_i\\).\n\n\n\n\nO elipsoide sólido de \\(\\mathbf{x}\\) satisfazendo:\n\n\\[(\\mathbf{x} - \\mathbf{\\mu})^t\\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}) \\leqslant \\chi_p^2(\\alpha)\\]\nonde \\(\\chi_p^2(\\alpha)\\) é o quantil superior \\(\\alpha\\) de uma distribuição \\(\\chi_p^2\\) , delimita \\(1 - \\alpha\\) de probabilidade."
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#distribuição-normal-multivariada-8",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#distribuição-normal-multivariada-8",
    "title": "Distribuição Normal Multivariada",
    "section": "Distribuição Normal Multivariada",
    "text": "Distribuição Normal Multivariada"
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#propriedades-da-distribuição-normal-multivariada",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#propriedades-da-distribuição-normal-multivariada",
    "title": "Distribuição Normal Multivariada",
    "section": "Propriedades da Distribuição Normal Multivariada",
    "text": "Propriedades da Distribuição Normal Multivariada\n\nSeja \\(\\mathbf{x} \\sim N_p(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\). Valem as propriedades:\n\n\n\nQualquer combinação linear das variáveis que compõem \\(\\mathbf{x}\\) tem distribuição normal univariada.\n\nMais especificamente, se \\(\\mathbf{a}^t = (a_1, a_2, \\cdots, a_p)\\) é um vetor de constantes, então\n\n\n\n\\[a_1X_1 + a_2X_2 + \\cdots + a_pX_p = \\mathbf{a}^t \\mathbf{x} \\sim N(\\mathbf{a}^t \\mathbf{\\mu}, \\mathbf{a}^t \\mathbf{\\Sigma} \\mathbf{a})\\]"
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#propriedades-da-distribuição-normal-multivariada-1",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#propriedades-da-distribuição-normal-multivariada-1",
    "title": "Distribuição Normal Multivariada",
    "section": "Propriedades da Distribuição Normal Multivariada",
    "text": "Propriedades da Distribuição Normal Multivariada\n\nSe \\(\\mathbf{A}\\) é uma matriz \\(q \\times p\\) de constantes, então \\(\\mathbf{A} \\mathbf{x}\\) tem distribuição normal multivariada.\n\nMais especificamente, \\(\\mathbf{A} \\mathbf{x} \\sim N_q(\\mathbf{A} \\mathbf{\\mu}, \\mathbf{A} \\mathbf{\\Sigma} \\mathbf{A}^t)\\), sendo\n\n\n\n\\[\\mathbf{A} = \\left[ \\begin{array}{cccc} a_{11} & a_{12} & \\cdots & a_{1p} \\\\ a_{21} & a_{22} & \\cdots & a_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{q1} & a_{q2} & \\cdots & a_{qp} \\end{array} \\right]\\]"
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#propriedades-da-distribuição-normal-multivariada-2",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#propriedades-da-distribuição-normal-multivariada-2",
    "title": "Distribuição Normal Multivariada",
    "section": "Propriedades da Distribuição Normal Multivariada",
    "text": "Propriedades da Distribuição Normal Multivariada\n\nComo caso particular da propriedade \\(1)\\), se \\(\\mathbf{x} \\sim N_p(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\), então \\(X_i \\sim \\mathcal{N}(\\mu_i, \\sigma_{ii}), \\,\\,\\, i = 1, 2, \\cdots, p\\), ou seja, a normalidade multivariada implica em marginais com distribuição normal univariada.\n\n\n\nImportante destacar que a recíproca da afirmação anterior não é necessariamente verdadeira, ou seja, marginais normalmente distribuídas não implicam, necessariamente, distribuição normal multivariada para a conjunta."
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#propriedades-da-distribuição-normal-multivariada-3",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#propriedades-da-distribuição-normal-multivariada-3",
    "title": "Distribuição Normal Multivariada",
    "section": "Propriedades da Distribuição Normal Multivariada",
    "text": "Propriedades da Distribuição Normal Multivariada\n\nAs partições de \\(\\mathbf{x}\\) são normalmente distribuídas com respectivos vetores de médias e matrizes de covariâncias.\n\n\\[\n\\mathbf{x}_{p \\times 1} =\n\\begin{bmatrix}\n\\underbrace{\\mathbf{x}_1}_{q \\times 1} \\\\\n\\underbrace{\\mathbf{x}_2}_{(p-q) \\times 1}\n\\end{bmatrix},\n\\quad\n\\boldsymbol{\\mu}_{p \\times 1} =\n\\begin{bmatrix}\n\\underbrace{\\boldsymbol{\\mu}_1}_{q \\times 1} \\\\\n\\underbrace{\\boldsymbol{\\mu}_2}_{(p-q) \\times 1}\n\\end{bmatrix},\n\\quad\n\\boldsymbol{\\Sigma}_{p \\times p} =\n\\begin{bmatrix}\n\\underbrace{\\boldsymbol{\\Sigma}_{11}}_{q \\times q} &\n\\underbrace{\\boldsymbol{\\Sigma}_{12}}_{q \\times (p - q)} \\\\\n\\underbrace{\\boldsymbol{\\Sigma}_{21}}_{(p-q) \\times q} &\n\\underbrace{\\boldsymbol{\\Sigma}_{22}}_{(p-q) \\times (p-q)}\n\\end{bmatrix}.\n\\]\nentão,\n\\[\n\\mathbf{x}_1 \\sim \\mathcal{N}_q(\\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma}_{11})\n\\quad \\text{e} \\quad\n\\mathbf{x}_2 \\sim \\mathcal{N}_{(p-q)}(\\boldsymbol{\\mu}_2, \\boldsymbol{\\Sigma}_{22}).\n\\]"
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#propriedades-da-distribuição-normal-multivariada-4",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#propriedades-da-distribuição-normal-multivariada-4",
    "title": "Distribuição Normal Multivariada",
    "section": "Propriedades da Distribuição Normal Multivariada",
    "text": "Propriedades da Distribuição Normal Multivariada\n\nEquivalência de covariância zero e independência para variáveis com distribuição normal.\n\nSe \\(\\mathbf{x}_1\\) e \\(\\mathbf{x}_2\\) são vetores aleatórios com distribuição normal e \\(Cov(\\mathbf{x}_1, \\mathbf{x}_2) = \\boldsymbol{\\Sigma}_{12} = \\boldsymbol{0}\\), então \\(\\mathbf{x}_1\\) e \\(\\mathbf{x}_2\\) são independentes.\nNo caso bivariado, \\(X_1\\) e \\(X_2\\) são independentes se \\(Cov(X_1,X_2) = \\sigma_{12} = 0\\).\nImportante destacar que a condição de independência apresentada não vale para variáveis sem distribuição normal."
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#propriedades-da-distribuição-normal-multivariada-5",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#propriedades-da-distribuição-normal-multivariada-5",
    "title": "Distribuição Normal Multivariada",
    "section": "Propriedades da Distribuição Normal Multivariada",
    "text": "Propriedades da Distribuição Normal Multivariada\n\nAs distribuições condicionais das componentes de um vetor aleatório \\(\\mathbf{x}\\) com distribuição normal multivariada são normais multivariadas, isto é, se\n\n\\[\n\\mathbf{x} =\n\\begin{bmatrix}\n\\mathbf{x}_1 \\\\[4pt]\n\\mathbf{x}_2\n\\end{bmatrix},\n\\quad\n\\boldsymbol{\\mu} =\n\\begin{bmatrix}\n\\boldsymbol{\\mu}_1 \\\\[4pt]\n\\boldsymbol{\\mu}_2\n\\end{bmatrix},\n\\quad\n\\boldsymbol{\\Sigma} =\n\\begin{bmatrix}\n\\boldsymbol{\\Sigma}_{11} & \\boldsymbol{\\Sigma}_{12} \\\\[4pt]\n\\boldsymbol{\\Sigma}_{21} & \\boldsymbol{\\Sigma}_{22}\n\\end{bmatrix},\n\\qquad |\\boldsymbol{\\Sigma}_{22}| &gt; 0\n\\]\nEntão, a distribuição condicional de \\(\\mathbf{x}_1\\) dado que \\(\\mathbf{x}_2 = \\mathbf{c}\\) é normal com:\n\\[\n\\mathbf{x}_1 \\mid \\mathbf{x}_2 = \\mathbf{c}\n\\sim\n\\mathcal{N}_q(\\boldsymbol{\\mu}^*, \\boldsymbol{\\Sigma}^*)\n\\]"
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#propriedades-da-distribuição-normal-multivariada-6",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#propriedades-da-distribuição-normal-multivariada-6",
    "title": "Distribuição Normal Multivariada",
    "section": "Propriedades da Distribuição Normal Multivariada",
    "text": "Propriedades da Distribuição Normal Multivariada\nonde:\n\\[\n\\boldsymbol{\\mu}^* = \\boldsymbol{\\mu}_1 + \\boldsymbol{\\Sigma}_{12}\\boldsymbol{\\Sigma}_{22}^{-1}(\\mathbf{c} - \\boldsymbol{\\mu}_2)\n\\]\ne\n\\[\n\\boldsymbol{\\Sigma}^* = \\boldsymbol{\\Sigma}_{11} - \\boldsymbol{\\Sigma}_{12}\\boldsymbol{\\Sigma}_{22}^{-1}\\boldsymbol{\\Sigma}_{21}.\n\\]"
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#propriedades-da-distribuição-normal-multivariada-7",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#propriedades-da-distribuição-normal-multivariada-7",
    "title": "Distribuição Normal Multivariada",
    "section": "Propriedades da Distribuição Normal Multivariada",
    "text": "Propriedades da Distribuição Normal Multivariada\n\nSe \\(\\mathbf{x} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), então\n\n\n\\((\\mathbf{x} - \\boldsymbol{\\mu})^{\\mathsf{t}} \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\) segue uma distribuição qui-quadrado com \\(p\\) graus de liberdade.\nO elipsóide sólido de probabilidade de \\(\\mathbf{x}\\) é definido pelo conjunto de pontos que satisfazem:\n\n\n\n\\[\\mathbf{x} : (\\mathbf{x} - \\boldsymbol{\\mu})^{\\mathsf{T}} \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}) \\leq \\chi^2_p(\\alpha)\\] onde \\(\\chi^2_p(\\alpha)\\) é o quantil superior de ordem \\(\\alpha\\) da distribuição qui-quadrado com \\(p\\) graus de liberdade. Esse elipsóide delimita a região que contém aproximadamente \\(100 \\times (1 - \\alpha)\\%\\) da probabilidade da distribuição."
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#elipsóide-de-confiança-95",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#elipsóide-de-confiança-95",
    "title": "Distribuição Normal Multivariada",
    "section": "Elipsóide de confiança (95%)",
    "text": "Elipsóide de confiança (95%)"
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#elipsóide-de-confiança-95-1",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#elipsóide-de-confiança-95-1",
    "title": "Distribuição Normal Multivariada",
    "section": "Elipsóide de confiança (95%)",
    "text": "Elipsóide de confiança (95%)\nInterpretação geométrica\n\nO centro do elipsóide é a média \\(\\boldsymbol{\\mu}\\);\n\nA forma e a orientação são determinadas por \\(\\boldsymbol{\\Sigma}\\);\n\nO eixo maior aponta na direção do autovetor associado ao maior autovalor de \\(\\boldsymbol{\\Sigma}\\);\n\nO raio na direção de cada autovetor é proporcional à raiz quadrada do autovalor correspondente."
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#verificando-a-suposição-de-normalidade-multivariada",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#verificando-a-suposição-de-normalidade-multivariada",
    "title": "Distribuição Normal Multivariada",
    "section": "Verificando a suposição de normalidade multivariada",
    "text": "Verificando a suposição de normalidade multivariada\n\nUma vez que parte dos métodos de análise multivariada requer a suposição de normalidade (multivariada) dos dados, precisamos de recursos para verificar a validade de tal suposição.\n\n\n\nEmbora, como dito anteriormente, normalidade para as distribuições marginais não implique em normalidade para a conjunta, o contrário é necessariamente válido.\n\n\n\n\nDessa forma, analisar a normalidade univariada (e eventualmente bivariada) para as \\(p\\) variáveis pode fornecer indicativos favoráveis (ou definitivamente contrários) à suposição de normalidade multivariada."
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#verificando-a-suposição-de-normalidade-multivariada-1",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#verificando-a-suposição-de-normalidade-multivariada-1",
    "title": "Distribuição Normal Multivariada",
    "section": "Verificando a suposição de normalidade multivariada",
    "text": "Verificando a suposição de normalidade multivariada\n\nAlguns recursos para verificar normalidade (caso univariado).\n\n\n\nGráficos:\n\nHistograma com curva de densidade não paramétrica;\nBoxplot;\nGráfico quantil-quantil.\n\n\n\n\n\n\nTestes:\n\nShapiro Wilks;\nAnderson Darling;\nLilliefors…"
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#verificando-a-suposição-de-normalidade-multivariada-2",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#verificando-a-suposição-de-normalidade-multivariada-2",
    "title": "Distribuição Normal Multivariada",
    "section": "Verificando a suposição de normalidade multivariada",
    "text": "Verificando a suposição de normalidade multivariada\n\nUm procedimento para verificação de normalidade multivariada pode ser estabelecido a partir da medida de distância de Mahalanobis:\n\n\\[d_i^2 = (\\mathbf{x}_i - \\bar{\\mathbf{x}})^t \\boldsymbol{S}^{-1}(\\mathbf{x}_i - \\bar{\\mathbf{x}}), \\,\\,\\, i = 1,2, \\cdots, n\\]\n\n\nSe a população de fato for normalmente distribuída, um procedimento válido (sobretudo se \\(n\\) e \\(n - p\\) forem grandes), é comparar os quantis das distâncias quadráticas com os quantis de uma distribuição \\(\\chi_p^2\\)."
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#verificando-a-suposição-de-normalidade-multivariada-3",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#verificando-a-suposição-de-normalidade-multivariada-3",
    "title": "Distribuição Normal Multivariada",
    "section": "Verificando a suposição de normalidade multivariada",
    "text": "Verificando a suposição de normalidade multivariada\n\nA verificação da normalidade multivariada com base nas distâncias quadráticas se dá da seguinte forma:\n\nOrdenar as \\(n\\) distâncias quadráticas de forma crescente, ou seja, \\(d^2_{(1)} \\leqslant d^2_{(2)} \\leqslant \\cdots \\leqslant d^2_{(n)}\\)\n\nObter os quantis \\((i - 1/2)/n\\) da distribuição \\(\\chi_p^2\\), para \\(i = 1, 2, \\cdots, n\\). Vamos denotá-los por \\(q_{(1)}, q_{(2)}, \\cdots, q_{(n)}\\).\nPlotar a dispersão dos pontos \\((d^2_{(1)}, q_{(1)}), (d^2_{(2)}, q_{(2)}), \\cdots (d^2_{(n)}, q_{(n)})\\).\n\n\n\n\n\nQuanto mais os pontos estiverem dispersos próximos à reta identidade, mais forte o indicativo de normalidade multivariada."
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#verificando-a-suposição-de-normalidade-multivariada-4",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#verificando-a-suposição-de-normalidade-multivariada-4",
    "title": "Distribuição Normal Multivariada",
    "section": "Verificando a suposição de normalidade multivariada",
    "text": "Verificando a suposição de normalidade multivariada"
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#verificando-a-suposição-de-normalidade-multivariada-5",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#verificando-a-suposição-de-normalidade-multivariada-5",
    "title": "Distribuição Normal Multivariada",
    "section": "Verificando a suposição de normalidade multivariada",
    "text": "Verificando a suposição de normalidade multivariada\n\nUm procedimento confirmatório disponível para a checagem da normalidade multivariada é a versão multivariada do teste de Shapiro-Wilks.\n\n\n\nA hipótese nula desse teste é a hipótese de normalidade multivariada, contra a hipótese alternativa de não normalidade.\n\n\n\n\nO teste baseia-se na transformação das variáveis originais num conjunto de variáveis padronizadas e (aproximadamente) independentes, seguido do cálculo e soma das estatísticas do teste univariado de Shapiro-Wilks.\n\n\n\n\nA distribuição da estatística do teste, sob a hipótese nula, não pode ser obtida analiticamente, mas determinada via simulação.\n\n\n\n\nNo R, pacotes \\(\\mathtt{mvShapiroTest}\\), \\(\\mathtt{MVN}\\) e \\(\\mathtt{goft}\\)."
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#verificando-a-suposição-de-normalidade-multivariada-6",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#verificando-a-suposição-de-normalidade-multivariada-6",
    "title": "Distribuição Normal Multivariada",
    "section": "Verificando a suposição de normalidade multivariada",
    "text": "Verificando a suposição de normalidade multivariada\n\nCaso a suposição de normalidade não seja atendida, deve-se considerar como alternativas:\n\nUtilizar métodos baseados em outras distribuições de probabilidades, ou que não requerem a suposição de normalidade;\nTransformar os dados para alcançar normalidade (ex: transformação de Box-Cox);\nCategorizar os dados;\nInvestigar a presença de outliers.\n\n\n\n\n\n\nNota: Boa parte dos métodos que vamos estudar são robustos a afastamentos moderados da hipótese de normalidade multivariada."
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#verificando-a-existência-de-outliers",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#verificando-a-existência-de-outliers",
    "title": "Distribuição Normal Multivariada",
    "section": "Verificando a existência de outliers",
    "text": "Verificando a existência de outliers\n\nOutliers são resultados extremos que devem ser identificados e examinados com cautela, verificando possível impacto nos resultados das análises.\n\n\n\nNo contexto univariado, gráficos (histograma, boxplot, gráfico quanti-quantil), além de regras empíricas (como pontos afastados a mais de três desvios padrões da média) são ferramentas auxiliares na identificação de outliers.\n\n\n\n\nJá no contexto multivariado, identificar outliers é uma tarefa mais complicada.\n\n\n\n\nGráficos de dispersão bi e tridimensionais e matrizes de gráficos de dispersão podem ajudar."
  },
  {
    "objectID": "aulas/distr_normal_multi/distr_normal_multi.html#verificando-a-existência-de-outliers-1",
    "href": "aulas/distr_normal_multi/distr_normal_multi.html#verificando-a-existência-de-outliers-1",
    "title": "Distribuição Normal Multivariada",
    "section": "Verificando a existência de outliers",
    "text": "Verificando a existência de outliers\n\nUma vez mais, o conceito de distância pode ser aplicado aqui como recurso para identificação de outliers.\n\n\n\nNesse caso, calculamos \\(d_i^2 = (\\mathbf{x}_i - \\bar{\\mathbf{x}})^t \\boldsymbol{S}^{-1}(\\mathbf{x}_i - \\bar{\\mathbf{x}})\\), \\(i = 1,2, \\cdots, n\\) e comparamos os valores obtidos com o quantil superior \\(\\alpha\\) da distribuição \\(\\chi^2_p\\).\n\n\n\n\nEm geral consideramos um valor pequeno para \\(\\alpha\\) (como 1%, ou 0,5%), tendo em mente que, mesmo sob normalidade multivariada, valores extremos naturalmente ocorrem."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas",
    "title": "Revisão de Álgebra Matricial",
    "section": "Definição e propriedades básicas",
    "text": "Definição e propriedades básicas\n\nUma matriz é um conjunto de números ou variáveis dispostos em linhas e colunas.\n\n\n\nUma matriz \\(\\mathbf{A}\\) de \\(n\\) linhas e \\(p\\) colunas (dimensão \\(n \\times p\\)) pode ser representada, genericamente, por:\n\n\\[{\\mathbf A} = \\left[ \\begin{array}{cccc} a_{11} & a_{12} & \\cdots & a_{1p} \\\\ a_{21} & a_{22} & \\cdots & a_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{np} \\end{array} \\right]\\]\n\n\n\nA matriz \\(\\mathbf{A}\\) pode ser denotada ainda por \\(\\mathbf{A} = \\{a_{ij}\\}\\), onde o primeiro índice indica linha, o segundo coluna e \\(a_{ij}\\) é o termo geral da matriz."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-1",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-1",
    "title": "Revisão de Álgebra Matricial",
    "section": "Definição e propriedades básicas",
    "text": "Definição e propriedades básicas\n\nUm vetor \\(\\mathbf{x}\\), de dimensão \\(n\\), é representado, genericamente, por:\n\n\\[\\mathbf{x} = \\left[ \\begin{array}{c} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{array} \\right] \\]\n\n\nNuma análise multivariada com \\(n\\) indivíduos e \\(p\\) variáveis, as linhas da matriz de dados (observações dos indivíduos) podem ser consideradas \\(n\\) vetores de dimensão \\(p\\): \\(\\mathbf{x}_i^t = (x_{i1}, x_{i2}, \\cdots, x_{ip}), \\,\\,\\,\\,\\, i = 1, 2, \\cdots, n\\);"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-2",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-2",
    "title": "Revisão de Álgebra Matricial",
    "section": "Definição e propriedades básicas",
    "text": "Definição e propriedades básicas\n\nAs colunas da matriz de dados (observações referentes à variáveis) podem ser consideradas \\(p\\) vetores de dimensão \\(n\\):\n\n\\[\\mathbf{x}_j^t = (x_{1j}, x_{2j}, \\cdots, x_{nj}), \\,\\,\\,\\,\\, j = 1, 2, \\cdots, p\\]\n\n\nA multiplicação de um vetor \\(\\mathbf{x} = (x_1, x_2 , \\cdots, x_p)^t\\) por um escalar real \\(c\\) resulta em um vetor \\(\\mathbf{y} = c \\mathbf{x} = (cx_1 , cx_2 , \\cdots, cx_p)^t\\), de igual dimensão em relação ao vetor original;\n\n\n\n\nGeometricamente, a multiplicação de um vetor por um escalar pode mudar seu tamanho e sentido, mas não sua direção."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-3",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-3",
    "title": "Revisão de Álgebra Matricial",
    "section": "Definição e propriedades básicas",
    "text": "Definição e propriedades básicas\n\nA soma de dois vetores \\(\\mathbf{x}\\) e \\(\\mathbf{y}\\), de iguais dimensões, resulta em um terceiro vetor dado por:\n\n\\[\\mathbf{z} = \\mathbf{x} + \\mathbf{y} = (x_1 + y_1, x_2 + y_2, \\cdots, x_p + y_p)^t\\]\n\n\nA diferença de dois vetores \\(\\mathbf{x}\\) e \\(\\mathbf{y}\\), de iguais dimensões, resulta em um terceiro vetor dado por:\n\n\\[\\mathbf{w} = \\mathbf{x} - \\mathbf{y} = (x_1 - y_1, x_2 - y_2, \\cdots, x_p - y_p)^t\\]"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-4",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-4",
    "title": "Revisão de Álgebra Matricial",
    "section": "Definição e propriedades básicas",
    "text": "Definição e propriedades básicas\n\nO produto interno de dois vetores \\(\\mathbf{x}\\) e \\(\\mathbf{y}\\) é definido por:\n\n\\[\\mathbf{v} = \\mathbf{x}^t\\mathbf{y} = \\displaystyle{\\sum_{i=1}^{p}} x_iy_i = x_1 y_1 + x_2 y_2 + \\cdots + x_p y_p\\]\n\n\nO tamanho do vetor \\(\\mathbf{x} = (x_1, x_2, \\cdots, x_p)^t\\) é definido pela distância do ponto \\(p\\)-dimensional, determinado por suas coordenadas, à origem:\n\n\\[L_x = \\sqrt{\\mathbf{x}^t\\mathbf{x}} = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_p^2}\\]"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-5",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-5",
    "title": "Revisão de Álgebra Matricial",
    "section": "Definição e propriedades básicas",
    "text": "Definição e propriedades básicas\n\nO cosseno do ângulo \\(\\theta\\) entre os vetores \\(\\mathbf{x}\\) e \\(\\mathbf{y}\\) definidos em \\(\\mathbb{R}^p\\) é dado por:\n\n\\[\\cos({\\theta}) = \\dfrac{\\mathbf{x}^t\\mathbf{y}}{\\sqrt{\\mathbf{x}^t\\mathbf{x}} \\sqrt{\\mathbf{y}^t\\mathbf{y}}}\\]\n\n\nDois vetores \\(\\mathbf{x}\\) e \\(\\mathbf{y}\\) são entre si se o ângulo \\(\\theta\\) entre eles é \\(90^o\\), de tal forma que \\(\\cos(\\theta) = 0\\), ou, de forma equivalente, \\(\\mathbf{x}^t\\mathbf{y} = 0\\).\n\n\n\n\nA normalização de um vetor \\(\\mathbf{x}\\) corresponde à divisão de \\(\\mathbf{x}\\) por \\(L_x\\), de tal forma que o vetor resultante tenha comprimento unitário:\n\n\\[\\mathbf{x}^* = \\dfrac{\\mathbf{x}}{L_x}\\]"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-6",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-6",
    "title": "Revisão de Álgebra Matricial",
    "section": "Definição e propriedades básicas",
    "text": "Definição e propriedades básicas\n\nA projeção de um vetor \\(\\mathbf{x}\\) em um vetor \\(\\mathbf{y}\\) é um novo vetor, com coordenadas:\n\n\\[\\text{Projeção de } \\mathbf{x} \\text{ em } \\mathbf{y} = \\dfrac{\\mathbf{x}^t\\mathbf{y}}{\\mathbf{y}^t\\mathbf{y}} \\mathbf{y}\\]\n\n\nO comprimento da projeção de \\(\\mathbf{x}\\) em \\(\\mathbf{y}\\) é dado por:\n\n\\[\\text{Tamanho da projeção de } \\mathbf{x} \\text{ em } \\mathbf{y} = \\dfrac{|\\mathbf{x}^t\\mathbf{y}|}{L_y} =  L_x \\cos(\\theta)\\]"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-7",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-7",
    "title": "Revisão de Álgebra Matricial",
    "section": "Definição e propriedades básicas",
    "text": "Definição e propriedades básicas\n\nIgualdade de matrizes: Dizemos que duas matrizes \\(\\mathbf{A}\\) e \\(\\mathbf{B}\\) são iguais se elas tem iguais dimensões e \\(\\{a_{ij}\\} = \\{b_{ij}\\}\\) para todo \\(i\\) e para todo \\(j\\).\n\n\n\nMatriz transposta: A transposta de uma matriz \\(\\mathbf{A}_{n \\times p}\\) é a matriz \\(\\mathbf{A}^t_{p \\times n}\\) tal que \\(\\{a_{ij}\\} = \\{a_{ji}\\}\\) para todo \\(i\\) e para todo \\(j\\):\n\n\\[\\mathbf{A}^t = \\left[ \\begin{array}{cccc} a_{11} & a_{21} & \\cdots & a_{n1} \\\\ a_{12} & a_{22} & \\cdots & a_{n2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        a_{1p} & a_{2p} & \\cdots & a_{np} \\end{array} \\right]\\]"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-8",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-8",
    "title": "Revisão de Álgebra Matricial",
    "section": "Definição e propriedades básicas",
    "text": "Definição e propriedades básicas\n\nMatriz simétrica: Dizemos que uma matriz \\(\\mathbf{A}_{p \\times p}\\) é simétrica se \\(\\{a_{ij}\\} = \\{a_{ji}\\}\\) para todo \\(i\\) e para todo \\(j\\), ou seja, \\(\\mathbf{A}^t = \\mathbf{A}\\).\n\n\n\nDiagonal de uma matriz: A diagonal de uma matriz quadrada \\(\\mathbf{A}_{p \\times p}\\) corresponde ao conjunto de elementos \\(a_{11}, a_{22}, \\cdots, a_{pp}\\).\n\n\n\n\nMatriz diagonal: Dizemos que a matriz quadrada \\(\\mathbf{A}_{p \\times p}\\) é diagonal se todos os elementos fora da diagonal são iguais a zero:\n\n\\[\\mathbf{A} = \\left[ \\begin{array}{cccc} a_{11} & 0 & \\cdots & 0 \\\\ 0 & a_{22} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        0 & 0 & \\cdots & a_{pp} \\end{array} \\right]\\]"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-9",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-9",
    "title": "Revisão de Álgebra Matricial",
    "section": "Definição e propriedades básicas",
    "text": "Definição e propriedades básicas\n\nMatriz identidade: Dizemos que a matriz quadrada \\(\\mathbf{I}_{p\\times p}\\) é uma matriz identidade se ela é uma matriz diagonal com todos os elementos da diagonal iguais a 1:\n\n\\[\\mathbf{I} = \\left[ \\begin{array}{cccc} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        0 & 0 & \\cdots & 1 \\end{array} \\right]\\]"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-10",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-10",
    "title": "Revisão de Álgebra Matricial",
    "section": "Definição e propriedades básicas",
    "text": "Definição e propriedades básicas\n\nMatriz triangular superior: Dizemos que a matriz quadrada \\(\\mathbf{A}_{p \\times p}\\) é uma matriz triangular superior se todos os elementos abaixo da diagonal são iguais a zero:\n\n\\[\\mathbf{A} = \\left[ \\begin{array}{cccc} a_{11} & a_{12} & \\cdots & a_{1p} \\\\ 0 & a_{22} & \\cdots & a_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & a_{pp} \\end{array} \\right]\\]\n\n\nUma matriz triangular inferior é definida de forma semelhante."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes",
    "title": "Revisão de Álgebra Matricial",
    "section": "Operações envolvendo matrizes",
    "text": "Operações envolvendo matrizes\n\nA soma de duas matrizes \\(\\mathbf{A}_{n \\times p}\\) e \\(\\mathbf{B}_{n \\times p}\\) de iguais dimensões é a matriz resultante das somas dos elementos nas posições correspondentes:\n\n\\[\\mathbf{A} + \\mathbf{B} = \\left[ \\begin{array}{cccc} a_{11} + b_{11} & a_{12} + b_{12} & \\cdots & a_{1p} + b_{1p}\\\\ a_{21} +  b_{21}& a_{22} + b_{22}& \\cdots & a_{2p} + b_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        a_{n1} + b_{n1} & a_{n2} + b_{n2} & \\cdots & a_{np} + b_{np} \\end{array} \\right]\\]"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes-1",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes-1",
    "title": "Revisão de Álgebra Matricial",
    "section": "Operações envolvendo matrizes",
    "text": "Operações envolvendo matrizes\n\nA diferença de duas matrizes \\(\\mathbf{A}_{n \\times p}\\) e \\(\\mathbf{B}_{n \\times p}\\) de iguais dimensões é a matriz resultante das diferenças dos elementos nas posições correspondentes:\n\n\\[\\mathbf{A} - \\mathbf{B} = \\left[ \\begin{array}{cccc} a_{11} - b_{11} & a_{12} - b_{12} & \\cdots & a_{1p} - b_{1p}\\\\ a_{21} -  b_{21}& a_{22} - b_{22}& \\cdots & a_{2p} - b_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\       a_{n1} - b_{n1} & a_{n2} - b_{n2} & \\cdots & a_{np} - b_{np} \\end{array} \\right]\\]"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes-2",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes-2",
    "title": "Revisão de Álgebra Matricial",
    "section": "Operações envolvendo matrizes",
    "text": "Operações envolvendo matrizes\n\nSejam \\(\\mathbf{A}_{n \\times k}\\) e \\(\\mathbf{B}_{k \\times p}\\) duas matrizes, tais que o número de linhas da segunda é igual ao número de colunas da primeira. O produto \\(\\mathbf{AB}\\) é definido por:\n\n\\[\\mathbf{A} \\mathbf{B} = \\left[ \\begin{array}{cccc} \\sum_{r = 1}^k a_{1r}. b_{r1} & \\sum_{r = 1}^k a_{1r}. b_{r2} & \\cdots & \\sum_{r = 1}^k a_{1r}. b_{rp}\\\\ \\sum_{r = 1}^k a_{2r}. b_{r1} & \\sum_{r = 1}^k a_{2r}. b_{r2} & \\cdots & \\sum_{r = 1}^k a_{2r}. b_{rp} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        \\sum_{r = 1}^k a_{nr}. b_{r1} & \\sum_{r = 1}^k a_{nr}. b_{r2} & \\cdots & \\sum_{r = 1}^k a_{nr}. b_{rp} \\end{array} \\right]\\]\n\n\nDizemos que uma matriz quadrada \\(\\mathbf{Q}\\) é ortogonal se \\(\\mathbf{QQ}^t = \\mathbf{Q}^t \\mathbf{Q} = \\mathbf{I}\\)."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes-3",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes-3",
    "title": "Revisão de Álgebra Matricial",
    "section": "Operações envolvendo matrizes",
    "text": "Operações envolvendo matrizes\n\nSejam \\(\\mathbf{A}_{n \\times p}\\) e \\(c\\) uma constante. O produto \\(c \\mathbf{A}\\) resulta no produto de cada elemento de \\(\\mathbf{A}\\) por \\(c\\):\n\n\\[c\\mathbf{A} = \\left[ \\begin{array}{cccc} ca_{11} & ca_{12} & \\cdots & ca_{1p} \\\\ ca_{21} & ca_{22} & \\cdots & ca_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    ca_{n1} & ca_{n2} & \\cdots & ca_{np} \\end{array} \\right]\\]"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes-4",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes-4",
    "title": "Revisão de Álgebra Matricial",
    "section": "Operações envolvendo matrizes",
    "text": "Operações envolvendo matrizes\n\nSejam \\(\\mathbf{A}\\), \\(\\mathbf{B}\\) e \\(\\mathbf{C}\\) matrizes com dimensões compatíveis para as operações consideradas. Então:\n\n\\((\\mathbf{A}^t)^t = \\mathbf{A}\\);\n\\(\\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}\\);\n\\((\\mathbf{A} + \\mathbf{B})^t = \\mathbf{A}^t + \\mathbf{B}^t\\);\n\\((\\mathbf{A} - \\mathbf{B})^t = \\mathbf{A}^t - \\mathbf{B}^t\\);\n\\((\\mathbf{AB})^t = \\mathbf{B}^t \\mathbf{A}^t\\);\n\\(\\mathbf{AB} \\neq \\mathbf{BA}\\), a menos de situações bem específicas;"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes-5",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes-5",
    "title": "Revisão de Álgebra Matricial",
    "section": "Operações envolvendo matrizes",
    "text": "Operações envolvendo matrizes\n\nSejam \\(\\mathbf{A}\\), \\(\\mathbf{B}\\) e \\(\\mathbf{C}\\) matrizes com dimensões compatíveis para as operações consideradas. Então:\n\n\\(\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{AB} + \\mathbf{AC}\\), valendo o mesmo ao substituir a soma pela diferença;\n\\((\\mathbf{A} + \\mathbf{B}) \\mathbf{C} = \\mathbf{AC} + \\mathbf{BC}\\), valendo o mesmo ao substituir a soma pela diferença;\n\\((\\mathbf{A} + \\mathbf{B})\\mathbf{C} \\neq \\mathbf{CA} + \\mathbf{CB}\\), a menos de situações bem específicas;\n\\(\\mathbf{IA} = \\mathbf{AI} = \\mathbf{A}\\), para qualquer \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes-6",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes-6",
    "title": "Revisão de Álgebra Matricial",
    "section": "Operações envolvendo matrizes",
    "text": "Operações envolvendo matrizes\n\nO traço de uma matriz de uma matriz \\(\\mathbf{A}_{p \\times p}\\), denotado por \\(\\text{tr}(\\mathbf{A})\\), corresponde à soma dos elementos da diagonal de \\(\\mathbf{A}\\):\n\n\\[\\text{tr}(\\mathbf{A}) = \\displaystyle{\\sum_{i=1}^{p}a_{ii}}\\]\n\n\nSejam \\(\\mathbf{A}\\) e \\(\\mathbf{B}\\) matrizes quadradas. Então:\n\n\\(\\text{tr}(\\mathbf{A} + \\mathbf{B}) = \\text{tr}(\\mathbf{A}) + \\text{tr}(\\mathbf{B})\\)\n\\(\\text{tr}(\\mathbf{A} \\mathbf{B}) = \\text{tr}(\\mathbf{B} \\mathbf{A})\\)"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#combinações-lineares-e-formas-quadráticas",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#combinações-lineares-e-formas-quadráticas",
    "title": "Revisão de Álgebra Matricial",
    "section": "Combinações lineares e formas quadráticas",
    "text": "Combinações lineares e formas quadráticas\n\nPara um conjunto de constantes \\(a_1, a_2, \\cdots, a_p\\), o vetor \\(\\mathbf{y} = a_1 \\mathbf{x}_1 + a_2 \\mathbf{x}_ 2 + \\cdots + a_p \\mathbf{x}_p\\) é uma combinação linear dos vetores \\(\\mathbf{x}_1, \\mathbf{x}_2, \\cdots, \\mathbf{x}_p\\) .\n\n\n\nO conjunto de vetores \\(\\mathbf{x}_1, \\mathbf{x}_2, \\cdots, \\mathbf{x}_p\\) é dito linearmente dependente se há um conjunto de constantes \\(a_1, a_2, \\cdots, a_p\\), nem todas nulas, tal que:\n\n\\[a_1 \\mathbf{x}_1 + a_2 \\mathbf{x}_ 2 + \\cdots + a_p \\mathbf{x}_p = 0\\]\n\n\n\nCaso contrário os vetores são linearmente independentes."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#combinações-lineares-e-formas-quadráticas-1",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#combinações-lineares-e-formas-quadráticas-1",
    "title": "Revisão de Álgebra Matricial",
    "section": "Combinações lineares e formas quadráticas",
    "text": "Combinações lineares e formas quadráticas\n\nFormas quadráticas surgem de forma recorrente na estatística multivariada, por exemplo, na definição de distâncias.\n\n\n\nUma forma quadrática, definida a partir de uma matriz simétrica \\(\\mathbf{A}_{p \\times p}\\), é definida como:\n\n\\[Q(\\mathbf{x}) = {\\mathbf{x}^t} \\mathbf{A} \\mathbf{x} =\\displaystyle{\\sum_{i=1}^p a_{ii} x_{i}^2} + 2 \\displaystyle{\\sum_{i = 1}^{p-1}} \\displaystyle{\\sum_{k = i+1}^{p}} a_{ik} x_i  x_k  = \\displaystyle{\\sum_{i=1}^p} \\displaystyle{\\sum_{k=1}^p} a_{ik} x_i  x_k\\]\npara \\(\\mathbf{x} \\neq \\mathbf{0}\\) definido em \\(\\mathbb{R}^p\\)."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#combinações-lineares-e-formas-quadráticas-2",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#combinações-lineares-e-formas-quadráticas-2",
    "title": "Revisão de Álgebra Matricial",
    "section": "Combinações lineares e formas quadráticas",
    "text": "Combinações lineares e formas quadráticas\n\nClassificamos a matriz \\(\\mathbf{A}\\), e a consequente forma quadrática \\(\\mathbf{x}^t \\mathbf{A}\\mathbf{x}\\), como positiva definida se \\(Q(\\mathbf{x}) &gt; 0\\) para qualquer \\(\\mathbf{x} \\neq \\mathbf{0}\\).\n\n\n\nOutras classificações:\n\nPositiva semidefinida: \\(Q(\\mathbf{x}) \\geqslant 0\\)\nNegativa definida: \\(Q(\\mathbf{x}) &lt; 0\\)\nNegativa semidefinida: \\(Q(\\mathbf{x}) \\leqslant 0\\)\nIndefinida: \\(Q(\\mathbf{x}) &gt; 0\\) para alguns \\(\\mathbf{x} \\in \\mathbb{R}^p\\) e \\(Q(\\mathbf{x}) &lt; 0\\) para outros \\(\\mathbf{x} \\in \\mathbb{R}^p\\)"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#matriz-inversa",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#matriz-inversa",
    "title": "Revisão de Álgebra Matricial",
    "section": "Matriz inversa",
    "text": "Matriz inversa\n\nMatriz inversa: Considere uma matriz \\(\\mathbf{A}_{p \\times p}\\). Caso exista uma matriz \\(\\mathbf{B}_{p \\times p}\\) tal que\n\n\\[\\mathbf{AB} = \\mathbf{BA} = \\mathbf{I}\\]\ndizemos que \\(\\mathbf{B}\\) é a matriz inversa de \\(\\mathbf{A}\\), sendo usualmente denotada por \\(\\mathbf{A}^{-1}\\).\n\n\nQuando uma matriz possui uma matriz inversa, dizemos que ela é não-singular. Caso contrário, ela é classificada como singular."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#matriz-inversa-1",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#matriz-inversa-1",
    "title": "Revisão de Álgebra Matricial",
    "section": "Matriz inversa",
    "text": "Matriz inversa\n\nA condição fundamental para que uma matriz tenha inversa é que suas colunas sejam linearmente independentes (matriz de \\(rank\\) completo).\n\n\n\nO \\(rank\\) de uma matriz \\(\\mathbf{A}_{n \\times p}\\) , denotado por \\(rank(\\mathbf{A})\\), é definido como o número de linhas (ou colunas) linearmente independentes de \\(\\mathbf{A}\\).\n\n\n\n\nDizemos que a matriz quadrada \\(\\mathbf{A}_{p \\times p}\\) tem \\(rank\\) completo se \\(rank(\\mathbf{A}) = p\\), configurando uma matriz não singular.\n\n\n\n\nPara matrizes de \\(rank\\) incompleto ou não-quadradas, define-se a inversa generalizada de \\(\\mathbf{A}\\) como a matriz \\(\\mathbf{A}^-\\) que satisfaz \\(\\mathbf{A} \\mathbf{A}^- \\mathbf{A} = \\mathbf{A}\\)."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#matriz-inversa-2",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#matriz-inversa-2",
    "title": "Revisão de Álgebra Matricial",
    "section": "Matriz inversa",
    "text": "Matriz inversa\n\nA inversa de uma matriz diagonal é dada pela matriz diagonal composta pelos inversos dos elementos da matriz original:\n\n\\[\\mathbf{A} = \\left[ \\begin{array}{cccc} a_{11} & 0 & \\cdots & 0 \\\\ 0 & a_{22} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & a_{pp} \\end{array} \\right]; \\,\\,\\,\\,\\,\\,\\,\\,\\, \\mathbf{A}^{-1} = \\left[ \\begin{array}{cccc} \\frac{1}{a_{11}} & 0 & \\cdots & 0 \\\\ 0 & \\frac{1}{a_{22}} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\frac{1}{a_{pp}} \\end{array} \\right]\\]"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#matriz-inversa-3",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#matriz-inversa-3",
    "title": "Revisão de Álgebra Matricial",
    "section": "Matriz inversa",
    "text": "Matriz inversa\n\n\\(\\mathbf{A}\\) e \\(\\mathbf{B}\\) não singulares \\((p \\times p)\\), \\((\\mathbf{AB})^{-1} = \\mathbf{B}^{-1} \\mathbf{A}^{-1}\\);\n\n\n\nPara \\(c\\) uma constante real diferente de zero, \\((c \\mathbf{B})^{-1} = c^{-1}(\\mathbf{A})^{-1}\\);\n\n\n\n\n\\((\\mathbf{A}^t)^{-1} = (\\mathbf{A}^{-1})^t\\);\n\n\n\n\nSe \\(rank(\\mathbf{A}) = p\\) então \\(\\mathbf{A}^{-1}\\) existe;\n\n\n\n\nSe \\(\\mathbf{A}\\) é ortogonal, então \\(\\mathbf{A}^{-1}\\) existe, além do que \\(\\mathbf{A}^{-1} = \\mathbf{A}^t\\);\n\n\n\n\nSe \\(\\mathbf{B}\\) é não singular, \\(\\mathbf{AB} = \\mathbf{CB}\\) implica \\(\\mathbf{A} = \\mathbf{C}\\)."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#determinante",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#determinante",
    "title": "Revisão de Álgebra Matricial",
    "section": "Determinante",
    "text": "Determinante\n\nO determinante de uma matriz \\(\\mathbf{A}_{p \\times p}\\) , denotado por \\(\\det(\\mathbf{A})\\) ou \\(|\\mathbf{A}|\\), é definido como:\n\\[\\det(\\mathbf{A}) = \\begin{cases} a_{11} & \\text{ se } p = 1 \\\\ \\sum \\limits_{j=1}^p a_{ij} |\\mathbf{A}_{ij}| (-1)^{i+j} & \\text{ se } p &gt; 1\\end{cases}\\]\n\nsendo \\(\\mathbf{A}_{ij}\\) a matriz \\((p - 1) \\times (p - 1)\\) resultante da exclusão da \\(i\\)-ésima linha e \\(j\\)-ésima coluna de \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#determinante-1",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#determinante-1",
    "title": "Revisão de Álgebra Matricial",
    "section": "Determinante",
    "text": "Determinante"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#determinante-2",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#determinante-2",
    "title": "Revisão de Álgebra Matricial",
    "section": "Determinante",
    "text": "Determinante\n\nSejam as matrizes \\(\\mathbf{A}\\) e \\(\\mathbf{B}\\) quadradas de ordem \\(p\\) e seja \\(c\\) um escalar. Então,\n\n\\(\\left|c \\mathbf{A}\\right| = c^p \\left| \\mathbf{A} \\right|\\);\n\\(\\left|\\mathbf{A}^t \\right| = \\left|\\mathbf{A}\\right|\\);\n\\(\\left|\\mathbf{A}^{-1} \\right| = \\displaystyle{\\dfrac {1}{\\left|\\mathbf{A}\\right|}} = \\left|\\mathbf{A}\\right|^{-1}\\);\nSe \\(rank(\\mathbf{A}) &lt; p\\) então \\(|\\mathbf{A}| = 0\\);\nSe \\(rank(\\mathbf{A}) = p\\) então \\(|\\mathbf{A}| \\neq 0\\);"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#determinante-3",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#determinante-3",
    "title": "Revisão de Álgebra Matricial",
    "section": "Determinante",
    "text": "Determinante\n\nSejam as matrizes \\(\\mathbf{A}\\) e \\(\\mathbf{B}\\) quadradas de ordem \\(p\\) e seja \\(c\\) um escalar. Então,\n\n\\(\\left| \\mathbf{AB} \\right| = \\left|\\mathbf{A}\\right| \\left|\\mathbf{B}\\right|\\);\n\\(\\left| \\mathbf{ABA}^{-1} \\right| = \\left|\\mathbf{A}\\right| \\left|\\mathbf{B}\\right|  \\left|\\mathbf{A}^{-1}\\right|\\);\nSe \\(\\mathbf{A}\\) é uma matriz diagonal, então \\(|\\mathbf{A}| = \\displaystyle{\\prod_{i=1}^p a_{ii}}\\);\nSe uma matriz \\(\\mathbf{A}\\) é singular, então \\(\\left| \\mathbf{A} \\right| = 0\\);\nSe uma matriz \\(\\mathbf{A}\\) é não-singular, então \\(\\left| \\mathbf{A} \\right| \\neq 0\\);\nSe uma matriz \\(\\mathbf{A}\\) é positiva definida, então \\(\\left| \\mathbf{A} \\right| &gt; 0\\)."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#autovalores-e-autovetores",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#autovalores-e-autovetores",
    "title": "Revisão de Álgebra Matricial",
    "section": "Autovalores e autovetores",
    "text": "Autovalores e autovetores\n\nSeja \\(\\mathbf{A}\\) uma matriz quadrada e \\(\\mathbf{I}\\) a matriz identidade, ambas \\(p \\times p\\). Os escalares \\(\\lambda_1, \\lambda_2, \\cdots, \\lambda_p\\) que são a solução da equação polinomial \\(|\\mathbf{A} - \\lambda \\mathbf{I}| = 0\\) são chamados autovalores (ou valores característicos) de \\(\\mathbf{A}\\).\n\n\n\nA equação \\(|\\mathbf{A} - \\lambda \\mathbf{I}| = 0\\) (como função de \\(\\lambda\\)) é chamada equação característica.\n\n\n\n\nSeja \\(\\mathbf{A}\\) uma matriz quadrada \\(p \\times p\\) e \\(\\lambda\\) um autovalor de \\(\\mathbf{A}\\). Então, o vetor \\(\\mathbf{x}\\) \\((p \\times 1)\\), não nulo, que satisfaz:\n\n\\[\\mathbf{Ax} = \\lambda \\mathbf{x}\\]\né chamado autovetor (ou vetor característico) de \\(\\mathbf{A}\\) associado ao autovalor \\(\\lambda\\)."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#autovalores-e-autovetores-1",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#autovalores-e-autovetores-1",
    "title": "Revisão de Álgebra Matricial",
    "section": "Autovalores e autovetores",
    "text": "Autovalores e autovetores\n\nPara qualquer matriz simétrica \\(\\mathbf{A}\\) com autovalores \\(\\lambda_1, \\lambda_2, \\cdots, \\lambda_p\\), valem:\n\n\\[\\text{tr}(\\mathbf{A}) = \\displaystyle{\\sum_{i=1}^{p}\\lambda_{i}} \\hspace{1cm} \\text{e} \\hspace{1cm} \\left|\\mathbf{A} \\right| = \\displaystyle{\\prod_{i=1}^{p}\\lambda_{i}}\\]\n\n\nSe todos os autovalores da matriz \\(\\mathbf{A}\\) são positivos maiores que zero, então a matriz \\(\\mathbf{A}\\) é positiva definida;\n\n\n\n\nSe os autovalores da matriz \\(\\mathbf{A}\\) são positivos ou iguais a zero, então a matriz \\(\\mathbf{A}\\) é positiva semidefinida. Neste caso, o número de autovalores positivos será igual ao posto da matriz \\(\\mathbf{A}\\)\n\n\n\n\nOs autovetores de uma matriz \\(\\mathbf{A}\\) simétrica de dimensão \\(p \\times p\\) são ortogonais."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#teorema-da-decomposição-espectral",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#teorema-da-decomposição-espectral",
    "title": "Revisão de Álgebra Matricial",
    "section": "Teorema da decomposição espectral",
    "text": "Teorema da decomposição espectral\n\nComo resultado da ortogonalidade dos autovetores de \\(\\mathbf{A}\\) tem-se o Teorema da Decomposição Espectral.\n\n\n\nToda matriz simétrica \\(\\mathbf{A}\\) de ordem \\(p \\times p\\) pode ser decomposta em:\n\n\\[\\mathbf{A} = \\mathbf{C} \\mathbf{\\Lambda} \\mathbf{C}^t = \\displaystyle{\\sum_{i = 1}^p \\lambda_i {\\mathbf{e}_i \\mathbf{e}_i^t}}\\]"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#teorema-da-decomposição-espectral-1",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#teorema-da-decomposição-espectral-1",
    "title": "Revisão de Álgebra Matricial",
    "section": "Teorema da decomposição espectral",
    "text": "Teorema da decomposição espectral\nem que \\(\\mathbf{\\Lambda}\\) é a matriz diagonal dos autovalores:\n\\[\\mathbf{\\Lambda} = \\left[ \\begin{array}{cccc} \\lambda_1 & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\lambda_p \\end{array} \\right]\\]\ne \\(\\mathbf{C}\\) é a matriz ortogonal com os autovetores normalizados de \\(\\mathbf{A}\\) nas colunas:\n\\[\\mathbf{C} = \\left[\\begin{array}{rrrr} \\mathbf{e}_1 & \\mathbf{e}_2 & \\cdots & \\mathbf{e}_p \\end{array} \\right]\\]"
  },
  {
    "objectID": "cronograma.html",
    "href": "cronograma.html",
    "title": "Cronograma da Disciplina",
    "section": "",
    "text": "Esta página contém um esboço dos tópicos, conteúdos e tarefas para o semestre. Este cronograma será atualizado conforme o semestre avança.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSemana\nData\nTópico\nArtigo\nSlides\nEC\nLE\nScript\nMC\nProjeto\n\n\n\n\n1\nTer, 07/10\nApresentação da disciplina\n\n\n\n\n\n\n\n\n\n\nQui, 09/10\nNão haverá aula!\n\n\n\n\n\n\n\n\n\n2\nTer, 14/10\nRevisão de Álgebra Linear\n\n\n\n\n\n\n\n\n\n\nQui, 16/10\nRevisão de Álgebra Linear\n\n\n\n\n\n\n\n\n\n3\nTer, 21/10\nIntrodução à Estatística Multivariada\n\n\n\n\n\n\n\n\n\n\nQui, 23/10\nIntrodução à Estatística Multivariada\n\n\n\n\n\n\n\n\n\n4\nTer, 28/10\nIntrodução à Estatística Multivariada\n\n\n\n\n\n\n\n\n\n\nQui, 30/10\nNão haverá aula: Paralização docente\n\n\n\n\n\n\n\n\n\n5\nTer, 04/11\nDistribuição Normal Multivariada\n\n\n\n\n\n\n\n\n\n\nQui, 06/11\nDistribuição Normal Multivariada\n\n\n\n\n\n\n\n\n\n6\nTer, 11/11\nAnálise de Componentes Principais\n\n\n\n\n\n\n\n\n\n\nQui, 13/11\nAnálise de Componentes Principais",
    "crumbs": [
      "Cronograma da Disciplina"
    ]
  },
  {
    "objectID": "exercicios/lista02.html",
    "href": "exercicios/lista02.html",
    "title": "Lista de exercícios 02: Introdução à Estatística Multivariada",
    "section": "",
    "text": "Data de entrega: 17 de novembro de 2025\n\n\n\nSe \\(z_i = ay_i\\), para \\(i = 1 \\cdots n\\), mostre que \\({\\overline{z}} =  a {\\overline{y}}\\).\n\n\n\nSe \\(z_i = ay_i\\), para \\(i = 1 \\cdots n\\), mostre que \\(s^2_z = a^2s^2\\).\n\n\n\nA tabela abaixo fornece os dados de três variáveis (g/kg) medidas em 10 locais diferentes no Estado de Minas Gerais. As variáveis são:\n\n\\[\n\\begin{aligned}\nX_1 &= \\text{cálcio disponível no solo;} \\\\\nX_2 &= \\text{potássio disponível no solo;} \\\\\nX_3 &= \\text{fósforo disponível no solo.}\n\\end{aligned}\n\\]\n\n\n\n\n\nLocais\n\n\n\\(X_1\\)\n\n\n\\(X_2\\)\n\n\n\\(X_3\\)\n\n\n\n\n\n\n1\n\n\n35\n\n\n12\n\n\n2,4\n\n\n\n\n2\n\n\n35\n\n\n13\n\n\n2,1\n\n\n\n\n3\n\n\n40\n\n\n14\n\n\n1,9\n\n\n\n\n4\n\n\n25\n\n\n11\n\n\n1,8\n\n\n\n\n5\n\n\n26\n\n\n15\n\n\n2,3\n\n\n\n\n6\n\n\n32\n\n\n11\n\n\n2,5\n\n\n\n\n7\n\n\n21\n\n\n10\n\n\n3,0\n\n\n\n\n8\n\n\n30\n\n\n5\n\n\n1,0\n\n\n\n\n9\n\n\n33\n\n\n15\n\n\n1,1\n\n\n\n\n10\n\n\n27\n\n\n16\n\n\n2,6\n\n\n\n\n\nEncontre o vetor de médias amostrais \\(\\mathbf{\\overline{x}}\\).\nCalcule a matriz de covariâncias amostrais \\(\\boldsymbol{S}\\).\nObtenha a matriz de correlações amostrais \\(\\boldsymbol{R}\\).\n\n\n\nUsando os dados apresentados na Tabela acima, calcule:\n\n\nA variância generalizada amostral.\nA variância total amostral.\n\n\n\nUsando os dados apresentados na Tabela do exercício 3, defina \\(Z = 3X_1 - X_2 + 2X_3\\) e calcule \\(\\overline{z}\\) e \\(s^2_Z\\)\n\n\n\nUsando os dados apresentados na Tabela do exercício 3, defina \\(W = -2X_1 + 3X_2 + X_3\\) e calcule:\n\n\n\\(\\overline{w}\\) e \\(s^2_W\\).\n\\(\\overline{\\mathbf{y}}\\) e \\(\\boldsymbol{S}_y\\) se \\(\\boldsymbol{y} = \\left[\\begin{array}{cc} Z & W \\end{array} \\right]\\).\nEncontre a matriz \\(\\boldsymbol{R}_{y}\\).\n\n\n\nAinda utilizando os dados da Tabela do exercício 3, defina as seguintes combinações lineares das variáveis:\n\n\\[\n\\begin{aligned}\nZ_1 &= X_1 + X_2 + X_3 \\\\\nZ_2 &= 2X_1 - 3X_2 + 2X_3 \\\\\nZ_3 &= -X_1 - 2X_2 - 3X_3\n\\end{aligned}\n\\]\n\nEncontre \\(\\overline{\\mathbf{z}}\\) e \\(\\boldsymbol{S_z}\\).\nAtravés de \\(\\boldsymbol{S_z}\\), encontre \\(\\boldsymbol{R_z}\\).\n\n\n\nConsidere as amostras com 8 observações e 3 variáveis apresentadas a seguir:\n\n\n\n\n\n\n\\(X_1\\)\n\n\n3\n\n\n5\n\n\n6\n\n\n4\n\n\n8\n\n\n9\n\n\n6\n\n\n7\n\n\n\n\n\n\n\\(X_2\\)\n\n\n6\n\n\n11\n\n\n11\n\n\n9\n\n\n15\n\n\n16\n\n\n10\n\n\n12\n\n\n\n\n\\(X_3\\)\n\n\n14\n\n\n9\n\n\n9\n\n\n13\n\n\n2\n\n\n2\n\n\n9\n\n\n5\n\n\n\n\n\nCalcule \\(\\overline{\\mathbf{x}}, \\boldsymbol{S}, \\boldsymbol{R}\\).\nCalcule as distâncias euclidiana, euclidiana padronizada e Mahalanobis de um ponto \\(P = (X_1, X_2, X_3) = (5, 12, 8)\\) em relação ao a \\(\\overline{\\mathbf{x}}\\).\n\n\n\nSejam dois vetores aleatórios \\({\\mathbf{x}} = [2 \\hspace{0.2cm} 3]^t\\) e \\({\\mathbf{y}} = [2 \\hspace{0.2cm} 1]^t\\) e considere a matriz de covariâncias amostral igual a \\(\\boldsymbol{S} = \\left[ \\begin{array}{ll} 10 & 6 \\\\6 & 8 \\end{array} \\right]\\). Determine:\n\n\nA distância euclidiana entre os dois vetores.\nA distância generalizada de Karl Pearson.\nA distância generalizada de Mahalanobis.\n\n\n\nObtenha as variâncias generalizada e total da matriz de covariâncias amostral apresentada a seguir. Determine a matriz de correlações e as variâncias generalizada e total correspondentes.\n\n\\[\\boldsymbol{S} = \\left[ \\begin{array}{ll} 32 & 12 \\\\ 12 & 10 \\end{array} \\right]\\]"
  },
  {
    "objectID": "exercicios.html",
    "href": "exercicios.html",
    "title": "Listas de exercícios",
    "section": "",
    "text": "Lista de exercícios 01\nLista de exercícios 02\nLista de exercícios 03",
    "crumbs": [
      "Listas de exercícios"
    ]
  },
  {
    "objectID": "plano.html",
    "href": "plano.html",
    "title": "Plano de Ensino",
    "section": "",
    "text": "Leia com atenção o plano de ensino da disciplina que será oferecida neste período. Nele estão as regras do jogo.\n\nPlano de ensino",
    "crumbs": [
      "Plano de Ensino"
    ]
  },
  {
    "objectID": "programacao/semana-2.html",
    "href": "programacao/semana-2.html",
    "title": "Semana 02",
    "section": "",
    "text": "Revisão de Álgebra Matricial"
  },
  {
    "objectID": "programacao/semana-2.html#slides",
    "href": "programacao/semana-2.html#slides",
    "title": "Semana 02",
    "section": "",
    "text": "Revisão de Álgebra Matricial"
  },
  {
    "objectID": "programacao/semana-2.html#entrega-lista-de-exercícios-01",
    "href": "programacao/semana-2.html#entrega-lista-de-exercícios-01",
    "title": "Semana 02",
    "section": "Entrega lista de exercícios 01",
    "text": "Entrega lista de exercícios 01\n\n\n\n\n\n\nFique Atento!\n\n\n\n Lista de exercícios 01\nEntrega via Moodle\n\nData de entrega:\n\n31 de outubro de 2025"
  },
  {
    "objectID": "programacao/semana-4.html",
    "href": "programacao/semana-4.html",
    "title": "Semana 04",
    "section": "",
    "text": "Introdução à Estatística Multivariada"
  },
  {
    "objectID": "programacao/semana-4.html#slides",
    "href": "programacao/semana-4.html#slides",
    "title": "Semana 04",
    "section": "",
    "text": "Introdução à Estatística Multivariada"
  },
  {
    "objectID": "programacao/semana-4.html#script-r",
    "href": "programacao/semana-4.html#script-r",
    "title": "Semana 04",
    "section": "Script R",
    "text": "Script R\nDescrição e visualização de dados multivariados utilizando a base Iris"
  },
  {
    "objectID": "programacao/semana-4.html#entrega-lista-de-exercícios-02",
    "href": "programacao/semana-4.html#entrega-lista-de-exercícios-02",
    "title": "Semana 04",
    "section": "Entrega lista de exercícios 02",
    "text": "Entrega lista de exercícios 02\n\n\n\n\n\n\nFique Atento!\n\n\n\n Lista de exercícios 02\nEntrega via Moodle\n\nData de entrega:\n\n17 de novembro de 2025"
  },
  {
    "objectID": "programacao/semana-6.html",
    "href": "programacao/semana-6.html",
    "title": "Semana 06",
    "section": "",
    "text": "Análise de Componentes Principais"
  },
  {
    "objectID": "programacao/semana-6.html#slides",
    "href": "programacao/semana-6.html#slides",
    "title": "Semana 06",
    "section": "",
    "text": "Análise de Componentes Principais"
  },
  {
    "objectID": "programacao/semana-6.html#entrega-lista-de-exercícios-03",
    "href": "programacao/semana-6.html#entrega-lista-de-exercícios-03",
    "title": "Semana 06",
    "section": "Entrega lista de exercícios 03",
    "text": "Entrega lista de exercícios 03\n\n\n\n\n\n\nFique Atento!\n\n\n\n Lista de exercícios 03\nEntrega via Moodle\n\nData de entrega:\n\n24 de novembro de 2025"
  }
]