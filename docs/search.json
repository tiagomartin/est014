[
  {
    "objectID": "programacao/semana-2.html",
    "href": "programacao/semana-2.html",
    "title": "Semana 02",
    "section": "",
    "text": "Revisão de Álgebra Matricial"
  },
  {
    "objectID": "programacao/semana-2.html#slides",
    "href": "programacao/semana-2.html#slides",
    "title": "Semana 02",
    "section": "",
    "text": "Revisão de Álgebra Matricial"
  },
  {
    "objectID": "programacao/semana-2.html#entrega-lista-de-exercícios-01",
    "href": "programacao/semana-2.html#entrega-lista-de-exercícios-01",
    "title": "Semana 02",
    "section": "Entrega lista de exercícios 01",
    "text": "Entrega lista de exercícios 01\n\n\n\n\n\n\nFique Atento!\n\n\n\n Lista de exercícios 01\nEntrega via Moodle\n\nData de entrega:\n\n27 de outubro de 2025"
  },
  {
    "objectID": "plano.html",
    "href": "plano.html",
    "title": "Plano de Ensino",
    "section": "",
    "text": "Leia com atenção o plano de ensino da disciplina que será oferecida neste período. Nele estão as regras do jogo.\n\nPlano de ensino",
    "crumbs": [
      "Plano de Ensino"
    ]
  },
  {
    "objectID": "exercicios.html",
    "href": "exercicios.html",
    "title": "Listas de exercícios",
    "section": "",
    "text": "Lista de exercícios 01",
    "crumbs": [
      "Listas de exercícios"
    ]
  },
  {
    "objectID": "cronograma.html",
    "href": "cronograma.html",
    "title": "Cronograma da Disciplina",
    "section": "",
    "text": "Esta página contém um esboço dos tópicos, conteúdos e tarefas para o semestre. Este cronograma será atualizado conforme o semestre avança.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSemana\nData\nTópico\nArtigo\nSlides\nEC\nLE\nScript\nMC\nProjeto\n\n\n\n\n1\nTer, 07/10\nApresentação da disciplina\n\n\n\n\n\n\n\n\n\n\nQui, 09/10\nNão haverá aula!\n\n\n\n\n\n\n\n\n\n2\nTer, 14/10\nRevisão de Álgebra Linear\n\n\n\n\n\n\n\n\n\n\nQui, 16/10\nRevisão de Álgebra Linear",
    "crumbs": [
      "Cronograma da Disciplina"
    ]
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas",
    "title": "Revisão de Álgebra Matricial",
    "section": "Definição e propriedades básicas",
    "text": "Definição e propriedades básicas\n\nUma matriz é um conjunto de números ou variáveis dispostos em linhas e colunas.\n\n\n\nUma matriz \\(\\mathbf{A}\\) de \\(n\\) linhas e \\(p\\) colunas (dimensão \\(n \\times p\\)) pode ser representada, genericamente, por:\n\n\\[{\\mathbf A} = \\left[ \\begin{array}{cccc} a_{11} & a_{12} & \\cdots & a_{1p} \\\\ a_{21} & a_{22} & \\cdots & a_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{np} \\end{array} \\right]\\]\n\n\n\nA matriz \\(\\mathbf{A}\\) pode ser denotada ainda por \\(\\mathbf{A} = \\{a_{ij}\\}\\), onde o primeiro índice indica linha, o segundo coluna e \\(a_{ij}\\) é o termo geral da matriz."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-1",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-1",
    "title": "Revisão de Álgebra Matricial",
    "section": "Definição e propriedades básicas",
    "text": "Definição e propriedades básicas\n\nUm vetor \\(\\mathbf{x}\\), de dimensão \\(n\\), é representado, genericamente, por:\n\n\\[\\mathbf{x} = \\left[ \\begin{array}{c} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{array} \\right] \\]\n\n\nNuma análise multivariada com \\(n\\) indivíduos e \\(p\\) variáveis, as linhas da matriz de dados (observações dos indivíduos) podem ser consideradas \\(n\\) vetores de dimensão \\(p\\): \\(\\mathbf{x}_i^t = (x_{i1}, x_{i2}, \\cdots, x_{ip}), \\,\\,\\,\\,\\, i = 1, 2, \\cdots, n\\);"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-2",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-2",
    "title": "Revisão de Álgebra Matricial",
    "section": "Definição e propriedades básicas",
    "text": "Definição e propriedades básicas\n\nAs colunas da matriz de dados (observações referentes à variáveis) podem ser consideradas \\(p\\) vetores de dimensão \\(n\\):\n\n\\[\\mathbf{x}_j^t = (x_{1j}, x_{2j}, \\cdots, x_{nj}), \\,\\,\\,\\,\\, j = 1, 2, \\cdots, p\\]\n\n\nA multiplicação de um vetor \\(\\mathbf{x} = (x_1, x_2 , \\cdots, x_p)^t\\) por um escalar real \\(c\\) resulta em um vetor \\(\\mathbf{y} = c \\mathbf{x} = (cx_1 , cx_2 , \\cdots, cx_p)^t\\), de igual dimensão em relação ao vetor original;\n\n\n\n\nGeometricamente, a multiplicação de um vetor por um escalar pode mudar seu tamanho e sentido, mas não sua direção."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-3",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-3",
    "title": "Revisão de Álgebra Matricial",
    "section": "Definição e propriedades básicas",
    "text": "Definição e propriedades básicas\n\nA soma de dois vetores \\(\\mathbf{x}\\) e \\(\\mathbf{y}\\), de iguais dimensões, resulta em um terceiro vetor dado por:\n\n\\[\\mathbf{z} = \\mathbf{x} + \\mathbf{y} = (x_1 + y_1, x_2 + y_2, \\cdots, x_p + y_p)^t\\]\n\n\nA diferença de dois vetores \\(\\mathbf{x}\\) e \\(\\mathbf{y}\\), de iguais dimensões, resulta em um terceiro vetor dado por:\n\n\\[\\mathbf{w} = \\mathbf{x} - \\mathbf{y} = (x_1 - y_1, x_2 - y_2, \\cdots, x_p - y_p)^t\\]"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-4",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-4",
    "title": "Revisão de Álgebra Matricial",
    "section": "Definição e propriedades básicas",
    "text": "Definição e propriedades básicas\n\nO produto interno de dois vetores \\(\\mathbf{x}\\) e \\(\\mathbf{y}\\) é definido por:\n\n\\[\\mathbf{v} = \\mathbf{x}^t\\mathbf{y} = \\displaystyle{\\sum_{i=1}^{p}} x_iy_i = x_1 y_1 + x_2 y_2 + \\cdots + x_p y_p\\]\n\n\nO tamanho do vetor \\(\\mathbf{x} = (x_1, x_2, \\cdots, x_p)^t\\) é definido pela distância do ponto \\(p\\)-dimensional, determinado por suas coordenadas, à origem:\n\n\\[L_x = \\sqrt{\\mathbf{x}^t\\mathbf{x}} = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_p^2}\\]"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-5",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-5",
    "title": "Revisão de Álgebra Matricial",
    "section": "Definição e propriedades básicas",
    "text": "Definição e propriedades básicas\n\nO cosseno do ângulo \\(\\theta\\) entre os vetores \\(\\mathbf{x}\\) e \\(\\mathbf{y}\\) definidos em \\(\\mathbb{R}^p\\) é dado por:\n\n\\[\\cos({\\theta}) = \\dfrac{\\mathbf{x}^t\\mathbf{y}}{\\sqrt{\\mathbf{x}^t\\mathbf{x}} \\sqrt{\\mathbf{y}^t\\mathbf{y}}}\\]\n\n\nDois vetores \\(\\mathbf{x}\\) e \\(\\mathbf{y}\\) são entre si se o ângulo \\(\\theta\\) entre eles é \\(90^o\\), de tal forma que \\(\\cos(\\theta) = 0\\), ou, de forma equivalente, \\(\\mathbf{x}^t\\mathbf{y} = 0\\).\n\n\n\n\nA normalização de um vetor \\(\\mathbf{x}\\) corresponde à divisão de \\(\\mathbf{x}\\) por \\(L_x\\), de tal forma que o vetor resultante tenha comprimento unitário:\n\n\\[\\mathbf{x}^* = \\dfrac{\\mathbf{x}}{L_x}\\]"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-6",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-6",
    "title": "Revisão de Álgebra Matricial",
    "section": "Definição e propriedades básicas",
    "text": "Definição e propriedades básicas\n\nA projeção de um vetor \\(\\mathbf{x}\\) em um vetor \\(\\mathbf{y}\\) é um novo vetor, com coordenadas:\n\n\\[\\text{Projeção de } \\mathbf{x} \\text{ em } \\mathbf{y} = \\dfrac{\\mathbf{x}^t\\mathbf{y}}{\\mathbf{y}^t\\mathbf{y}} \\mathbf{y}\\]\n\n\nO comprimento da projeção de \\(\\mathbf{x}\\) em \\(\\mathbf{y}\\) é dado por:\n\n\\[\\text{Tamanho da projeção de } \\mathbf{x} \\text{ em } \\mathbf{y} = \\dfrac{|\\mathbf{x}^t\\mathbf{y}|}{L_y} =  L_x \\cos(\\theta)\\]"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-7",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-7",
    "title": "Revisão de Álgebra Matricial",
    "section": "Definição e propriedades básicas",
    "text": "Definição e propriedades básicas\n\nIgualdade de matrizes: Dizemos que duas matrizes \\(\\mathbf{A}\\) e \\(\\mathbf{B}\\) são iguais se elas tem iguais dimensões e \\(\\{a_{ij}\\} = \\{b_{ij}\\}\\) para todo \\(i\\) e para todo \\(j\\).\n\n\n\nMatriz transposta: A transposta de uma matriz \\(\\mathbf{A}_{n \\times p}\\) é a matriz \\(\\mathbf{A}^t_{p \\times n}\\) tal que \\(\\{a_{ij}\\} = \\{a_{ji}\\}\\) para todo \\(i\\) e para todo \\(j\\):\n\n\\[\\mathbf{A}^t = \\left[ \\begin{array}{cccc} a_{11} & a_{21} & \\cdots & a_{n1} \\\\ a_{12} & a_{22} & \\cdots & a_{n2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        a_{1p} & a_{2p} & \\cdots & a_{np} \\end{array} \\right]\\]"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-8",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-8",
    "title": "Revisão de Álgebra Matricial",
    "section": "Definição e propriedades básicas",
    "text": "Definição e propriedades básicas\n\nMatriz simétrica: Dizemos que uma matriz \\(\\mathbf{A}_{p \\times p}\\) é simétrica se \\(\\{a_{ij}\\} = \\{a_{ji}\\}\\) para todo \\(i\\) e para todo \\(j\\), ou seja, \\(\\mathbf{A}^t = \\mathbf{A}\\).\n\n\n\nDiagonal de uma matriz: A diagonal de uma matriz quadrada \\(\\mathbf{A}_{p \\times p}\\) corresponde ao conjunto de elementos \\(a_{11}, a_{22}, \\cdots, a_{pp}\\).\n\n\n\n\nMatriz diagonal: Dizemos que a matriz quadrada \\(\\mathbf{A}_{p \\times p}\\) é diagonal se todos os elementos fora da diagonal são iguais a zero:\n\n\\[\\mathbf{A} = \\left[ \\begin{array}{cccc} a_{11} & 0 & \\cdots & 0 \\\\ 0 & a_{22} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        0 & 0 & \\cdots & a_{pp} \\end{array} \\right]\\]"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-9",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-9",
    "title": "Revisão de Álgebra Matricial",
    "section": "Definição e propriedades básicas",
    "text": "Definição e propriedades básicas\n\nMatriz identidade: Dizemos que a matriz quadrada \\(\\mathbf{I}_{p\\times p}\\) é uma matriz identidade se ela é uma matriz diagonal com todos os elementos da diagonal iguais a 1:\n\n\\[\\mathbf{I} = \\left[ \\begin{array}{cccc} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        0 & 0 & \\cdots & 1 \\end{array} \\right]\\]"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-10",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#definição-e-propriedades-básicas-10",
    "title": "Revisão de Álgebra Matricial",
    "section": "Definição e propriedades básicas",
    "text": "Definição e propriedades básicas\n\nMatriz triangular superior: Dizemos que a matriz quadrada \\(\\mathbf{A}_{p \\times p}\\) é uma matriz triangular superior se todos os elementos abaixo da diagonal são iguais a zero:\n\n\\[\\mathbf{A} = \\left[ \\begin{array}{cccc} a_{11} & a_{12} & \\cdots & a_{1p} \\\\ 0 & a_{22} & \\cdots & a_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & a_{pp} \\end{array} \\right]\\]\n\n\nUma matriz triangular inferior é definida de forma semelhante."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes",
    "title": "Revisão de Álgebra Matricial",
    "section": "Operações envolvendo matrizes",
    "text": "Operações envolvendo matrizes\n\nA soma de duas matrizes \\(\\mathbf{A}_{n \\times p}\\) e \\(\\mathbf{B}_{n \\times p}\\) de iguais dimensões é a matriz resultante das somas dos elementos nas posições correspondentes:\n\n\\[\\mathbf{A} + \\mathbf{B} = \\left[ \\begin{array}{cccc} a_{11} + b_{11} & a_{12} + b_{12} & \\cdots & a_{1p} + b_{1p}\\\\ a_{21} +  b_{21}& a_{22} + b_{22}& \\cdots & a_{2p} + b_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        a_{n1} + b_{n1} & a_{n2} + b_{n2} & \\cdots & a_{np} + b_{np} \\end{array} \\right]\\]"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes-1",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes-1",
    "title": "Revisão de Álgebra Matricial",
    "section": "Operações envolvendo matrizes",
    "text": "Operações envolvendo matrizes\n\nA diferença de duas matrizes \\(\\mathbf{A}_{n \\times p}\\) e \\(\\mathbf{B}_{n \\times p}\\) de iguais dimensões é a matriz resultante das diferenças dos elementos nas posições correspondentes:\n\n\\[\\mathbf{A} - \\mathbf{B} = \\left[ \\begin{array}{cccc} a_{11} - b_{11} & a_{12} - b_{12} & \\cdots & a_{1p} - b_{1p}\\\\ a_{21} -  b_{21}& a_{22} - b_{22}& \\cdots & a_{2p} - b_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\       a_{n1} - b_{n1} & a_{n2} - b_{n2} & \\cdots & a_{np} - b_{np} \\end{array} \\right]\\]"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes-2",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes-2",
    "title": "Revisão de Álgebra Matricial",
    "section": "Operações envolvendo matrizes",
    "text": "Operações envolvendo matrizes\n\nSejam \\(\\mathbf{A}_{n \\times k}\\) e \\(\\mathbf{B}_{k \\times p}\\) duas matrizes, tais que o número de linhas da segunda é igual ao número de colunas da primeira. O produto \\(\\mathbf{AB}\\) é definido por:\n\n\\[\\mathbf{A} \\mathbf{B} = \\left[ \\begin{array}{cccc} \\sum_{r = 1}^k a_{1r}. b_{r1} & \\sum_{r = 1}^k a_{1r}. b_{r2} & \\cdots & \\sum_{r = 1}^k a_{1r}. b_{rp}\\\\ \\sum_{r = 1}^k a_{2r}. b_{r1} & \\sum_{r = 1}^k a_{2r}. b_{r2} & \\cdots & \\sum_{r = 1}^k a_{2r}. b_{rp} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        \\sum_{r = 1}^k a_{nr}. b_{r1} & \\sum_{r = 1}^k a_{nr}. b_{r2} & \\cdots & \\sum_{r = 1}^k a_{nr}. b_{rp} \\end{array} \\right]\\]\n\n\nDizemos que uma matriz quadrada \\(\\mathbf{Q}\\) é ortogonal se \\(\\mathbf{QQ}^t = \\mathbf{Q}^t \\mathbf{Q} = \\mathbf{I}\\)."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes-3",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes-3",
    "title": "Revisão de Álgebra Matricial",
    "section": "Operações envolvendo matrizes",
    "text": "Operações envolvendo matrizes\n\nSejam \\(\\mathbf{A}_{n \\times p}\\) e \\(c\\) uma constante. O produto \\(c \\mathbf{A}\\) resulta no produto de cada elemento de \\(\\mathbf{A}\\) por \\(c\\):\n\n\\[c\\mathbf{A} = \\left[ \\begin{array}{cccc} ca_{11} & ca_{12} & \\cdots & ca_{1p} \\\\ ca_{21} & ca_{22} & \\cdots & ca_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    ca_{n1} & ca_{n2} & \\cdots & ca_{np} \\end{array} \\right]\\]"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes-4",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes-4",
    "title": "Revisão de Álgebra Matricial",
    "section": "Operações envolvendo matrizes",
    "text": "Operações envolvendo matrizes\n\nSejam \\(\\mathbf{A}\\), \\(\\mathbf{B}\\) e \\(\\mathbf{C}\\) matrizes com dimensões compatíveis para as operações consideradas. Então:\n\n\\((\\mathbf{A}^t)^t = \\mathbf{A}\\);\n\\(\\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}\\);\n\\((\\mathbf{A} + \\mathbf{B})^t = \\mathbf{A}^t + \\mathbf{B}^t\\);\n\\((\\mathbf{A} - \\mathbf{B})^t = \\mathbf{A}^t - \\mathbf{B}^t\\);\n\\((\\mathbf{AB})^t = \\mathbf{B}^t \\mathbf{A}^t\\);\n\\(\\mathbf{AB} \\neq \\mathbf{BA}\\), a menos de situações bem específicas;"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes-5",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes-5",
    "title": "Revisão de Álgebra Matricial",
    "section": "Operações envolvendo matrizes",
    "text": "Operações envolvendo matrizes\n\nSejam \\(\\mathbf{A}\\), \\(\\mathbf{B}\\) e \\(\\mathbf{C}\\) matrizes com dimensões compatíveis para as operações consideradas. Então:\n\n\\(\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{AB} + \\mathbf{AC}\\), valendo o mesmo ao substituir a soma pela diferença;\n\\((\\mathbf{A} + \\mathbf{B}) \\mathbf{C} = \\mathbf{AC} + \\mathbf{BC}\\), valendo o mesmo ao substituir a soma pela diferença;\n\\((\\mathbf{A} + \\mathbf{B})\\mathbf{C} \\neq \\mathbf{CA} + \\mathbf{BA}\\), a menos de situações bem específicas;\n\\(\\mathbf{IA} = \\mathbf{AI} = \\mathbf{A}\\), para qualquer \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes-6",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#operações-envolvendo-matrizes-6",
    "title": "Revisão de Álgebra Matricial",
    "section": "Operações envolvendo matrizes",
    "text": "Operações envolvendo matrizes\n\nO traço de uma matriz de uma matriz \\(\\mathbf{A}_{p \\times p}\\), denotado por \\(\\text{tr}(\\mathbf{A})\\), corresponde à soma dos elementos da diagonal de \\(\\mathbf{A}\\):\n\n\\[\\text{tr}(\\mathbf{A}) = \\displaystyle{\\sum_{i=1}^{p}a_{ii}}\\]\n\n\nSejam \\(\\mathbf{A}\\) e \\(\\mathbf{B}\\) matrizes quadradas. Então:\n\n\\(\\text{tr}(\\mathbf{A} + \\mathbf{B}) = \\text{tr}(\\mathbf{A}) + \\text{tr}(\\mathbf{B})\\)\n\\(\\text{tr}(\\mathbf{A} \\mathbf{B}) = \\text{tr}(\\mathbf{B} \\mathbf{A})\\)"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#combinações-lineares-e-formas-quadráticas",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#combinações-lineares-e-formas-quadráticas",
    "title": "Revisão de Álgebra Matricial",
    "section": "Combinações lineares e formas quadráticas",
    "text": "Combinações lineares e formas quadráticas\n\nPara um conjunto de constantes \\(a_1, a_2, \\cdots, a_p\\), o vetor \\(\\mathbf{y} = a_1 \\mathbf{x}_1 + a_2 \\mathbf{x}_ 2 + \\cdots + a_p \\mathbf{x}_p\\) é uma combinação linear dos vetores \\(\\mathbf{x}_1, \\mathbf{x}_2, \\cdots, \\mathbf{x}_p\\) .\n\n\n\nO conjunto de vetores \\(\\mathbf{x}_1, \\mathbf{x}_2, \\cdots, \\mathbf{x}_p\\) é dito linearmente dependente se há um conjunto de constantes \\(a_1, a_2, \\cdots, a_p\\), nem todas nulas, tal que:\n\n\\[a_1 \\mathbf{x}_1 + a_2 \\mathbf{x}_ 2 + \\cdots + a_p \\mathbf{x}_p = 0\\]\n\n\n\nCaso contrário os vetores são linearmente independentes."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#combinações-lineares-e-formas-quadráticas-1",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#combinações-lineares-e-formas-quadráticas-1",
    "title": "Revisão de Álgebra Matricial",
    "section": "Combinações lineares e formas quadráticas",
    "text": "Combinações lineares e formas quadráticas\n\nFormas quadráticas surgem de forma recorrente na estatística multivariada, por exemplo, na definição de distâncias.\n\n\n\nUma forma quadrática, definida a partir de uma matriz simétrica \\(\\mathbf{A}_{p \\times p}\\), é definida como:\n\n\\[Q(\\mathbf{x}) = {\\mathbf{x}^t} \\mathbf{A} \\mathbf{x} =\\displaystyle{\\sum_{i=1}^p a_{ii} x_{i}^2} + 2 \\displaystyle{\\sum_{i = 1}^{p-1}} \\displaystyle{\\sum_{k = i+1}^{p}} a_{ik} x_i  x_k  = \\displaystyle{\\sum_{i=1}^p} \\displaystyle{\\sum_{k=1}^p} a_{ik} x_i  x_k\\]\npara \\(\\mathbf{x} \\neq \\mathbf{0}\\) definido em \\(\\mathbb{R}^p\\)."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#combinações-lineares-e-formas-quadráticas-2",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#combinações-lineares-e-formas-quadráticas-2",
    "title": "Revisão de Álgebra Matricial",
    "section": "Combinações lineares e formas quadráticas",
    "text": "Combinações lineares e formas quadráticas\n\nClassificamos a matriz \\(\\mathbf{A}\\), e a consequente forma quadrática \\(\\mathbf{x}^t \\mathbf{A}\\mathbf{x}\\), como positiva definida se \\(Q(\\mathbf{x}) &gt; 0\\) para qualquer \\(\\mathbf{x} \\neq \\mathbf{0}\\).\n\n\n\nOutras classificações:\n\nPositiva semidefinida: \\(Q(\\mathbf{x}) \\geqslant 0\\)\nNegativa definida: \\(Q(\\mathbf{x}) &lt; 0\\)\nNegativa semidefinida: \\(Q(\\mathbf{x}) \\leqslant 0\\)\nIndefinida: \\(Q(\\mathbf{x}) &gt; 0\\) para alguns \\(\\mathbf{x} \\in \\mathbb{R}^p\\) e \\(Q(\\mathbf{x}) &lt; 0\\) para outros \\(\\mathbf{x} \\in \\mathbb{R}^p\\)"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#matriz-inversa",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#matriz-inversa",
    "title": "Revisão de Álgebra Matricial",
    "section": "Matriz inversa",
    "text": "Matriz inversa\n\nMatriz inversa: Considere uma matriz \\(\\mathbf{A}_{p \\times p}\\). Caso exista uma matriz \\(\\mathbf{B}_{p \\times p}\\) tal que\n\n\\[\\mathbf{AB} = \\mathbf{BA} = \\mathbf{I}\\]\ndizemos que \\(\\mathbf{B}\\) é a matriz inversa de \\(\\mathbf{A}\\), sendo usualmente denotada por \\(\\mathbf{A}^{-1}\\).\n\n\nQuando uma matriz possui uma matriz inversa, dizemos que ela é não-singular. Caso contrário, ela é classificada como singular."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#matriz-inversa-1",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#matriz-inversa-1",
    "title": "Revisão de Álgebra Matricial",
    "section": "Matriz inversa",
    "text": "Matriz inversa\n\nA condição fundamental para que uma matriz tenha inversa é que suas colunas sejam linearmente independentes (matriz de \\(rank\\) completo).\n\n\n\nO \\(rank\\) de uma matriz \\(\\mathbf{A}_{n \\times p}\\) , denotado por \\(rank(\\mathbf{A})\\), é definido como o número de linhas (ou colunas) linearmente independentes de \\(\\mathbf{A}\\).\n\n\n\n\nDizemos que a matriz quadrada \\(\\mathbf{A}_{p \\times p}\\) tem \\(rank\\) completo se \\(rank(\\mathbf{A}) = p\\), configurando uma matriz não singular.\n\n\n\n\nPara matrizes de \\(rank\\) incompleto ou não-quadradas, define-se a inversa generalizada de \\(\\mathbf{A}\\) como a matriz \\(\\mathbf{A}^-\\) que satisfaz \\(\\mathbf{A} \\mathbf{A}^- \\mathbf{A} = \\mathbf{A}\\)."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#matriz-inversa-2",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#matriz-inversa-2",
    "title": "Revisão de Álgebra Matricial",
    "section": "Matriz inversa",
    "text": "Matriz inversa\n\nA inversa de uma matriz diagonal é dada pela matriz diagonal composta pelos inversos dos elementos da matriz original:\n\n\\[\\mathbf{A} = \\left[ \\begin{array}{cccc} a_{11} & 0 & \\cdots & 0 \\\\ 0 & a_{22} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & a_{pp} \\end{array} \\right]; \\,\\,\\,\\,\\,\\,\\,\\,\\, \\mathbf{A}^{-1} = \\left[ \\begin{array}{cccc} \\frac{1}{a_{11}} & 0 & \\cdots & 0 \\\\ 0 & \\frac{1}{a_{22}} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\frac{1}{a_{pp}} \\end{array} \\right]\\]"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#matriz-inversa-3",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#matriz-inversa-3",
    "title": "Revisão de Álgebra Matricial",
    "section": "Matriz inversa",
    "text": "Matriz inversa\n\n\\(\\mathbf{A}\\) e \\(\\mathbf{B}\\) não singulares \\((p \\times p)\\), \\((\\mathbf{AB})^{-1} = \\mathbf{B}^{-1} \\mathbf{A}^{-1}\\);\n\n\n\nPara \\(c\\) uma constante real diferente de zero, \\((c \\mathbf{B})^{-1} = c^{-1}(\\mathbf{A})^{-1}\\);\n\n\n\n\n\\((\\mathbf{A}^t)^{-1} = (\\mathbf{A}^{-1})^t\\);\n\n\n\n\nSe \\(rank(\\mathbf{A}) = p\\) então \\(\\mathbf{A}^{-1}\\) existe;\n\n\n\n\nSe \\(\\mathbf{A}\\) é ortogonal, então \\(\\mathbf{A}^{-1}\\) existe, além do que \\(\\mathbf{A}^{-1} = \\mathbf{A}^t\\);\n\n\n\n\nSe \\(\\mathbf{B}\\) é não singular, \\(\\mathbf{AB} = \\mathbf{CB}\\) implica \\(\\mathbf{A} = \\mathbf{C}\\)."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#determinante",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#determinante",
    "title": "Revisão de Álgebra Matricial",
    "section": "Determinante",
    "text": "Determinante\n\nO determinante de uma matriz \\(\\mathbf{A}_{p \\times p}\\) , denotado por \\(\\det(\\mathbf{A})\\) ou \\(|\\mathbf{A}|\\), é definido como:\n\\[\\det(\\mathbf{A}) = \\begin{cases} a_{11} & \\text{ se } p = 1 \\\\ \\sum \\limits_{j=1}^p a_{ij} |\\mathbf{A}_{ij}| (-1)^{i+j} & \\text{ se } p &gt; 1\\end{cases}\\]\n\nsendo \\(\\mathbf{A}_{ij}\\) a matriz \\((p - 1) \\times (p - 1)\\) resultante da exclusão da \\(i\\)-ésima linha e \\(j\\)-ésima coluna de \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#determinante-1",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#determinante-1",
    "title": "Revisão de Álgebra Matricial",
    "section": "Determinante",
    "text": "Determinante"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#determinante-2",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#determinante-2",
    "title": "Revisão de Álgebra Matricial",
    "section": "Determinante",
    "text": "Determinante\n\nSejam as matrizes \\(\\mathbf{A}\\) e \\(\\mathbf{B}\\) quadradas de ordem \\(p\\) e seja \\(c\\) um escalar. Então,\n\n\\(\\left|c \\mathbf{A}\\right| = c^n \\left| \\mathbf{A} \\right|\\);\n\\(\\left|\\mathbf{A}^t \\right| = \\left|\\mathbf{A}\\right|\\);\n\\(\\left|\\mathbf{A}^{-1} \\right| = \\displaystyle{\\dfrac {1}{\\left|\\mathbf{A}\\right|}} = \\left|\\mathbf{A}\\right|^{-1}\\);\nSe \\(rank(\\mathbf{A}) &lt; p\\) então \\(|\\mathbf{A}| = 0\\);\nSe \\(rank(\\mathbf{A}) = p\\) então \\(|\\mathbf{A}| \\neq 0\\);"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#determinante-3",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#determinante-3",
    "title": "Revisão de Álgebra Matricial",
    "section": "Determinante",
    "text": "Determinante\n\nSejam as matrizes \\(\\mathbf{A}\\) e \\(\\mathbf{B}\\) quadradas de ordem \\(p\\) e seja \\(c\\) um escalar. Então,\n\n\\(\\left| \\mathbf{AB} \\right| = \\left|\\mathbf{A}\\right| \\left|\\mathbf{B}\\right|\\);\n\\(\\left| \\mathbf{ABA}^{-1} \\right| = \\left|\\mathbf{A}\\right| \\left|\\mathbf{B}\\right|  \\left|\\mathbf{A}^{-1}\\right|\\);\nSe \\(\\mathbf{A}\\) é uma matriz diagonal, então \\(|\\mathbf{A}| = \\displaystyle{\\prod_{i=1}^p a_{ii}}\\);\nSe uma matriz \\(\\mathbf{A}\\) é singular, então \\(\\left| \\mathbf{A} \\right| = 0\\);\nSe uma matriz \\(\\mathbf{A}\\) é não-singular, então \\(\\left| \\mathbf{A} \\right| \\neq 0\\);\nSe uma matriz \\(\\mathbf{A}\\) é positiva definida, então \\(\\left| \\mathbf{A} \\right| &gt; 0\\)."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#autovalores-e-autovetores",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#autovalores-e-autovetores",
    "title": "Revisão de Álgebra Matricial",
    "section": "Autovalores e autovetores",
    "text": "Autovalores e autovetores\n\nSeja \\(\\mathbf{A}\\) uma matriz quadrada e \\(\\mathbf{I}\\) a matriz identidade, ambas \\(p \\times p\\). Os escalares \\(\\lambda_1, \\lambda_2, \\cdots, \\lambda_p\\) que são a solução da equação polinomial \\(|\\mathbf{A} - \\lambda \\mathbf{I}| = 0\\) são chamados autovalores (ou valores característicos) de \\(\\mathbf{A}\\).\n\n\n\nA equação \\(|\\mathbf{A} - \\lambda \\mathbf{I}| = 0\\) (como função de \\(\\lambda\\)) é chamada equação característica.\n\n\n\n\nSeja \\(\\mathbf{A}\\) uma matriz quadrada \\(p \\times p\\) e \\(\\lambda\\) um autovalor de \\(\\mathbf{A}\\). Então, o vetor \\(\\mathbf{x}\\) \\((p \\times 1)\\), não nulo, que satisfaz:\n\n\\[\\mathbf{Ax} = \\lambda \\mathbf{x}\\]\né chamado autovetor (ou vetor característico) de \\(\\mathbf{A}\\) associado ao autovalor \\(\\lambda\\)."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#autovalores-e-autovetores-1",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#autovalores-e-autovetores-1",
    "title": "Revisão de Álgebra Matricial",
    "section": "Autovalores e autovetores",
    "text": "Autovalores e autovetores\n\nPara qualquer matriz simétrica \\(\\mathbf{A}\\) com autovalores \\(\\lambda_1, \\lambda_2, \\cdots, \\lambda_p\\), valem:\n\n\\[\\text{tr}(\\mathbf{A}) = \\displaystyle{\\sum_{i=1}^{p}\\lambda_{i}} \\hspace{1cm} \\text{e} \\hspace{1cm} \\left|\\mathbf{A} \\right| = \\displaystyle{\\prod_{i=1}^{p}\\lambda_{i}}\\]\n\n\nSe todos os autovalores da matriz \\(\\mathbf{A}\\) são positivos maiores que zero, então a matriz \\(\\mathbf{A}\\) é positiva definida;\n\n\n\n\nSe os autovalores da matriz \\(\\mathbf{A}\\) são positivos ou iguais a zero, então a matriz \\(\\mathbf{A}\\) é positiva semidefinida. Neste caso, o número de autovalores positivos será igual ao posto da matriz \\(\\mathbf{A}\\)\n\n\n\n\nOs autovetores de uma matriz \\(\\mathbf{A}\\) simétrica de dimensão \\(p \\times p\\) são ortogonais."
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#teorema-da-decomposição-espectral",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#teorema-da-decomposição-espectral",
    "title": "Revisão de Álgebra Matricial",
    "section": "Teorema da decomposição espectral",
    "text": "Teorema da decomposição espectral\n\nComo resultado da ortogonalidade dos autovetores de \\(\\mathbf{A}\\) tem-se o Teorema da Decomposição Espectral.\n\n\n\nToda matriz simétrica \\(\\mathbf{A}\\) de ordem \\(p \\times p\\) pode ser decomposta em:\n\n\\[\\mathbf{A} = \\mathbf{C} \\mathbf{\\Lambda} \\mathbf{C}^t = \\displaystyle{\\sum_{i = 1}^p \\lambda_i {\\mathbf{e}_i \\mathbf{e}_i^t}}\\]"
  },
  {
    "objectID": "aulas/revisao_alg_linear/rev_alg_linear.html#teorema-da-decomposição-espectral-1",
    "href": "aulas/revisao_alg_linear/rev_alg_linear.html#teorema-da-decomposição-espectral-1",
    "title": "Revisão de Álgebra Matricial",
    "section": "Teorema da decomposição espectral",
    "text": "Teorema da decomposição espectral\nem que \\(\\mathbf{\\Lambda}\\) é a matriz diagonal dos autovalores:\n\\[\\mathbf{\\Lambda} = \\left[ \\begin{array}{cccc} \\lambda_1 & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\lambda_p \\end{array} \\right]\\]\ne \\(\\mathbf{C}\\) é a matriz ortogonal com os autovetores normalizados de \\(\\mathbf{A}\\) nas colunas:\n\\[\\mathbf{C} = \\left[\\begin{array}{rrrr} \\mathbf{e}_1 & \\mathbf{e}_2 & \\cdots & \\mathbf{e}_p \\end{array} \\right]\\]"
  },
  {
    "objectID": "analise_dados/descr_vis_iris.html",
    "href": "analise_dados/descr_vis_iris.html",
    "title": "Descrição e visualização de dados multivariados utilizando a base Iris",
    "section": "",
    "text": "A base iris (FISHER (1936)) é um dos conjuntos de dados mais clássicos e didáticos da estatística. Ela contém 150 observações (flores) de três espécies, setosa, versicolor e virginica, medidas em quatro variáveis contínuas:\n\nSepal.Length (cm)\n\nSepal.Width (cm)\n\nPetal.Length (cm)\n\nPetal.Width (cm)\n\n\n\n\n\n\nNosso objetivo é revisar conceitos fundamentais de Estatística Multivariada:\n\n\nVetor de médias\n\nMatrizes de covariância e correlação\n\nVariância total e generalizada\n\nMatrizes de distância (euclidiana e de Mahalanobis)\n\nVisualizações (pares, correlograma, heatmaps)"
  },
  {
    "objectID": "analise_dados/descr_vis_iris.html#distância-euclidiana",
    "href": "analise_dados/descr_vis_iris.html#distância-euclidiana",
    "title": "Descrição e visualização de dados multivariados utilizando a base Iris",
    "section": "6.1 Distância Euclidiana",
    "text": "6.1 Distância Euclidiana\n\nD_euclid &lt;- euclidean_dist(data = X)\nas.matrix(D_euclid)[1:6, 1:6]\n\n          1         2        3         4         5         6\n1 0.0000000 0.5385165 0.509902 0.6480741 0.1414214 0.6164414\n2 0.5385165 0.0000000 0.300000 0.3316625 0.6082763 1.0908712\n3 0.5099020 0.3000000 0.000000 0.2449490 0.5099020 1.0862780\n4 0.6480741 0.3316625 0.244949 0.0000000 0.6480741 1.1661904\n5 0.1414214 0.6082763 0.509902 0.6480741 0.0000000 0.6164414\n6 0.6164414 1.0908712 1.086278 1.1661904 0.6164414 0.0000000\n\n\n\n📏 Distância Euclidiana representa a distância geométrica entre dois pontos."
  },
  {
    "objectID": "analise_dados/descr_vis_iris.html#distância-de-karl-pearson",
    "href": "analise_dados/descr_vis_iris.html#distância-de-karl-pearson",
    "title": "Descrição e visualização de dados multivariados utilizando a base Iris",
    "section": "6.2 Distância de Karl Pearson",
    "text": "6.2 Distância de Karl Pearson\n\nD_karlP &lt;- scaled_euclidean_dist(data = X)\nas.matrix(D_karlP)[1:6, 1:6]\n\n          1         2         3         4         5         6\n1 0.0000000 1.1722914 0.8427840 1.0999999 0.2592702 1.0349769\n2 1.1722914 0.0000000 0.5216255 0.4325508 1.3818560 2.1739229\n3 0.8427840 0.5216255 0.0000000 0.2829432 0.9882608 1.8477070\n4 1.0999999 0.4325508 0.2829432 0.0000000 1.2459861 2.0937597\n5 0.2592702 1.3818560 0.9882608 1.2459861 0.0000000 0.8971079\n6 1.0349769 2.1739229 1.8477070 2.0937597 0.8971079 0.0000000\n\n\n\n📏 Distância de Karl Pearson leva em conta as diferenças de escala."
  },
  {
    "objectID": "analise_dados/descr_vis_iris.html#distância-de-mahalanobis",
    "href": "analise_dados/descr_vis_iris.html#distância-de-mahalanobis",
    "title": "Descrição e visualização de dados multivariados utilizando a base Iris",
    "section": "6.3 Distância de Mahalanobis",
    "text": "6.3 Distância de Mahalanobis\n\nD_mahal &lt;- mahalanobis_dist(data = X)\nas.matrix(D_mahal)[1:6, 1:6]\n\n          1         2         3         4         5         6\n1 0.0000000 1.3544572 0.9687298 1.4057253 0.5899110 1.1382566\n2 1.3544572 0.0000000 0.9697905 1.4527546 1.8106890 2.4484066\n3 0.9687298 0.9697905 0.0000000 0.7170009 1.1253440 1.9227214\n4 1.4057253 1.4527546 0.7170009 0.0000000 1.3293220 2.2425573\n5 0.5899110 1.8106890 1.1253440 1.3293220 0.0000000 0.9446158\n6 1.1382566 2.4484066 1.9227214 2.2425573 0.9446158 0.0000000\n\n\n\n📏 Distância de Mahalanobis leva em conta correlações entre variáveis e diferenças de escala."
  },
  {
    "objectID": "analise_dados/descr_vis_iris.html#gráfico-de-pares-por-espécie",
    "href": "analise_dados/descr_vis_iris.html#gráfico-de-pares-por-espécie",
    "title": "Descrição e visualização de dados multivariados utilizando a base Iris",
    "section": "7.1 Gráfico de pares por espécie",
    "text": "7.1 Gráfico de pares por espécie\n\nGGally::ggpairs(bind_cols(X, Species = y),\n                columns = 1:4, aes(color = Species, alpha = 0.8)) + theme_bw()\n\n\n\n\n\n\n\n\n\n💡 Interpretação: O “pairs plot” da iris mostra que setosa tem pétalas muito pequenas e sépalas mais largas, enquanto versicolor e virginica apresentam pétalas maiores (com alguma sobreposição), e em Sepal.Width a espécie setosa desloca-se à direita, versicolor à esquerda e virginica fica intermediária. As correlações globais destacam Petal.Length vs. Petal.Width como fortíssima (≈0,96) e Sepal.Length bem associado às pétalas; já Sepal.Width aparece negativamente correlacionado no agregado. Contudo, por espécie as relações com Sepal.Width tendem a ser positivas, e os dispersogramas envolvendo medidas de pétala exibem a melhor separação entre espécies. Em síntese, as pétalas dominam a estrutura e a discriminação, com Sepal.Width fornecendo informação complementar."
  },
  {
    "objectID": "analise_dados/descr_vis_iris.html#correlograma",
    "href": "analise_dados/descr_vis_iris.html#correlograma",
    "title": "Descrição e visualização de dados multivariados utilizando a base Iris",
    "section": "7.2 Correlograma",
    "text": "7.2 Correlograma\n\nggcorrplot(R, hc.order = TRUE, type = \"lower\",\n           lab = TRUE, tl.cex = 10,\n           title = \"Matriz de Correlações - iris\")\n\n\n\n\n\n\n\n\n\n💡 Interpretação: O mapa de correlações da iris evidencia três padrões centrais: (1) forte associação positiva entre as medidas de pétala: Petal.Length vs. Petal.Width ≈ 0,96 e também de Sepal.Length com as pétalas (≈ 0,87 e 0,82), indicando redundância informacional e provável multicolinearidade; (2) Sepal.Width apresenta correlações negativas com as demais variáveis (≈ −0,12 com Sepal.Length, −0,43 com Petal.Length e −0,37 com Petal.Width), sugerindo um eixo de variação em sentido oposto ao das pétalas; e (3) como consequência, em tarefas de PCA ou classificação, as pétalas tendem a dominar a separação entre espécies, enquanto Sepal.Width adiciona sinal complementar."
  },
  {
    "objectID": "analise_dados/descr_vis_iris.html#heatmap-das-correlações",
    "href": "analise_dados/descr_vis_iris.html#heatmap-das-correlações",
    "title": "Descrição e visualização de dados multivariados utilizando a base Iris",
    "section": "7.3 Heatmap das correlações",
    "text": "7.3 Heatmap das correlações\n\npheatmap(R, cluster_rows = TRUE, cluster_cols = TRUE,\n         main = \"Heatmap da Matriz de Correlações (iris)\")\n\n\n\n\n\n\n\n\n\n💡 Interpretação: O heatmap das correlações da iris confirma dois blocos de variáveis: (i) Petal.Length e Petal.Width fortemente positivas entre si (vermelho intenso) e também bem alinhadas com Sepal.Length (vermelho), formando um grupo altamente correlacionado que indica redundância e tende a dominar a variação; (ii) Sepal.Width aparece em azul frente às demais, mostrando correlação negativa moderada, o que a isola no dendrograma e sugere um eixo complementar de informação. Em termos práticos, as pétalas são as melhores para discriminar espécies (e podem sofrer multicolinearidade), enquanto Sepal.Width acrescenta sinal em direção oposta."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sobre",
    "section": "",
    "text": "Disciplina oferecida no semestre 2025/2"
  },
  {
    "objectID": "analises.html",
    "href": "analises.html",
    "title": "Análise de dados",
    "section": "",
    "text": "Descrição e Visualização da base de dados Iris"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#análise-estatística-multivariada",
    "href": "aulas/intro_est_multi/intro_est_multi.html#análise-estatística-multivariada",
    "title": "Introdução à Estatística Multivariada",
    "section": "Análise Estatística Multivariada",
    "text": "Análise Estatística Multivariada\n\nA capacidade de coleta e armazenamento de dados tem aumentado significativamente ao longo do tempo, tornando cada vez maior a quantidade de informações (variáveis) que se dispõe sobre cada indivíduo.\n\n\n\nAssim, tem-se a necessidade de transformar essa grande quantidade de dados em conhecimento, que possa fundamentar a compreensão de diferentes fenômenos e subsidiar tomadas de decisões.\n\n\n\n\nNesse contexto, técnicas de análise de dados que permitam explorar e compreender as relações existentes entre múltiplas variáveis tornam-se essenciais na análise"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#definição-de-análise-multivariada",
    "href": "aulas/intro_est_multi/intro_est_multi.html#definição-de-análise-multivariada",
    "title": "Introdução à Estatística Multivariada",
    "section": "Definição de Análise Multivariada",
    "text": "Definição de Análise Multivariada\n\n\n\n\nAnálise Multivariada\n\n\nA análise multivariada contempla um conjunto de métodos estatísticos utilizados na análise conjunta de múltiplas variáveis avaliadas nos indivíduos sob estudo."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#principais-objetivos-da-análise-multivariada",
    "href": "aulas/intro_est_multi/intro_est_multi.html#principais-objetivos-da-análise-multivariada",
    "title": "Introdução à Estatística Multivariada",
    "section": "Principais objetivos da Análise Multivariada",
    "text": "Principais objetivos da Análise Multivariada\nMétodos de Análise Multivariada podem ser aplicados para diversas finalidades, dentre as quais podemos destacar:\n\n\nRedução ou simplificação de dados;\n\n\n\n\nClassificação;\n\n\n\n\nAgrupamento;\n\n\n\n\ndentre outros…"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#exemplos-de-aplicações",
    "href": "aulas/intro_est_multi/intro_est_multi.html#exemplos-de-aplicações",
    "title": "Introdução à Estatística Multivariada",
    "section": "Exemplos de aplicações",
    "text": "Exemplos de aplicações\n\n\nRedução ou simplificação de dados\n\nDados referentes aos sintomas de determinada doença e limitações relatadas pelos pacientes, decorrentes da doença ou do tratamento, podem ser usados para a elaboração de um índice de qualidade de vida;\nDiferentes indicadores demográficos e sócio-econômicos podem ser usados para a elaboração de um gráfico, em duas dimensões, em que proximidade entre pontos (representando bairros de um município, por exemplo) configure similaridade entre eles."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#exemplos-de-aplicações-1",
    "href": "aulas/intro_est_multi/intro_est_multi.html#exemplos-de-aplicações-1",
    "title": "Introdução à Estatística Multivariada",
    "section": "Exemplos de aplicações",
    "text": "Exemplos de aplicações\n\n\nAgrupamento\n\nDados cadastrais podem ser utilizados com o objetivo de definir grupos de clientes de uma loja de departamentos similares quanto a informações disponíveis em suas fichas;\nVariáveis referentes à contabilidade de indústrias no último ano (gastos com mão de obra, investimento em matéria prima, produção…) podem ser usadas para agrupá-las em clusters de indústrias com características contábeis similares."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#exemplos-de-aplicações-2",
    "href": "aulas/intro_est_multi/intro_est_multi.html#exemplos-de-aplicações-2",
    "title": "Introdução à Estatística Multivariada",
    "section": "Exemplos de aplicações",
    "text": "Exemplos de aplicações\n\n\nClassificação\n\nResultados de diversas variáveis psicológicas e comportamentais podem ser usados para criar uma regra de discriminação de usuários de drogas que reincidem, após período de abstinência, daqueles que não reincidem;\nVariáveis referentes à anatomia de moscas (comprimento de asas, peso, coloração…) podem ser usadas para discriminar moscas em uma de quatro espécies distintas segundo os valores apresentados para tais variáveis."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#disposição-dos-dados-em-uma-análise-multivariada",
    "href": "aulas/intro_est_multi/intro_est_multi.html#disposição-dos-dados-em-uma-análise-multivariada",
    "title": "Introdução à Estatística Multivariada",
    "section": "Disposição dos dados em uma análise multivariada",
    "text": "Disposição dos dados em uma análise multivariada\n\nNuma análise multivariada, dispõe-se, em geral, de uma amostra de \\(n\\) indivíduos, com \\(p &gt; 1\\) variáveis avaliadas em cada um deles.\n\n\n\nO uso de técnicas multivariadas permite analisar simultaneamente as \\(p\\) variáveis.\n\n\n\n\nOs métodos multivariados consistem, muitas vezes, de generalizações de procedimentos univariados utilizados para fins semelhantes."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#disposição-dos-dados-em-uma-análise-multivariada-1",
    "href": "aulas/intro_est_multi/intro_est_multi.html#disposição-dos-dados-em-uma-análise-multivariada-1",
    "title": "Introdução à Estatística Multivariada",
    "section": "Disposição dos dados em uma análise multivariada",
    "text": "Disposição dos dados em uma análise multivariada\n\nVamos denotar por \\(x_{ij}\\) o valor da variável \\(j\\) verificado no indivíduo \\(i\\), \\(i = 1, 2, \\cdots, n\\); \\(j = 1, 2, \\cdots, p\\).\n\n\n\nA disposição usual dos dados é na forma convencional, entrando com os indivíduos nas linhas e as variáveis nas colunas.\n\n\n\n\n\n\n\n\n\n\nVar 1\n\n\nVar 2\n\n\n\\(\\cdots\\)\n\n\nVar j\n\n\n\\(\\cdots\\)\n\n\nVar p\n\n\n\n\n\n\nInd 1\n\n\n\\(x_{11}\\)\n\n\n\\(x_{12}\\)\n\n\n\\(\\cdots\\)\n\n\n\\(x_{1j}\\)\n\n\n\\(\\cdots\\)\n\n\n\\(x_{1p}\\)\n\n\n\n\nInd 2\n\n\n\\(x_{21}\\)\n\n\n\\(x_{22}\\)\n\n\n\\(\\cdots\\)\n\n\n\\(x_{2j}\\)\n\n\n\\(\\cdots\\)\n\n\n\\(x_{2p}\\)\n\n\n\n\n\\(\\vdots\\)\n\n\n\\(\\vdots\\)\n\n\n\\(\\vdots\\)\n\n\n\\(\\vdots\\)\n\n\n\\(\\vdots\\)\n\n\n\\(\\vdots\\)\n\n\n\\(\\vdots\\)\n\n\n\n\nInd i\n\n\n\\(x_{i1}\\)\n\n\n\\(x_{i2}\\)\n\n\n\\(\\cdots\\)\n\n\n\\(x_{ij}\\)\n\n\n\\(\\cdots\\)\n\n\n\\(x_{ip}\\)\n\n\n\n\n\\(\\vdots\\)\n\n\n\\(\\vdots\\)\n\n\n\\(\\vdots\\)\n\n\n\\(\\cdots\\)\n\n\n\\(\\vdots\\)\n\n\n\\(\\cdots\\)\n\n\n\\(\\vdots\\)\n\n\n\n\nInd n\n\n\n\\(x_{n1}\\)\n\n\n\\(x_{n2}\\)\n\n\n\\(\\cdots\\)\n\n\n\\(x_{nj}\\)\n\n\n\\(\\cdots\\)\n\n\n\\(x_{np}\\)"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#representação-matricial-dos-dados",
    "href": "aulas/intro_est_multi/intro_est_multi.html#representação-matricial-dos-dados",
    "title": "Introdução à Estatística Multivariada",
    "section": "Representação matricial dos dados",
    "text": "Representação matricial dos dados\n\nPara fins de apresentação e desenvolvimento da teoria, a representação matricial dos dados é necessária.\n\n\n\nA matriz dos dados tem dimensão \\(n \\times p\\), apresentando nas linhas os \\(n\\) indivíduos e nas colunas as \\(p\\) variáveis.\n\n\n\n\\[ \\mathbf{X}_{n \\times p} = \\left[ \\begin{array}{cccc} x_{11} & x_{12} & \\cdots & x_{1p} \\\\ x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n1} & x_{n2} & \\cdots & x_{np} \\end{array} \\right] \\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios",
    "href": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios",
    "title": "Introdução à Estatística Multivariada",
    "section": "Vetores aleatórios",
    "text": "Vetores aleatórios\n\nPara o tratamento probabilístico de dados multivariados, vamos relembrar a definição de vetor aleatório.\n\n\n\nUm vetor aleatório de dimensão \\(p\\) é um vetor em que cada um de seus \\(p\\) componentes é uma variável aleatória.\n\n\n\n\nVamos denotar um vetor aleatório \\(\\mathbf{x}\\) por \\(\\mathbf{x} = (X_1, X_2, \\cdots, X_p)^t\\). A função distribuição de probabilidade conjunta de \\(\\mathbf{x}\\) é definida como:\n\n\\[F(\\mathbf{x}) = P(X_1 \\leqslant x_1, X_2 \\leqslant x_2, \\cdots, X_p \\leqslant x_p)\\]\npara \\(\\mathbf{x} \\in \\mathbb{R}^p\\)"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-1",
    "href": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-1",
    "title": "Introdução à Estatística Multivariada",
    "section": "Vetores aleatórios",
    "text": "Vetores aleatórios\n\nSe \\(\\mathbf{x}\\) for contínua, então a função densidade de probabilidade \\(f(\\mathbf{x})\\) fica definida por:\n\n\\[f(\\mathbf{x}) = \\dfrac{\\partial F(\\mathbf{x})}{\\partial x_1 \\partial x_2 \\cdots \\partial x_p},\\]\nsatisfazendo\n\\[\\int \\limits_{-\\infty}^{\\infty} \\int \\limits_{-\\infty}^{\\infty} \\cdots \\int \\limits_{-\\infty}^{\\infty} f(x_1, x_2, \\cdots, x_p)dx_1 dx_2 \\cdots dx_p = 1\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-2",
    "href": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-2",
    "title": "Introdução à Estatística Multivariada",
    "section": "Vetores aleatórios",
    "text": "Vetores aleatórios\ne,\n\\[f(x_1, x_2, \\cdots, x_p) \\geqslant 0\\]\npara qualquer conjunto de valores \\(x_1, x_2, \\cdots, x_p\\)."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-3",
    "href": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-3",
    "title": "Introdução à Estatística Multivariada",
    "section": "Vetores aleatórios",
    "text": "Vetores aleatórios\n\nCada elemento de \\(\\mathbf{x} = (X_1, X_2, \\cdots, X_p)^t\\) é uma variável aleatória \\(X_i\\), \\(i = 1, 2, \\cdots, p\\), cuja distribuição (denominada distribuição marginal) fica determinada pela função densidade de probabilidade \\(f(x_i)\\):\n\n\\[f(x_i) = \\int \\limits_{x_1} \\int \\limits_{x_2} \\cdots \\int \\limits_{x_p} f(\\mathbf{x})dx_1 dx_2 \\cdots dx_p, \\,\\,\\,\\, j \\neq i\\]\n\n\nSe \\(X_1, X_2, \\cdots, X_p\\) forem independentes, então:\n\n\\[f(x_1, x_2, \\cdots, x_p) = \\prod \\limits_{i=1}^p f(x_i) = f(x_1) f(x_2) \\cdots f(x_p)\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-4",
    "href": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-4",
    "title": "Introdução à Estatística Multivariada",
    "section": "Vetores aleatórios",
    "text": "Vetores aleatórios\n\nA média e a variância de \\(X_i\\), ficam definidas como:\n\n\\[\\mu_i = E(X_i) = \\int \\limits_{-\\infty}^{\\infty} x_i f(x_i)dx_i\\]\n\\[\\sigma_{ii} = Var(X_i) = \\int \\limits_{-\\infty}^{\\infty} (x_i - \\mu_i)^2 f(x_i)dx_i\\]\npara \\(i = 1, 2, \\cdots, p\\)."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-5",
    "href": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-5",
    "title": "Introdução à Estatística Multivariada",
    "section": "Vetores aleatórios",
    "text": "Vetores aleatórios\n\nPara duas variáveis aleatórias \\(X_i\\) e \\(X_j\\), a covariância é um parâmetro da distribuição conjunta bivariada que mede a associação linear entre elas:\n\n\\[\\sigma_{ij} = Cov(X_i, X_j) = \\int \\limits_{-\\infty}^{\\infty} \\int \\limits_{-\\infty}^{\\infty} (x_i - \\mu_i)(x_j - \\mu_j) f(x_i, x_j)dx_idx_j\\]\n\n\nSe \\(X_i\\) e \\(X_j\\) forem independentes, então \\(Cov(X_i, X_j ) = 0\\)."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-6",
    "href": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-6",
    "title": "Introdução à Estatística Multivariada",
    "section": "Vetores aleatórios",
    "text": "Vetores aleatórios\n\nO coeficiente de correlação de \\(X_i\\) e \\(X_j\\) é definido como:\n\n\\[\\rho_{ij} = \\dfrac{\\sigma_{ij}}{\\sigma_i \\sigma_j}\\]\nsendo uma medida de associação linear adimensional, tal que \\(-1 \\leqslant \\rho \\leqslant 1\\).\n\n\nTodos os resultados equivalentes para o caso discreto são obtidos substituindo adequadamente as integrais por somas."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-7",
    "href": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-7",
    "title": "Introdução à Estatística Multivariada",
    "section": "Vetores aleatórios",
    "text": "Vetores aleatórios\n\nA esperança matemática de um vetor aleatório \\(\\mathbf{x} = (X_1, X_2, \\cdots, X_p)^t\\) é definida pelo vetor de mesma dimensão em que cada elemento corresponde à esperança matemática da respectiva variável.\n\n\\[ \\mathbf{\\mu} = E(\\mathbf{x}) =  E \\left( \\left[ \\begin{array}{c} X_{1}  \\\\ X_{2} \\\\ \\vdots\n\\\\ X_{p} \\end{array} \\right] \\right)  = \\left[ \\begin{array}{c} E(X_{1})  \\\\ E(X_{2}) \\\\ \\vdots\n\\\\ E(X_{p}) \\end{array} \\right]  = \\left[ \\begin{array}{c} \\mu_1  \\\\ \\mu_2 \\\\ \\vdots\n\\\\ \\mu_p \\end{array} \\right] \\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-8",
    "href": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-8",
    "title": "Introdução à Estatística Multivariada",
    "section": "Vetores aleatórios",
    "text": "Vetores aleatórios\n\nA matriz de variâncias e covariâncias de um vetor aleatório \\(\\mathbf{x} = (X_1, X_2, \\cdots, X_p)^t\\) é definida pela matriz \\(\\mathbf{\\Sigma}\\) dada por:\n\n\\[\\begin{eqnarray*} \\mathbf{\\Sigma} &=& E \\left[(\\mathbf{x} - \\mathbf{\\mu})(\\mathbf{x} -\n\\mathbf{\\mu})^t \\right] \\\\ &=&\n      \\left[ \\begin{array}{ccc} E(X_1 - \\mu_1)^2  & \\cdots & E(X_1 - \\mu_1)( X_p - \\mu_p) \\\\ E(X_2 - \\mu_2)(X_1 - \\mu_1)  & \\cdots & E(X_2 - \\mu_2)( X_p - \\mu_p)\\\\ \\vdots & \\ddots & \\vdots \\\\ E( X_p - \\mu_p)(X_1 - \\mu_1) & \\cdots & E( X_p - \\mu_p)^2 \\end{array} \\right]\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-9",
    "href": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-9",
    "title": "Introdução à Estatística Multivariada",
    "section": "Vetores aleatórios",
    "text": "Vetores aleatórios\n\nDe maneira semelhante, a matriz de correlações do vetor aleatório \\(\\mathbf{x}\\) fica dada por:\n\n\\[ \\mathbf{P} = \\left[ \\begin{array}{cccc} 1 & \\rho_{12} & \\cdots & \\rho_{1p} \\\\ \\rho_{21} & 1 & \\cdots & \\rho_{2p}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\rho_{p1} & \\rho_{p2} & \\cdots & 1\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-10",
    "href": "aulas/intro_est_multi/intro_est_multi.html#vetores-aleatórios-10",
    "title": "Introdução à Estatística Multivariada",
    "section": "Vetores aleatórios",
    "text": "Vetores aleatórios\n\nA matriz de correlações pode ser determinada facilmente a partir da matriz de covariâncias por:\n\n\\[\\mathbf{P} = \\left(\\mathbf{V}^\\frac{1}{2}\\right)^{-1} \\mathbf{\\Sigma} \\left(\\mathbf{V}^\\frac{1}{2}\\right)^{-1}\\]\nsendo \\(\\mathbf{V}^\\frac{1}{2}\\) a matriz diagonal com elementos \\(\\sqrt{\\sigma_{11}}, \\sqrt{\\sigma_{22}}, \\cdots, \\sqrt{\\sigma_{pp}}\\)."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#propriedades-da-média-e-da-variância-de-vetores-aleatórios",
    "href": "aulas/intro_est_multi/intro_est_multi.html#propriedades-da-média-e-da-variância-de-vetores-aleatórios",
    "title": "Introdução à Estatística Multivariada",
    "section": "Propriedades da média e da variância de vetores aleatórios",
    "text": "Propriedades da média e da variância de vetores aleatórios\n\nSejam \\(\\mathbf{A}\\) e \\(\\mathbf{B}\\) matrizes matrizes e \\(c\\) um vetor de constantes (todos com dimensões compatíveis às operações apresentadas). Sejam \\(\\mathbf{x}\\) e \\(\\mathbf{y}\\) vetores aleatórios.\n\n\\(E(\\mathbf{x} + \\mathbf{y}) = E(\\mathbf{x}) + E(\\mathbf{y})\\);\n\\(E(\\mathbf{AxB}) = \\mathbf{A} E(\\mathbf{x}) \\mathbf{B}\\);\n\\(E(\\mathbf{Ax} + c) = \\mathbf{A} E(\\mathbf{x}) + c\\);\n\\(Cov(\\mathbf{x}) = E(\\mathbf{x} \\mathbf{x}^t) - \\mathbf{\\mu}_x \\mathbf{\\mu}_x^t\\);\n\\(Cov(c^t \\mathbf{x}) = c^t Cov(\\mathbf{x})c\\);\n\\(Cov(\\mathbf{A}^t \\mathbf{x} + c) = \\mathbf{A} Cov(\\mathbf{x})\\mathbf{A}^t\\)."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados",
    "href": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados",
    "title": "Introdução à Estatística Multivariada",
    "section": "Descrição de dados multivariados",
    "text": "Descrição de dados multivariados\n\nNa prática, todas as matrizes teóricas ( \\(\\mathbf{\\mu}\\), \\(\\mathbf{\\Sigma}\\), \\(\\mathbf{P}\\) e \\(\\mathbf{V}\\) ) são desconhecidas e precisam ser estimadas através de dados amostrais.\n\n\n\nSeja então, uma amostra aleatória multivariada de tamanho \\(n\\).\n\n\n\n\nPodemos calcular a média amostral separadamente para cada variável:\n\n\\[\\bar{x}_j = \\dfrac{1}{n} \\sum \\limits_{i=1}^n x_{ij}, \\,\\,\\,\\,\\,\\, j = 1, 2,  \\cdots, p\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-1",
    "href": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-1",
    "title": "Introdução à Estatística Multivariada",
    "section": "Descrição de dados multivariados",
    "text": "Descrição de dados multivariados\n\nO vetor de médias amostrais fica definido por:\n\n\\[\\bar{\\mathbf{x}} = \\left[ \\begin{array}{c} \\bar{x}_1  \\\\ \\bar{x}_2\\\\ \\vdots  \\\\ \\bar{x}_p \\end{array} \\right]\\]\n\n\nMatricialmente, temos \\(\\bar{\\mathbf{x}} = \\dfrac{1}{n} \\mathbf{X}^t \\mathbf{j}\\), sendo \\(\\mathbf{X}\\) a matriz de dados e \\(\\mathbf{j}\\) o vetor de 1’s de dimensão \\(n\\)."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-2",
    "href": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-2",
    "title": "Introdução à Estatística Multivariada",
    "section": "Descrição de dados multivariados",
    "text": "Descrição de dados multivariados\n\nA variância amostral para a \\(j\\)-ésima variável e a covariância amostral para as variáveis \\(X_j\\) e \\(X_k\\) são definidas, respectivamente, por:\n\n\\[ s_{jj} = \\frac{\\displaystyle\\sum _{i=1}^n (x_{ij}-\\bar{x_j})^2}{n-1} \\]\n\\[ s_{jk} = \\frac{\\displaystyle\\sum _{i=1}^n (x_{ij}-\\bar{x_j})(x_{ik}-\\bar{x_k})}{n-1}, \\hspace{0.5cm} j \\neq k\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-3",
    "href": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-3",
    "title": "Introdução à Estatística Multivariada",
    "section": "Descrição de dados multivariados",
    "text": "Descrição de dados multivariados\n\nA matriz de covariâncias amostral (simétrica) fica definida por:\n\n\\[\\mathbf{S}_{p \\times p} = \\left[ \\begin{array}{cccc} s_{11} & s_{12} & \\cdots & s_{1p} \\\\ s_{21} & s_{22} & \\cdots & s_{2p}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ s_{p1} & s_{p2} & \\cdots & s_{pp} \\end{array} \\right]  \\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-4",
    "href": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-4",
    "title": "Introdução à Estatística Multivariada",
    "section": "Descrição de dados multivariados",
    "text": "Descrição de dados multivariados\n\nPodemos expressar a matriz de covariâncias em termos dos vetores observados:\n\n\\[\\mathbf{S} = \\dfrac{1}{n-1} \\sum \\limits_{i=1}^n (\\mathbf{x}_i - \\bar{\\mathbf{x}})(\\mathbf{x}_i - \\bar{\\mathbf{x}})^t = \\dfrac{1}{n-1} \\left( \\sum \\limits_{i=1}^n \\mathbf{x}_i \\mathbf{x}_i^t - n \\bar{\\mathbf{x}}\\bar{\\mathbf{x}}^t \\right) \\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-5",
    "href": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-5",
    "title": "Introdução à Estatística Multivariada",
    "section": "Descrição de dados multivariados",
    "text": "Descrição de dados multivariados\n\nO coeficiente de correlação amostral entre as variáveis \\(X_j\\) e \\(X_k\\) é dado por:\n\n\\[r_{jk} = \\dfrac{s_{jk}}{\\sqrt{s_{jj}} \\sqrt{s_{kk}}} = \\dfrac{\\sum \\limits_{i=1}^n (x_{ij} - \\bar{x}_j)(x_{ik} - \\bar{x}_k)}{\\sqrt{\\sum \\limits_{i=1}^n (x_{ij} - \\bar{x}_j)^2} \\sqrt{\\sum \\limits_{i=1}^n (x_{ik} - \\bar{x}_k)^2}}\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-6",
    "href": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-6",
    "title": "Introdução à Estatística Multivariada",
    "section": "Descrição de dados multivariados",
    "text": "Descrição de dados multivariados\n\nA matriz de correlações amostrais (simétrica) é determinada pelos coeficientes calculados para cada par de variáveis.\n\n\\[\\mathbf{R}_{p \\times p} = \\left[ \\begin{array}{cccc} 1 & r_{12} & \\cdots & r_{1p} \\\\\nr_{21} & 1 & \\cdots & r_{2p}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nr_{p1} & r_{p2} & \\cdots & 1\n\\end{array} \\right]\n\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-7",
    "href": "aulas/intro_est_multi/intro_est_multi.html#descrição-de-dados-multivariados-7",
    "title": "Introdução à Estatística Multivariada",
    "section": "Descrição de dados multivariados",
    "text": "Descrição de dados multivariados\n\nComo complemento às medidas descritivas apresentadas, o uso de gráficos permite extrair informações importantes dos dados. Alguns gráficos (e recursos adicionais):\n\nHistograma;\nBoxplot;\nDiagrama de dispersão;\nCorrelograma;\nMatrizes de gráficos;\nFaces de Chernoff;\nGráficos tridimensionais…"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#variância-generalizada-e-variância-total",
    "href": "aulas/intro_est_multi/intro_est_multi.html#variância-generalizada-e-variância-total",
    "title": "Introdução à Estatística Multivariada",
    "section": "Variância generalizada e variância total",
    "text": "Variância generalizada e variância total\n\nEm algumas situações, é de interesse exprimir a varição presente nos dados multivariados em um único valor.\n\n\n\nUma das alternativas para isso é a variância generalizada, que é definida como o determinante da matriz de covariâncias amostral.\n\n\\[\\text{Variância generalizada} = |\\mathbf{S}|\\]\n\n\n\nNaturalmente, por resumir toda a variação em um único valor, parte da informação referente à variação dos dados se perde nesse resumo, originando possíveis distorções."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#variância-generalizada-e-variância-total-1",
    "href": "aulas/intro_est_multi/intro_est_multi.html#variância-generalizada-e-variância-total-1",
    "title": "Introdução à Estatística Multivariada",
    "section": "Variância generalizada e variância total",
    "text": "Variância generalizada e variância total\n\nA variância generalizada pode ser definida ainda a partir das variáveis originais padronizadas \\((x_{ij} - \\bar{x}_j)/s_{jj}\\), contornando problema de diferentes escalas das variáveis. Como resultado da padronização, temos:\n\n\\[\\text{Variância generalizada das variáveis padronizadas} = |\\mathbf{R}|\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#variância-generalizada-e-variância-total-2",
    "href": "aulas/intro_est_multi/intro_est_multi.html#variância-generalizada-e-variância-total-2",
    "title": "Introdução à Estatística Multivariada",
    "section": "Variância generalizada e variância total",
    "text": "Variância generalizada e variância total\n\nAlgumas propriedades da variância generalizada:\n\nQuanto maior a variância de uma variável, maior sua contribuição para a variância generalizada;\nQuanto maior a correlação entre duas variáveis, menor a variância generalizada;\nCaso duas ou mais variáveis sejam perfeitamente correlacionadas a variância generalizada atingirá seu mínimo valor (zero);\nA variância generalizada será máxima caso as variáveis tenham correlação nula."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#variância-generalizada-e-variância-total-3",
    "href": "aulas/intro_est_multi/intro_est_multi.html#variância-generalizada-e-variância-total-3",
    "title": "Introdução à Estatística Multivariada",
    "section": "Variância generalizada e variância total",
    "text": "Variância generalizada e variância total\n\nUma das limitações da variância generalizada é o fato de diferentes estruturas de correlação produzirem, algumas vezes, igual variância generalizada.\n\n\n\nExemplo: Calcular, para cada matriz de covariâncias, a variância generalizada e a correlação entre as variáveis.\n\n\\[\\mathbf{S}_1 = \\left[ \\begin{array}{rr} 10 & 8  \\\\  8 & 10 \\end{array} \\right], \\,\\,\\,\\,\\,\\,\\,\\, \\mathbf{S}_2 = \\left[ \\begin{array}{rr} 10 & -8  \\\\  -8 & 10 \\end{array} \\right], \\,\\,\\,\\,\\,\\,\\,\\, \\mathbf{S}_3 = \\left[ \\begin{array}{rr} 9 & 0  \\\\  0 & 4 \\end{array} \\right]\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#variância-generalizada-e-variância-total-4",
    "href": "aulas/intro_est_multi/intro_est_multi.html#variância-generalizada-e-variância-total-4",
    "title": "Introdução à Estatística Multivariada",
    "section": "Variância generalizada e variância total",
    "text": "Variância generalizada e variância total\n\nOutra medida usada para resumir a variação total em um único valor é a variância total, definida por:\n\n\\[\\text{Variância total} = \\text{tr}(\\mathbf{S}) = s_{11} + s_{22} + \\cdots + s_{pp}\\]\n\n\nPor se basear apenas na diagonal de \\(\\mathbf{S}\\), a variância total claramente ignora a estrutura de correlação dos dados."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#distâncias",
    "href": "aulas/intro_est_multi/intro_est_multi.html#distâncias",
    "title": "Introdução à Estatística Multivariada",
    "section": "Distâncias",
    "text": "Distâncias\n\nBoa parte das técnicas multivariadas baseiam-se no conceito de distância.\n\n\n\nUsamos distâncias para medir quão semelhantes são dois indivíduos com relação aos valores observados para \\(p\\) variáveis.\n\n\n\n\nSejam dois vetores \\(\\mathbf{x}\\) e \\(\\mathbf{y} \\in \\mathbb{R}^p\\) e uma matriz \\(\\mathbf{\\Psi}\\), positiva definida. Então, a expressão geral para a distância quadrática entre os vetores \\(\\mathbf{x}\\) e \\(\\mathbf{y}\\) é dada por:\n\n\\[d^2(\\mathbf{x}, \\mathbf{y}) = \\left| \\mathbf{x} - \\mathbf{y} \\right|^2_{\\mathbf{\\Psi}} = (\\mathbf{x} - \\mathbf{y})^t{\\mathbf{\\Psi}} (\\mathbf{x} - \\mathbf{y})\\]\nem que a matriz \\(\\mathbf{\\Psi}\\), positiva definida, é chamada de métrica."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#distâncias-1",
    "href": "aulas/intro_est_multi/intro_est_multi.html#distâncias-1",
    "title": "Introdução à Estatística Multivariada",
    "section": "Distâncias",
    "text": "Distâncias\n\nConsideremos agora \\(\\mathbf{z}\\), um terceiro vetor de \\(\\mathbb{R}^p\\). Então as seguintes propriedades são válidas:\n\n\\(d(\\mathbf{x}, \\mathbf{y}) = d(\\mathbf{y}, \\mathbf{x})\\)\n\\(d(\\mathbf{x}, \\mathbf{y}) &gt; 0 \\hspace{0.5cm} \\forall \\hspace{0.2cm} \\mathbf{x} \\neq \\mathbf{y}\\)\n\\(d(\\mathbf{x}, \\mathbf{y}) = 0  \\hspace{0.5cm} \\text{se, e somente se,} \\hspace{0.5cm} \\mathbf{x} = \\mathbf{y}\\)\n\n\\(d(\\mathbf{x}, \\mathbf{y}) \\leq d(\\mathbf{x}, \\mathbf{z}) + d(\\mathbf{y}, \\mathbf{z}) \\,\\,\\,\\,\\,\\, \\text{(desigualdade triangular)}\\)\n\n\n\n\nDependendo da escolha da métrica \\(\\mathbf{\\Psi}\\), podemos obter diferentes medidas de distâncias, cada uma com suas características e aplicações. A escolha de uma medida adequada é fundamental para qualquer análise."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#distância-euclidiana",
    "href": "aulas/intro_est_multi/intro_est_multi.html#distância-euclidiana",
    "title": "Introdução à Estatística Multivariada",
    "section": "Distância Euclidiana",
    "text": "Distância Euclidiana\n\nSe a métrica \\(\\mathbf{\\Psi}\\) é dada por \\(\\mathbf{\\Psi} = \\mathbf{I}\\), temos a distância euclidiana quadrática. Neste caso, a expressão da distância é dada por:\n\n\\[d^2(\\mathbf{x}, \\mathbf{y}) = (\\mathbf{x} - \\mathbf{y})^t (\\mathbf{x} - \\mathbf{y})\\]\nde forma que a distância euclidiana é dada por:\n\\[d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{(\\mathbf{x} - \\mathbf{y})^t (\\mathbf{x} - \\mathbf{y})}\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#distância-euclidiana-1",
    "href": "aulas/intro_est_multi/intro_est_multi.html#distância-euclidiana-1",
    "title": "Introdução à Estatística Multivariada",
    "section": "Distância Euclidiana",
    "text": "Distância Euclidiana"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#distância-euclidiana-2",
    "href": "aulas/intro_est_multi/intro_est_multi.html#distância-euclidiana-2",
    "title": "Introdução à Estatística Multivariada",
    "section": "Distância Euclidiana",
    "text": "Distância Euclidiana\n\nA distância euclidiana confere mesmos pesos às diferenças verificadas em cada uma das \\(p\\) dimensões.\n\n\n\nNas análises estatísticas, em que distâncias serão aplicadas a dados de variáveis com diferentes variâncias, e na presença de covariâncias, incorporar tais características ao cálculo da distância pode ser fundamental.\n\n\n\n\nNesse contexto, duas medidas de distância mais apropriadas são as distâncias de Karl Pearson e a de Mahalanobis."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#distância-de-karl-pearson",
    "href": "aulas/intro_est_multi/intro_est_multi.html#distância-de-karl-pearson",
    "title": "Introdução à Estatística Multivariada",
    "section": "Distância de Karl Pearson",
    "text": "Distância de Karl Pearson\n\nSe considerarmos a métrica \\(\\mathbf{\\Psi}\\) como sendo igual a\n\n\\[\\mathbf{\\Psi} = \\mathbf{D}^{-1} = \\text{diag}\\left(\\dfrac{1}{s_{kk}}\\right), \\hspace{1cm} k = 1,\\cdots, p,\\]\nentão podemos definir a distância quadrática de Karl Pearson por:\n\\[d^2_p(\\mathbf{x}, \\mathbf{y}) = (\\mathbf{x} - \\mathbf{y})^t \\mathbf{D}^{-1}(\\mathbf{x} - \\mathbf{y})\\]\nde forma que a distância euclidiana ponderada é dada por,\n\\[d_p(\\mathbf{x}, \\mathbf{y}) = \\sqrt{(\\mathbf{x} - \\mathbf{y})^t\\mathbf{D}^{-1} (\\mathbf{x} - \\mathbf{y})}\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#distância-de-karl-pearson-1",
    "href": "aulas/intro_est_multi/intro_est_multi.html#distância-de-karl-pearson-1",
    "title": "Introdução à Estatística Multivariada",
    "section": "Distância de Karl Pearson",
    "text": "Distância de Karl Pearson"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#distância-de-karl-pearson-2",
    "href": "aulas/intro_est_multi/intro_est_multi.html#distância-de-karl-pearson-2",
    "title": "Introdução à Estatística Multivariada",
    "section": "Distância de Karl Pearson",
    "text": "Distância de Karl Pearson\n\nA distância de Karl Pearson evita o problema de heterogeneidade em um sistema de variáveis é dividindo cada variável por um fator que elimine o fator escala.\n\n\n\nTambém é conhecida como distância euclidiana ponderada ou padronizada"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#distância-de-mahalanobis",
    "href": "aulas/intro_est_multi/intro_est_multi.html#distância-de-mahalanobis",
    "title": "Introdução à Estatística Multivariada",
    "section": "Distância de Mahalanobis",
    "text": "Distância de Mahalanobis\n\nA distância de Mahalanobis configura um caso mais geral, em que são usadas tanto as variâncias quanto as covariâncias no cálculo da distância.\n\n\n\nSe considerarmos a métrica \\(\\mathbf{\\Psi}\\) igual a \\(\\mathbf{\\Psi} = \\mathbf{S}^{-1}\\), temos a chamada distância quadrática de Mahalanobis, dada por:\n\n\\[d^2_M(\\mathbf{x}, \\mathbf{y}) = (\\mathbf{x} - \\mathbf{y})^t \\mathbf{S}^{-1}(\\mathbf{x} - \\mathbf{y})\\]\nde forma que a distância de Mahalanobis é dada por,\n\\[d_M(\\mathbf{x}, \\mathbf{y}) = \\sqrt{(\\mathbf{x} - \\mathbf{y})^t\\mathbf{S}^{-1} (\\mathbf{x} - \\mathbf{y})}\\]"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#distância-de-mahalanobis-1",
    "href": "aulas/intro_est_multi/intro_est_multi.html#distância-de-mahalanobis-1",
    "title": "Introdução à Estatística Multivariada",
    "section": "Distância de Mahalanobis",
    "text": "Distância de Mahalanobis"
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#distância-de-mahalanobis-2",
    "href": "aulas/intro_est_multi/intro_est_multi.html#distância-de-mahalanobis-2",
    "title": "Introdução à Estatística Multivariada",
    "section": "Distância de Mahalanobis",
    "text": "Distância de Mahalanobis\n\nA distância de Mahalanobis é largamente aplicada, permitindo acomodar diferentes escalas e correlações entre as variáveis.\n\n\n\nDada sua definição, a distância de Mahalanobis, ao incorporar a inversa da matriz de covariância, tem como efeitos:\n\nPadronizar todas as variáveis de forma que apresentem mesma variância;\nEliminar correlações."
  },
  {
    "objectID": "aulas/intro_est_multi/intro_est_multi.html#matriz-de-distâncias",
    "href": "aulas/intro_est_multi/intro_est_multi.html#matriz-de-distâncias",
    "title": "Introdução à Estatística Multivariada",
    "section": "Matriz de distâncias",
    "text": "Matriz de distâncias\n\nSeja qual for a distância utilizada, é comum, em análises multivariadas, calculá-la para cada par de indivíduos e armazenar os valores em uma matriz, denominada matriz de distâncias.\n\n\\[\\mathbf{D}_{n \\times n} = \\left[ \\begin{array}{cccc} 0 & d_{12} & \\cdots & d_{1n} \\\\ d_{21} & 0 & \\cdots & d_{2n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ d_{n1} & d_{n2} & \\cdots & 0 \\end{array} \\right]\n        \\]\nem que \\(d_{ij} = d_{ji}\\) é a distância calculada para os indivíduos \\(i\\) e \\(j\\), utilizando uma métrica qualquer.\n\n\nA visualização das distâncias num gráfico do tipo “mapa de calor” permite uma apreciação conjunta das distâncias calculadas duas a duas."
  },
  {
    "objectID": "aulas.html",
    "href": "aulas.html",
    "title": "Aulas",
    "section": "",
    "text": "Revisão de Álgebra Matricial\nIntrodução à Estatística Multivariada\nDistribuição Normal Multivariada\nAnálise de Componentes Principais\nAnálise Fatorial\nAnálise de Conglomerados ou Agrupamentos\nAnálise Discriminante",
    "crumbs": [
      "Aulas"
    ]
  },
  {
    "objectID": "exercicios/lista01.html",
    "href": "exercicios/lista01.html",
    "title": "Lista de exercícios 01: Revisão de Álgebra Linear",
    "section": "",
    "text": "Data de entrega: 27 de outubro de 2025\n\n\n\nSejam as seguintes matrizes:\n\n\\[\\mathbf{A} = \\left(\\begin{array}{ccc} 4 & 2 & 3 \\\\ 7 & 5 & 8 \\end{array}\\right) \\text{     e     } \\mathbf{B} = \\left(\\begin{array}{ccc} 3 & -2 & 4\\\\ 6 & 9 & -5 \\end{array}\\right)\\]\n\nEncontre \\(\\mathbf{A + B}\\) e \\(\\mathbf{A- B}\\).\nEncontre \\(\\mathbf{AA}^t\\) e \\(\\mathbf{A}^t\\mathbf{A}\\).\nCalcule \\((\\mathbf{A+B})^t\\) e verifique que a matriz resultante é igual a \\(\\mathbf{A}^t + \\mathbf{B}^t\\).\nMostre que \\((\\mathbf{A}^t)^t = \\mathbf{A}\\).\n\n\n\nSejam as matrizes:\n\n\\[\\mathbf{A} = \\left(\\begin{array}{cc} 1 & 3 \\\\ 2 & -1 \\end{array}\\right)  \\text{      e      } \\mathbf{B} = \\left(\\begin{array}{cc} 2 & 0 \\\\ 1 & 5 \\end{array}\\right)\\]\n\nEncontre \\(\\mathbf{AB}\\) e \\(\\mathbf{BA}\\).\nEncontre \\(\\mathbf{|AB|}\\) e verifique se \\(\\mathbf{|AB| = |A| \\cdot |B|}\\).\nCalcule \\(\\mathbf{A+B}\\) e \\(\\mathbf{tr(A+B)}\\).\nCalcule \\(\\mathbf{tr(A)}\\) e \\(\\mathbf{tr(B)}\\) e verifique se \\(\\mathbf{tr(A+B) = tr(A) + tr(B)}\\).\nEncontre \\(\\mathbf{(AB)^t}\\) e verifique que \\(\\mathbf{(AB)^t = B^t A^t}\\).\n\n\n\nSejam as matrizes:\n\n\\[\\mathbf{A} = \\left(\\begin{array}{ccc}\n1 & 2& 3\\\\\n2 & -1 &1\n\\end{array}\\right)  \\text{      e      } \\mathbf{B} = \\left(\\begin{array}{cc}\n3 & -2 \\\\\n2 & 0 \\\\\n-1&1\n\\end{array}\\right)\\]\n\nEncontre \\(\\mathbf{AB}\\) e \\(\\mathbf{BA}\\).\nCalcule \\(\\mathbf{tr(AB)}\\) e \\(\\mathbf{tr(BA)}\\) e verifique se são iguais.\n\n\n\nSejam as matrizes:\n\n\\[\\mathbf{A} = \\left(\\begin{array}{ccc}\n1 & 2& 3\\\\\n2 & 4 & 6 \\\\\n5 & 10 & 15\n\\end{array}\\right)  \\text{      e      } \\mathbf{B} =  \\left(\\begin{array}{ccc}\n-1 &1& -2 \\\\\n-1 &1& -2 \\\\\n1&-1&2\n\\end{array}\\right)\\]\n\nMostre que \\(\\mathbf{AB = 0}\\).\nEncontre um vetor \\(\\mathbf{x}\\) tal que \\(\\mathbf{A} \\mathbf{x} = 0\\).\nMostre que \\(\\mathbf{|A|} = 0\\).\n\n\n\nSejam\n\n\\[\\mathbf{A} = \\left(\\begin{array}{ccc}\n1 & -1& 4\\\\\n-1 & 1 & 3 \\\\\n4 & 3 & 2\n\\end{array}\\right), \\mathbf{B} =  \\left(\\begin{array}{ccc}\n3 &-2& 4 \\\\\n7 &1& 0 \\\\\n2&3&5\n\\end{array}\\right), \\mathbf{x} = \\left(\\begin{array}{c}\n1 \\\\\n-1  \\\\\n2\n\\end{array}\\right), \\mathbf{y} = \\left(\\begin{array}{c}\n3  \\\\\n2  \\\\\n1\n\\end{array}\\right)\\]\nEncontre o que se pede:\n\n\\(\\mathbf{B} \\mathbf{x}\\)\n\\(\\mathbf{y^t B}\\)\n\\(\\mathbf{x^t A x}\\)\n\\(\\mathbf{x A y}\\).\n\\(\\mathbf{x y^t}\\).\n\\(\\mathbf{({x}-{y})^t A ({x}-{y})}\\)\n\n\n\nSejam\n\n\\[\\mathbf{A} = \\left(\\begin{array}{ccc}\n1 & 2& 3\\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{array}\\right)  \\text{      e      } \\mathbf{D} =  \\left(\\begin{array}{ccc}\na &0& 0 \\\\\n0 &b& 0 \\\\\n0&0&c\n\\end{array}\\right)\\]\nEncontre \\(\\mathbf{AD}\\), \\(\\mathbf{DA}\\) e \\(\\mathbf{DAD}\\)\n\n\nSejam\n\n\\[\\mathbf{A} = \\left(\\begin{array}{ccc} 1 & 3& 2\\\\ 2 & 0 & -1 \\end{array}\\right), \\mathbf{B} =  \\left(\\begin{array}{cc} 1 &2 \\\\ 0 &1 \\\\ 1&0 \\end{array}\\right), \\mathbf{C} = \\left(\\begin{array}{ccc} 2& 1& 1\\\\ 5 & -6 & -4 \\end{array}\\right)\\]\n\nEncontre \\(\\mathbf{AB}\\) e \\(\\mathbf{CB}\\). Elas são iguais?\nEncontre o posto das matrizes \\(\\mathbf{A}\\), \\(\\mathbf{B}\\) e \\(\\mathbf{C}\\).\n\n\n\nVerifique se as matrizes abaixo são ortogonais:\n\n\\[\\mathbf{A} =  \\displaystyle  \\frac{1}{169} \\cdot \\left(\\begin{array}{cc}\n5 & 12\\\\\n-12 & 5  \n\\end{array}\\right), \\mathbf{B} =  \\displaystyle  \\frac{1}{\\sqrt{2}} \\cdot \\left(\\begin{array}{cc}\n1 & 1\\\\\n1 & -1  \n\\end{array}\\right)\\]\n\n\nSeja a matriz\n\n\\[\\mathbf{A} =  \\left(\\begin{array}{cc}\n1 & 2\\\\\n2 & -2  \n\\end{array}\\right)\\]\n\nMostre que \\(\\mathbf{A}\\) é simétrica.\nObtenha os autovalores e autovetores da matriz \\(\\mathbf{A}\\).\nMostre que os autovetores são ortogonais.\nEscreva a decomposição espectral de \\(\\mathbf{A}\\).\nObtenha \\(\\mathbf{A^{-1}}\\), seus autovalores e autovetores e a decomposição espectral. Relacione com a decomposição espectral de \\(\\mathbf{A}\\).\nMostre que o determinante de \\(\\mathbf{A^{-1}}\\) é o inverso do determinante de \\(\\mathbf{A}\\).\nMostre que o determinante de \\(\\mathbf{A}\\) é o produto dos autovalores.\nEncontre a forma quadrática da matriz \\(\\mathbf{A}\\) e classifique-a.\nMostre que \\(\\mathbf{(A^t)^{-1} = (A^{-1})^t}\\).\n\n\n\nSeja a matriz\n\n\\[\\mathbf{A} =   \\left(\\begin{array}{ccc}\n3 & 6 &-1\\\\\n6 & 9 & 4 \\\\\n-1& 4& 3  \n\\end{array}\\right)\\]\n\nEncontre a decomposição espectral de \\(\\mathbf{A}\\).\nEncontre a decomposição espectral de \\(\\mathbf{A^2}\\) e mostre que a matriz diagonal de autovalores é igual ao quadrado da matriz \\(\\mathbf{D}\\) encontrada na parte \\(a.\\)\nEncontre a decomposição espectral de \\(\\mathbf{A^{-1}}\\) e mostre que a matriz diagonal de autovalores é igual à inversa da matriz \\(\\mathbf{D}\\) encontrada na parte \\(a.\\)\n\n\n\nDados os vetores \\(\\mathbf{x^t} = [1 \\hspace{0.5cm} 3]\\) e \\(\\mathbf{y^t} = [2 \\hspace{0.5cm} -5]\\):\n\n\nObtenha a norma de \\(\\mathbf{x}\\) e de \\(\\mathbf{y}\\).\nObtenha o ângulo e a distância entre esses vetores.\nObtenha a distância entre \\(\\mathbf{x}\\) e \\(\\mathbf{y}\\) na métrica\n\n\\[\\mathbf{A} =   \\left(\\begin{array}{cc}\n4 &0\\\\\n0 & 2  \n\\end{array}\\right)\\]\n\nObtenha a distância entre \\(\\mathbf{x}\\) e \\(\\mathbf{y}\\) na métrica\n\n\\[\\mathbf{A} =   \\left(\\begin{array}{cc}\n4 &2\\\\\n2 & 2  \n\\end{array}\\right)\\]\n\n\nUma forma quadrática \\(\\mathbf{x^t A x}\\) é dita ser positiva definida se a matriz \\(\\mathbf{A}\\) é positiva definida. A forma quadrática \\(3x_1^2 + 3x_2^2 - 2x_1x_2\\) é positiva definida?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estatística Multivariada I",
    "section": "",
    "text": "Página dedicada à disciplina EST014 - Estatística Multivariada I\n\n\nEstatística Multivariada I é uma disciplina que apresenta o estudo de técnicas estatísticas voltadas para a análise simultânea de múltiplas variáveis. O curso abrange temas como análise de componentes principais, análise fatorial, análise de agrupamentos e análise discriminante. Essas metodologias são amplamente aplicadas em diversas áreas do conhecimento, permitindo a extração de informações relevantes a partir de conjuntos de dados complexos. O objetivo da disciplina é fornecer tanto a fundamentação teórica quanto a aplicação prática dessas técnicas, capacitando os alunos a utilizar ferramentas estatísticas avançadas em suas pesquisas e projetos.\n\n\nEmenta\nRevisão de Álgebra Matricial. Introdução à Estatística Multivariada. Distribuição Normal Multivariada. Análise de Componentes Principais. Análise Fatorial. Análise de Conglomerados ou Agrupamentos. Análise Discriminante.\n\n\nConteúdo Programático\n\nRevisão de Álgebra Matricial: matrizes e vetores. Operações com matrizes. Inversão matricial. Formas quadráticas. Autovalores e autovetores. Teorema da decomposição espectral. Determinante.\nIntrodução à Estatística Multivariada: exemplos de aplicação. Definição de Vetores Aleatórios, Vetores de Médias e Matrizes de Covariâncias e Correlação. Interpretação destas Matrizes. Vetores de Médias Amostrais e Matrizes Covariâncias e Correlações Amostrais. Variância Generalizada e Variância Total. Distâncias: Euclidiana, Euclidiana padronizada e Mahalanobis.\nDistribuição Normal Multivariada: função densidade. Propriedades. Distribuição Normal Bivariada. Elipsóides de concentração. Métodos práticos de verificação da hipótese de normalidade multivariada.\nAnálise de Componentes Principais: construção das Componentes Principais pela Matriz de Covariância e pela Matriz de Correlação. Proporção da Variância Total Explicada pelas Componentes. Estimação das Componentes Principais e dos Escores. Exemplos Práticos de Aplicação.\nAnálise Fatorial: apresentação teórica da metodologia. Modelo de Fatores Ortogonais. Estimação dos Fatores pelos Métodos de Componentes Principais, de Fatores Principais e de Máxima Verossimilhança. Rotação de Fatores: Rotações Ortogonais e Oblíquas. Estimação dos Escores dos Fatores: Método de Mínimos Quadrados e Método de Regressão. Exemplos Práticos de Aplicação.\nAnálise de Conglomerados ou Agrupamentos: discussão dos vários Métodos de Formação de Conglomerados, Variáveis Quantitativas e Qualitativas. Métodos Hierárquicos: Método de Ligação Simples (Single Linkage), de Ligação Completa (Complete Linkage), de Ligação Média (Average Linkage), do Centróide, e de Ward. Métodos para encontrar o Número de Conglomerados Ótimo da Partição. Métodos Não Hierárquicos: Método das K-Médias (KMeans). Exemplos Práticos de Aplicação.\nAnálise Discriminante: discriminação e classificação em 2 grupos. Estimação das Probabilidades de Erro de Classificação. Discriminação e Classificação Multivariada. Função Discriminante de Fischer. Exemplos Práticos de Aplicação.\n\n\n\nHorário de Aulas\nNeste semestre, as aulas da disciplina serão ministradas no LABEST II.\n\n\n\nDia\nHorário\nLocal\n\n\n\n\nTerça-feira\n21:00 - 22:40\nLABEST III\n\n\nQuinta-feira\n19:00 - 20:40\nLABEST III\n\n\n\n\n\n\n\n\n\n\n\nReferências Bibliográficas\n\nAnderson, T. W. 2009. AN INTRODUCTION TO MULTIVARIATE STATISTICAL ANALYSIS, 3RD ED. Wiley India Pvt. Limited.\n\n\nCORRAR, Luiz J., Edilson PAULO, and José M. DIAS FILHO. 2007. Análise Multivariada: Para Os Cursos de Administração, Ciências Contábeis e Economia. Editora Atlas.\n\n\nFávero, L. P., and P. Belfiore. 2017. Manual de análise de Dados: Estatı́stica e Modelagem Multivariada Com Excel, SPSS e Stata. Elsevier Editora Ltda.\n\n\nFerreira, D. F. 2018. Estatística Multivariada. Editora UFLA.\n\n\nHair, J. F., W. C. Black, B. J. Babin, R. E. Anderson, and R. L. Tatham. 2009. Análise Multivariada de Dados - 6ed. Bookman.\n\n\nJohnson, R. A., and D. W. Wichern. 2007. Applied Multivariate Statistical Analysis. Applied Multivariate Statistical Analysis. Pearson Prentice Hall.\n\n\nLATTIN, James, J. Douglas CARROLL, and Paul E. GREEN. 2011. Análise de Dados Multivariados. CENGAGE Learning.\n\n\nMingoti, Sueli. 2005. Análise de Dados Através de Métodos de Estatística Multivariada: Uma Abordagem Aplicada.\n\n\nRencher, A. C., and W. F. Christensen. 2012. Methods of Multivariate Analysis. Wiley Series in Probability and Statistics. Wiley."
  },
  {
    "objectID": "programacao/semana-1.html",
    "href": "programacao/semana-1.html",
    "title": "Semana 01",
    "section": "",
    "text": "Sejam bem-vindos à disciplina EST014 - Estatística Multivariada I.\nLeiam com atenção o plano de ensino da disciplina. Nele estão as regras do jogo!"
  }
]